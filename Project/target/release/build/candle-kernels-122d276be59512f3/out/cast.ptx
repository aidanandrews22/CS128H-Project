//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-33567101
// Cuda compilation tools, release 12.3, V12.3.107
// Based on NVVM 7.0.1
//

.version 8.3
.target sm_89
.address_size 64

	// .globl	cast_bf16_bf16

.visible .entry cast_bf16_bf16(
	.param .u64 cast_bf16_bf16_param_0,
	.param .u64 cast_bf16_bf16_param_1,
	.param .u64 cast_bf16_bf16_param_2,
	.param .u64 cast_bf16_bf16_param_3,
	.param .u64 cast_bf16_bf16_param_4
)
{
	.reg .pred 	%p<17>;
	.reg .b16 	%rs<4>;
	.reg .b32 	%r<41>;
	.reg .b64 	%rd<65>;


	ld.param.u64 	%rd23, [cast_bf16_bf16_param_0];
	ld.param.u64 	%rd24, [cast_bf16_bf16_param_1];
	ld.param.u64 	%rd25, [cast_bf16_bf16_param_2];
	ld.param.u64 	%rd26, [cast_bf16_bf16_param_3];
	ld.param.u64 	%rd27, [cast_bf16_bf16_param_4];
	cvta.to.global.u64 	%rd1, %rd27;
	cvta.to.global.u64 	%rd2, %rd26;
	cvta.to.global.u64 	%rd3, %rd25;
	setp.eq.s64 	%p3, %rd24, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p16, %p2;
	@%p3 bra 	$L__BB0_4;

	mov.u64 	%rd59, 1;
	mov.u32 	%r34, 0;

$L__BB0_2:
	not.b32 	%r20, %r34;
	cvt.u64.u32 	%rd29, %r20;
	add.s64 	%rd30, %rd29, %rd24;
	and.b64  	%rd5, %rd30, 4294967295;
	add.s64 	%rd31, %rd5, %rd24;
	shl.b64 	%rd32, %rd31, 3;
	add.s64 	%rd33, %rd3, %rd32;
	ld.global.u64 	%rd34, [%rd33];
	setp.ne.s64 	%p5, %rd59, %rd34;
	mov.pred 	%p16, -1;
	@%p5 bra 	$L__BB0_4;

	shl.b64 	%rd35, %rd5, 3;
	add.s64 	%rd36, %rd3, %rd35;
	ld.global.u64 	%rd37, [%rd36];
	mul.lo.s64 	%rd59, %rd37, %rd59;
	add.s32 	%r34, %r34, 1;
	cvt.u64.u32 	%rd38, %r34;
	setp.lt.u64 	%p7, %rd38, %rd24;
	mov.pred 	%p16, %p2;
	@%p7 bra 	$L__BB0_2;

$L__BB0_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r35, %r21, %r3, %r22;
	cvt.u64.u32 	%rd60, %r35;
	@%p16 bra 	$L__BB0_8;
	bra.uni 	$L__BB0_5;

$L__BB0_8:
	setp.ge.u64 	%p10, %rd60, %rd23;
	@%p10 bra 	$L__BB0_17;

	mov.u32 	%r24, %nctaid.x;
	mul.lo.s32 	%r8, %r3, %r24;
	@%p3 bra 	$L__BB0_16;

$L__BB0_10:
	mov.u32 	%r37, 0;
	mov.u32 	%r38, %r35;
	mov.u32 	%r39, %r37;

$L__BB0_11:
	not.b32 	%r27, %r37;
	cvt.u64.u32 	%rd42, %r27;
	add.s64 	%rd43, %rd42, %rd24;
	cvt.u64.u32 	%rd11, %r38;
	shl.b64 	%rd44, %rd43, 3;
	and.b64  	%rd45, %rd44, 34359738360;
	add.s64 	%rd12, %rd3, %rd45;
	ld.global.u64 	%rd13, [%rd12];
	and.b64  	%rd46, %rd13, -4294967296;
	setp.eq.s64 	%p12, %rd46, 0;
	@%p12 bra 	$L__BB0_13;

	div.u64 	%rd62, %rd11, %rd13;
	mul.lo.s64 	%rd47, %rd62, %rd13;
	sub.s64 	%rd63, %rd11, %rd47;
	bra.uni 	$L__BB0_14;

$L__BB0_13:
	cvt.u32.u64 	%r28, %rd13;
	cvt.u32.u64 	%r29, %rd11;
	div.u32 	%r30, %r29, %r28;
	mul.lo.s32 	%r31, %r30, %r28;
	sub.s32 	%r32, %r29, %r31;
	cvt.u64.u32 	%rd62, %r30;
	cvt.u64.u32 	%rd63, %r32;

$L__BB0_14:
	shl.b64 	%rd48, %rd24, 3;
	add.s64 	%rd49, %rd12, %rd48;
	ld.global.u64 	%rd50, [%rd49];
	mul.lo.s64 	%rd51, %rd50, %rd63;
	cvt.u32.u64 	%r33, %rd51;
	add.s32 	%r39, %r39, %r33;
	cvt.u32.u64 	%r38, %rd62;
	add.s32 	%r37, %r37, 1;
	cvt.u64.u32 	%rd52, %r37;
	setp.lt.u64 	%p13, %rd52, %rd24;
	@%p13 bra 	$L__BB0_11;

	mul.wide.u32 	%rd53, %r39, 2;
	add.s64 	%rd54, %rd2, %rd53;
	ld.global.u16 	%rs2, [%rd54];
	shl.b64 	%rd55, %rd60, 1;
	add.s64 	%rd56, %rd1, %rd55;
	st.global.u16 	[%rd56], %rs2;
	add.s32 	%r35, %r35, %r8;
	cvt.u64.u32 	%rd60, %r35;
	setp.lt.u64 	%p14, %rd60, %rd23;
	@%p14 bra 	$L__BB0_10;
	bra.uni 	$L__BB0_17;

$L__BB0_16:
	ld.global.u16 	%rs3, [%rd2];
	shl.b64 	%rd57, %rd60, 1;
	add.s64 	%rd58, %rd1, %rd57;
	st.global.u16 	[%rd58], %rs3;
	add.s32 	%r35, %r35, %r8;
	cvt.u64.u32 	%rd60, %r35;
	setp.lt.u64 	%p15, %rd60, %rd23;
	@%p15 bra 	$L__BB0_16;
	bra.uni 	$L__BB0_17;

$L__BB0_5:
	setp.ge.u64 	%p8, %rd60, %rd23;
	@%p8 bra 	$L__BB0_17;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;

$L__BB0_7:
	shl.b64 	%rd39, %rd60, 1;
	add.s64 	%rd40, %rd2, %rd39;
	ld.global.u16 	%rs1, [%rd40];
	add.s64 	%rd41, %rd1, %rd39;
	st.global.u16 	[%rd41], %rs1;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd60, %r35;
	setp.lt.u64 	%p9, %rd60, %rd23;
	@%p9 bra 	$L__BB0_7;

$L__BB0_17:
	ret;

}
	// .globl	cast_bf16_u32
.visible .entry cast_bf16_u32(
	.param .u64 cast_bf16_u32_param_0,
	.param .u64 cast_bf16_u32_param_1,
	.param .u64 cast_bf16_u32_param_2,
	.param .u64 cast_bf16_u32_param_3,
	.param .u64 cast_bf16_u32_param_4
)
{
	.reg .pred 	%p<17>;
	.reg .b16 	%rs<4>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<44>;
	.reg .b64 	%rd<66>;


	ld.param.u64 	%rd23, [cast_bf16_u32_param_0];
	ld.param.u64 	%rd24, [cast_bf16_u32_param_1];
	ld.param.u64 	%rd25, [cast_bf16_u32_param_2];
	ld.param.u64 	%rd26, [cast_bf16_u32_param_3];
	ld.param.u64 	%rd27, [cast_bf16_u32_param_4];
	cvta.to.global.u64 	%rd1, %rd27;
	cvta.to.global.u64 	%rd2, %rd26;
	cvta.to.global.u64 	%rd3, %rd25;
	setp.eq.s64 	%p3, %rd24, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p16, %p2;
	@%p3 bra 	$L__BB1_4;

	mov.u64 	%rd60, 1;
	mov.u32 	%r37, 0;

$L__BB1_2:
	not.b32 	%r20, %r37;
	cvt.u64.u32 	%rd29, %r20;
	add.s64 	%rd30, %rd29, %rd24;
	and.b64  	%rd5, %rd30, 4294967295;
	add.s64 	%rd31, %rd5, %rd24;
	shl.b64 	%rd32, %rd31, 3;
	add.s64 	%rd33, %rd3, %rd32;
	ld.global.u64 	%rd34, [%rd33];
	setp.ne.s64 	%p5, %rd60, %rd34;
	mov.pred 	%p16, -1;
	@%p5 bra 	$L__BB1_4;

	shl.b64 	%rd35, %rd5, 3;
	add.s64 	%rd36, %rd3, %rd35;
	ld.global.u64 	%rd37, [%rd36];
	mul.lo.s64 	%rd60, %rd37, %rd60;
	add.s32 	%r37, %r37, 1;
	cvt.u64.u32 	%rd38, %r37;
	setp.lt.u64 	%p7, %rd38, %rd24;
	mov.pred 	%p16, %p2;
	@%p7 bra 	$L__BB1_2;

$L__BB1_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r38, %r21, %r3, %r22;
	cvt.u64.u32 	%rd61, %r38;
	@%p16 bra 	$L__BB1_8;
	bra.uni 	$L__BB1_5;

$L__BB1_8:
	setp.ge.u64 	%p10, %rd61, %rd23;
	@%p10 bra 	$L__BB1_17;

	mov.u32 	%r25, %nctaid.x;
	mul.lo.s32 	%r8, %r3, %r25;
	@%p3 bra 	$L__BB1_16;

$L__BB1_10:
	mov.u32 	%r40, 0;
	mov.u32 	%r41, %r38;
	mov.u32 	%r42, %r40;

$L__BB1_11:
	not.b32 	%r28, %r40;
	cvt.u64.u32 	%rd43, %r28;
	add.s64 	%rd44, %rd43, %rd24;
	cvt.u64.u32 	%rd11, %r41;
	shl.b64 	%rd45, %rd44, 3;
	and.b64  	%rd46, %rd45, 34359738360;
	add.s64 	%rd12, %rd3, %rd46;
	ld.global.u64 	%rd13, [%rd12];
	and.b64  	%rd47, %rd13, -4294967296;
	setp.eq.s64 	%p12, %rd47, 0;
	@%p12 bra 	$L__BB1_13;

	div.u64 	%rd63, %rd11, %rd13;
	mul.lo.s64 	%rd48, %rd63, %rd13;
	sub.s64 	%rd64, %rd11, %rd48;
	bra.uni 	$L__BB1_14;

$L__BB1_13:
	cvt.u32.u64 	%r29, %rd13;
	cvt.u32.u64 	%r30, %rd11;
	div.u32 	%r31, %r30, %r29;
	mul.lo.s32 	%r32, %r31, %r29;
	sub.s32 	%r33, %r30, %r32;
	cvt.u64.u32 	%rd63, %r31;
	cvt.u64.u32 	%rd64, %r33;

$L__BB1_14:
	shl.b64 	%rd49, %rd24, 3;
	add.s64 	%rd50, %rd12, %rd49;
	ld.global.u64 	%rd51, [%rd50];
	mul.lo.s64 	%rd52, %rd51, %rd64;
	cvt.u32.u64 	%r34, %rd52;
	add.s32 	%r42, %r42, %r34;
	cvt.u32.u64 	%r41, %rd63;
	add.s32 	%r40, %r40, 1;
	cvt.u64.u32 	%rd53, %r40;
	setp.lt.u64 	%p13, %rd53, %rd24;
	@%p13 bra 	$L__BB1_11;

	mul.wide.u32 	%rd54, %r42, 2;
	add.s64 	%rd55, %rd2, %rd54;
	ld.global.u16 	%rs2, [%rd55];
	// begin inline asm
	{ mov.b32 %f2, {0,%rs2};}

	// end inline asm
	cvt.rzi.u32.f32 	%r35, %f2;
	shl.b64 	%rd56, %rd61, 2;
	add.s64 	%rd57, %rd1, %rd56;
	st.global.u32 	[%rd57], %r35;
	add.s32 	%r38, %r38, %r8;
	cvt.u64.u32 	%rd61, %r38;
	setp.lt.u64 	%p14, %rd61, %rd23;
	@%p14 bra 	$L__BB1_10;
	bra.uni 	$L__BB1_17;

$L__BB1_16:
	ld.global.u16 	%rs3, [%rd2];
	// begin inline asm
	{ mov.b32 %f3, {0,%rs3};}

	// end inline asm
	cvt.rzi.u32.f32 	%r36, %f3;
	shl.b64 	%rd58, %rd61, 2;
	add.s64 	%rd59, %rd1, %rd58;
	st.global.u32 	[%rd59], %r36;
	add.s32 	%r38, %r38, %r8;
	cvt.u64.u32 	%rd61, %r38;
	setp.lt.u64 	%p15, %rd61, %rd23;
	@%p15 bra 	$L__BB1_16;
	bra.uni 	$L__BB1_17;

$L__BB1_5:
	setp.ge.u64 	%p8, %rd61, %rd23;
	@%p8 bra 	$L__BB1_17;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;

$L__BB1_7:
	shl.b64 	%rd39, %rd61, 1;
	add.s64 	%rd40, %rd2, %rd39;
	ld.global.u16 	%rs1, [%rd40];
	// begin inline asm
	{ mov.b32 %f1, {0,%rs1};}

	// end inline asm
	cvt.rzi.u32.f32 	%r24, %f1;
	shl.b64 	%rd41, %rd61, 2;
	add.s64 	%rd42, %rd1, %rd41;
	st.global.u32 	[%rd42], %r24;
	add.s32 	%r38, %r38, %r5;
	cvt.u64.u32 	%rd61, %r38;
	setp.lt.u64 	%p9, %rd61, %rd23;
	@%p9 bra 	$L__BB1_7;

$L__BB1_17:
	ret;

}
	// .globl	cast_bf16_f32
.visible .entry cast_bf16_f32(
	.param .u64 cast_bf16_f32_param_0,
	.param .u64 cast_bf16_f32_param_1,
	.param .u64 cast_bf16_f32_param_2,
	.param .u64 cast_bf16_f32_param_3,
	.param .u64 cast_bf16_f32_param_4
)
{
	.reg .pred 	%p<17>;
	.reg .b16 	%rs<4>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<41>;
	.reg .b64 	%rd<66>;


	ld.param.u64 	%rd23, [cast_bf16_f32_param_0];
	ld.param.u64 	%rd24, [cast_bf16_f32_param_1];
	ld.param.u64 	%rd25, [cast_bf16_f32_param_2];
	ld.param.u64 	%rd26, [cast_bf16_f32_param_3];
	ld.param.u64 	%rd27, [cast_bf16_f32_param_4];
	cvta.to.global.u64 	%rd1, %rd27;
	cvta.to.global.u64 	%rd2, %rd26;
	cvta.to.global.u64 	%rd3, %rd25;
	setp.eq.s64 	%p3, %rd24, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p16, %p2;
	@%p3 bra 	$L__BB2_4;

	mov.u64 	%rd60, 1;
	mov.u32 	%r34, 0;

$L__BB2_2:
	not.b32 	%r20, %r34;
	cvt.u64.u32 	%rd29, %r20;
	add.s64 	%rd30, %rd29, %rd24;
	and.b64  	%rd5, %rd30, 4294967295;
	add.s64 	%rd31, %rd5, %rd24;
	shl.b64 	%rd32, %rd31, 3;
	add.s64 	%rd33, %rd3, %rd32;
	ld.global.u64 	%rd34, [%rd33];
	setp.ne.s64 	%p5, %rd60, %rd34;
	mov.pred 	%p16, -1;
	@%p5 bra 	$L__BB2_4;

	shl.b64 	%rd35, %rd5, 3;
	add.s64 	%rd36, %rd3, %rd35;
	ld.global.u64 	%rd37, [%rd36];
	mul.lo.s64 	%rd60, %rd37, %rd60;
	add.s32 	%r34, %r34, 1;
	cvt.u64.u32 	%rd38, %r34;
	setp.lt.u64 	%p7, %rd38, %rd24;
	mov.pred 	%p16, %p2;
	@%p7 bra 	$L__BB2_2;

$L__BB2_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r35, %r21, %r3, %r22;
	cvt.u64.u32 	%rd61, %r35;
	@%p16 bra 	$L__BB2_8;
	bra.uni 	$L__BB2_5;

$L__BB2_8:
	setp.ge.u64 	%p10, %rd61, %rd23;
	@%p10 bra 	$L__BB2_17;

	mov.u32 	%r24, %nctaid.x;
	mul.lo.s32 	%r8, %r3, %r24;
	@%p3 bra 	$L__BB2_16;

$L__BB2_10:
	mov.u32 	%r37, 0;
	mov.u32 	%r38, %r35;
	mov.u32 	%r39, %r37;

$L__BB2_11:
	not.b32 	%r27, %r37;
	cvt.u64.u32 	%rd43, %r27;
	add.s64 	%rd44, %rd43, %rd24;
	cvt.u64.u32 	%rd11, %r38;
	shl.b64 	%rd45, %rd44, 3;
	and.b64  	%rd46, %rd45, 34359738360;
	add.s64 	%rd12, %rd3, %rd46;
	ld.global.u64 	%rd13, [%rd12];
	and.b64  	%rd47, %rd13, -4294967296;
	setp.eq.s64 	%p12, %rd47, 0;
	@%p12 bra 	$L__BB2_13;

	div.u64 	%rd63, %rd11, %rd13;
	mul.lo.s64 	%rd48, %rd63, %rd13;
	sub.s64 	%rd64, %rd11, %rd48;
	bra.uni 	$L__BB2_14;

$L__BB2_13:
	cvt.u32.u64 	%r28, %rd13;
	cvt.u32.u64 	%r29, %rd11;
	div.u32 	%r30, %r29, %r28;
	mul.lo.s32 	%r31, %r30, %r28;
	sub.s32 	%r32, %r29, %r31;
	cvt.u64.u32 	%rd63, %r30;
	cvt.u64.u32 	%rd64, %r32;

$L__BB2_14:
	shl.b64 	%rd49, %rd24, 3;
	add.s64 	%rd50, %rd12, %rd49;
	ld.global.u64 	%rd51, [%rd50];
	mul.lo.s64 	%rd52, %rd51, %rd64;
	cvt.u32.u64 	%r33, %rd52;
	add.s32 	%r39, %r39, %r33;
	cvt.u32.u64 	%r38, %rd63;
	add.s32 	%r37, %r37, 1;
	cvt.u64.u32 	%rd53, %r37;
	setp.lt.u64 	%p13, %rd53, %rd24;
	@%p13 bra 	$L__BB2_11;

	mul.wide.u32 	%rd54, %r39, 2;
	add.s64 	%rd55, %rd2, %rd54;
	ld.global.u16 	%rs2, [%rd55];
	// begin inline asm
	{ mov.b32 %f2, {0,%rs2};}

	// end inline asm
	shl.b64 	%rd56, %rd61, 2;
	add.s64 	%rd57, %rd1, %rd56;
	st.global.f32 	[%rd57], %f2;
	add.s32 	%r35, %r35, %r8;
	cvt.u64.u32 	%rd61, %r35;
	setp.lt.u64 	%p14, %rd61, %rd23;
	@%p14 bra 	$L__BB2_10;
	bra.uni 	$L__BB2_17;

$L__BB2_16:
	ld.global.u16 	%rs3, [%rd2];
	// begin inline asm
	{ mov.b32 %f3, {0,%rs3};}

	// end inline asm
	shl.b64 	%rd58, %rd61, 2;
	add.s64 	%rd59, %rd1, %rd58;
	st.global.f32 	[%rd59], %f3;
	add.s32 	%r35, %r35, %r8;
	cvt.u64.u32 	%rd61, %r35;
	setp.lt.u64 	%p15, %rd61, %rd23;
	@%p15 bra 	$L__BB2_16;
	bra.uni 	$L__BB2_17;

$L__BB2_5:
	setp.ge.u64 	%p8, %rd61, %rd23;
	@%p8 bra 	$L__BB2_17;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;

$L__BB2_7:
	shl.b64 	%rd39, %rd61, 1;
	add.s64 	%rd40, %rd2, %rd39;
	ld.global.u16 	%rs1, [%rd40];
	// begin inline asm
	{ mov.b32 %f1, {0,%rs1};}

	// end inline asm
	shl.b64 	%rd41, %rd61, 2;
	add.s64 	%rd42, %rd1, %rd41;
	st.global.f32 	[%rd42], %f1;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd61, %r35;
	setp.lt.u64 	%p9, %rd61, %rd23;
	@%p9 bra 	$L__BB2_7;

$L__BB2_17:
	ret;

}
	// .globl	cast_bf16_f64
.visible .entry cast_bf16_f64(
	.param .u64 cast_bf16_f64_param_0,
	.param .u64 cast_bf16_f64_param_1,
	.param .u64 cast_bf16_f64_param_2,
	.param .u64 cast_bf16_f64_param_3,
	.param .u64 cast_bf16_f64_param_4
)
{
	.reg .pred 	%p<17>;
	.reg .b16 	%rs<4>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<41>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<66>;


	ld.param.u64 	%rd23, [cast_bf16_f64_param_0];
	ld.param.u64 	%rd24, [cast_bf16_f64_param_1];
	ld.param.u64 	%rd25, [cast_bf16_f64_param_2];
	ld.param.u64 	%rd26, [cast_bf16_f64_param_3];
	ld.param.u64 	%rd27, [cast_bf16_f64_param_4];
	cvta.to.global.u64 	%rd1, %rd27;
	cvta.to.global.u64 	%rd2, %rd26;
	cvta.to.global.u64 	%rd3, %rd25;
	setp.eq.s64 	%p3, %rd24, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p16, %p2;
	@%p3 bra 	$L__BB3_4;

	mov.u64 	%rd60, 1;
	mov.u32 	%r34, 0;

$L__BB3_2:
	not.b32 	%r20, %r34;
	cvt.u64.u32 	%rd29, %r20;
	add.s64 	%rd30, %rd29, %rd24;
	and.b64  	%rd5, %rd30, 4294967295;
	add.s64 	%rd31, %rd5, %rd24;
	shl.b64 	%rd32, %rd31, 3;
	add.s64 	%rd33, %rd3, %rd32;
	ld.global.u64 	%rd34, [%rd33];
	setp.ne.s64 	%p5, %rd60, %rd34;
	mov.pred 	%p16, -1;
	@%p5 bra 	$L__BB3_4;

	shl.b64 	%rd35, %rd5, 3;
	add.s64 	%rd36, %rd3, %rd35;
	ld.global.u64 	%rd37, [%rd36];
	mul.lo.s64 	%rd60, %rd37, %rd60;
	add.s32 	%r34, %r34, 1;
	cvt.u64.u32 	%rd38, %r34;
	setp.lt.u64 	%p7, %rd38, %rd24;
	mov.pred 	%p16, %p2;
	@%p7 bra 	$L__BB3_2;

$L__BB3_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r35, %r21, %r3, %r22;
	cvt.u64.u32 	%rd61, %r35;
	@%p16 bra 	$L__BB3_8;
	bra.uni 	$L__BB3_5;

$L__BB3_8:
	setp.ge.u64 	%p10, %rd61, %rd23;
	@%p10 bra 	$L__BB3_17;

	mov.u32 	%r24, %nctaid.x;
	mul.lo.s32 	%r8, %r3, %r24;
	@%p3 bra 	$L__BB3_16;

$L__BB3_10:
	mov.u32 	%r37, 0;
	mov.u32 	%r38, %r35;
	mov.u32 	%r39, %r37;

$L__BB3_11:
	not.b32 	%r27, %r37;
	cvt.u64.u32 	%rd43, %r27;
	add.s64 	%rd44, %rd43, %rd24;
	cvt.u64.u32 	%rd11, %r38;
	shl.b64 	%rd45, %rd44, 3;
	and.b64  	%rd46, %rd45, 34359738360;
	add.s64 	%rd12, %rd3, %rd46;
	ld.global.u64 	%rd13, [%rd12];
	and.b64  	%rd47, %rd13, -4294967296;
	setp.eq.s64 	%p12, %rd47, 0;
	@%p12 bra 	$L__BB3_13;

	div.u64 	%rd63, %rd11, %rd13;
	mul.lo.s64 	%rd48, %rd63, %rd13;
	sub.s64 	%rd64, %rd11, %rd48;
	bra.uni 	$L__BB3_14;

$L__BB3_13:
	cvt.u32.u64 	%r28, %rd13;
	cvt.u32.u64 	%r29, %rd11;
	div.u32 	%r30, %r29, %r28;
	mul.lo.s32 	%r31, %r30, %r28;
	sub.s32 	%r32, %r29, %r31;
	cvt.u64.u32 	%rd63, %r30;
	cvt.u64.u32 	%rd64, %r32;

$L__BB3_14:
	shl.b64 	%rd49, %rd24, 3;
	add.s64 	%rd50, %rd12, %rd49;
	ld.global.u64 	%rd51, [%rd50];
	mul.lo.s64 	%rd52, %rd51, %rd64;
	cvt.u32.u64 	%r33, %rd52;
	add.s32 	%r39, %r39, %r33;
	cvt.u32.u64 	%r38, %rd63;
	add.s32 	%r37, %r37, 1;
	cvt.u64.u32 	%rd53, %r37;
	setp.lt.u64 	%p13, %rd53, %rd24;
	@%p13 bra 	$L__BB3_11;

	mul.wide.u32 	%rd54, %r39, 2;
	add.s64 	%rd55, %rd2, %rd54;
	ld.global.u16 	%rs2, [%rd55];
	// begin inline asm
	{ mov.b32 %f2, {0,%rs2};}

	// end inline asm
	cvt.f64.f32 	%fd2, %f2;
	shl.b64 	%rd56, %rd61, 3;
	add.s64 	%rd57, %rd1, %rd56;
	st.global.f64 	[%rd57], %fd2;
	add.s32 	%r35, %r35, %r8;
	cvt.u64.u32 	%rd61, %r35;
	setp.lt.u64 	%p14, %rd61, %rd23;
	@%p14 bra 	$L__BB3_10;
	bra.uni 	$L__BB3_17;

$L__BB3_16:
	ld.global.u16 	%rs3, [%rd2];
	// begin inline asm
	{ mov.b32 %f3, {0,%rs3};}

	// end inline asm
	cvt.f64.f32 	%fd3, %f3;
	shl.b64 	%rd58, %rd61, 3;
	add.s64 	%rd59, %rd1, %rd58;
	st.global.f64 	[%rd59], %fd3;
	add.s32 	%r35, %r35, %r8;
	cvt.u64.u32 	%rd61, %r35;
	setp.lt.u64 	%p15, %rd61, %rd23;
	@%p15 bra 	$L__BB3_16;
	bra.uni 	$L__BB3_17;

$L__BB3_5:
	setp.ge.u64 	%p8, %rd61, %rd23;
	@%p8 bra 	$L__BB3_17;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;

$L__BB3_7:
	shl.b64 	%rd39, %rd61, 1;
	add.s64 	%rd40, %rd2, %rd39;
	ld.global.u16 	%rs1, [%rd40];
	// begin inline asm
	{ mov.b32 %f1, {0,%rs1};}

	// end inline asm
	cvt.f64.f32 	%fd1, %f1;
	shl.b64 	%rd41, %rd61, 3;
	add.s64 	%rd42, %rd1, %rd41;
	st.global.f64 	[%rd42], %fd1;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd61, %r35;
	setp.lt.u64 	%p9, %rd61, %rd23;
	@%p9 bra 	$L__BB3_7;

$L__BB3_17:
	ret;

}
	// .globl	cast_u8_bf16
.visible .entry cast_u8_bf16(
	.param .u64 cast_u8_bf16_param_0,
	.param .u64 cast_u8_bf16_param_1,
	.param .u64 cast_u8_bf16_param_2,
	.param .u64 cast_u8_bf16_param_3,
	.param .u64 cast_u8_bf16_param_4
)
{
	.reg .pred 	%p<20>;
	.reg .b16 	%rs<4>;
	.reg .f32 	%f<16>;
	.reg .b32 	%r<50>;
	.reg .b64 	%rd<65>;


	ld.param.u64 	%rd23, [cast_u8_bf16_param_0];
	ld.param.u64 	%rd24, [cast_u8_bf16_param_1];
	ld.param.u64 	%rd25, [cast_u8_bf16_param_2];
	ld.param.u64 	%rd26, [cast_u8_bf16_param_3];
	ld.param.u64 	%rd27, [cast_u8_bf16_param_4];
	cvta.to.global.u64 	%rd1, %rd27;
	cvta.to.global.u64 	%rd2, %rd26;
	cvta.to.global.u64 	%rd3, %rd25;
	setp.eq.s64 	%p3, %rd24, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p19, %p2;
	@%p3 bra 	$L__BB4_4;

	mov.u64 	%rd59, 1;
	mov.u32 	%r43, 0;

$L__BB4_2:
	not.b32 	%r20, %r43;
	cvt.u64.u32 	%rd29, %r20;
	add.s64 	%rd30, %rd29, %rd24;
	and.b64  	%rd5, %rd30, 4294967295;
	add.s64 	%rd31, %rd5, %rd24;
	shl.b64 	%rd32, %rd31, 3;
	add.s64 	%rd33, %rd3, %rd32;
	ld.global.u64 	%rd34, [%rd33];
	setp.ne.s64 	%p5, %rd59, %rd34;
	mov.pred 	%p19, -1;
	@%p5 bra 	$L__BB4_4;

	shl.b64 	%rd35, %rd5, 3;
	add.s64 	%rd36, %rd3, %rd35;
	ld.global.u64 	%rd37, [%rd36];
	mul.lo.s64 	%rd59, %rd37, %rd59;
	add.s32 	%r43, %r43, 1;
	cvt.u64.u32 	%rd38, %r43;
	setp.lt.u64 	%p7, %rd38, %rd24;
	mov.pred 	%p19, %p2;
	@%p7 bra 	$L__BB4_2;

$L__BB4_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r44, %r21, %r3, %r22;
	cvt.u64.u32 	%rd60, %r44;
	@%p19 bra 	$L__BB4_8;
	bra.uni 	$L__BB4_5;

$L__BB4_8:
	setp.ge.u64 	%p11, %rd60, %rd23;
	@%p11 bra 	$L__BB4_17;

	mov.u32 	%r27, %nctaid.x;
	mul.lo.s32 	%r8, %r3, %r27;
	@%p3 bra 	$L__BB4_16;

$L__BB4_10:
	mov.u32 	%r46, 0;
	mov.u32 	%r47, %r44;
	mov.u32 	%r48, %r46;

$L__BB4_11:
	not.b32 	%r30, %r46;
	cvt.u64.u32 	%rd42, %r30;
	add.s64 	%rd43, %rd42, %rd24;
	cvt.u64.u32 	%rd11, %r47;
	shl.b64 	%rd44, %rd43, 3;
	and.b64  	%rd45, %rd44, 34359738360;
	add.s64 	%rd12, %rd3, %rd45;
	ld.global.u64 	%rd13, [%rd12];
	and.b64  	%rd46, %rd13, -4294967296;
	setp.eq.s64 	%p13, %rd46, 0;
	@%p13 bra 	$L__BB4_13;

	div.u64 	%rd62, %rd11, %rd13;
	mul.lo.s64 	%rd47, %rd62, %rd13;
	sub.s64 	%rd63, %rd11, %rd47;
	bra.uni 	$L__BB4_14;

$L__BB4_13:
	cvt.u32.u64 	%r31, %rd13;
	cvt.u32.u64 	%r32, %rd11;
	div.u32 	%r33, %r32, %r31;
	mul.lo.s32 	%r34, %r33, %r31;
	sub.s32 	%r35, %r32, %r34;
	cvt.u64.u32 	%rd62, %r33;
	cvt.u64.u32 	%rd63, %r35;

$L__BB4_14:
	shl.b64 	%rd48, %rd24, 3;
	add.s64 	%rd49, %rd12, %rd48;
	ld.global.u64 	%rd50, [%rd49];
	mul.lo.s64 	%rd51, %rd50, %rd63;
	cvt.u32.u64 	%r36, %rd51;
	add.s32 	%r48, %r48, %r36;
	cvt.u32.u64 	%r47, %rd62;
	add.s32 	%r46, %r46, 1;
	cvt.u64.u32 	%rd52, %r46;
	setp.lt.u64 	%p14, %rd52, %rd24;
	@%p14 bra 	$L__BB4_11;

	cvt.u64.u32 	%rd53, %r48;
	add.s64 	%rd54, %rd2, %rd53;
	ld.global.u8 	%r37, [%rd54];
	cvt.rp.f32.s32 	%f7, %r37;
	cvt.rm.f32.s32 	%f8, %r37;
	cvt.rz.f32.s32 	%f9, %r37;
	setp.neu.f32 	%p15, %f7, %f8;
	mov.b32 	%r38, %f9;
	or.b32  	%r39, %r38, 1;
	mov.b32 	%f10, %r39;
	selp.f32 	%f6, %f10, %f9, %p15;
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs2, %f6;}

	// end inline asm
	shl.b64 	%rd55, %rd60, 1;
	add.s64 	%rd56, %rd1, %rd55;
	st.global.u16 	[%rd56], %rs2;
	add.s32 	%r44, %r44, %r8;
	cvt.u64.u32 	%rd60, %r44;
	setp.lt.u64 	%p16, %rd60, %rd23;
	@%p16 bra 	$L__BB4_10;
	bra.uni 	$L__BB4_17;

$L__BB4_16:
	ld.global.u8 	%r40, [%rd2];
	cvt.rp.f32.s32 	%f12, %r40;
	cvt.rm.f32.s32 	%f13, %r40;
	cvt.rz.f32.s32 	%f14, %r40;
	setp.neu.f32 	%p17, %f12, %f13;
	mov.b32 	%r41, %f14;
	or.b32  	%r42, %r41, 1;
	mov.b32 	%f15, %r42;
	selp.f32 	%f11, %f15, %f14, %p17;
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs3, %f11;}

	// end inline asm
	shl.b64 	%rd57, %rd60, 1;
	add.s64 	%rd58, %rd1, %rd57;
	st.global.u16 	[%rd58], %rs3;
	add.s32 	%r44, %r44, %r8;
	cvt.u64.u32 	%rd60, %r44;
	setp.lt.u64 	%p18, %rd60, %rd23;
	@%p18 bra 	$L__BB4_16;
	bra.uni 	$L__BB4_17;

$L__BB4_5:
	setp.ge.u64 	%p8, %rd60, %rd23;
	@%p8 bra 	$L__BB4_17;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;

$L__BB4_7:
	add.s64 	%rd39, %rd2, %rd60;
	ld.global.u8 	%r24, [%rd39];
	cvt.rp.f32.s32 	%f2, %r24;
	cvt.rm.f32.s32 	%f3, %r24;
	cvt.rz.f32.s32 	%f4, %r24;
	setp.neu.f32 	%p9, %f2, %f3;
	mov.b32 	%r25, %f4;
	or.b32  	%r26, %r25, 1;
	mov.b32 	%f5, %r26;
	selp.f32 	%f1, %f5, %f4, %p9;
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs1, %f1;}

	// end inline asm
	shl.b64 	%rd40, %rd60, 1;
	add.s64 	%rd41, %rd1, %rd40;
	st.global.u16 	[%rd41], %rs1;
	add.s32 	%r44, %r44, %r5;
	cvt.u64.u32 	%rd60, %r44;
	setp.lt.u64 	%p10, %rd60, %rd23;
	@%p10 bra 	$L__BB4_7;

$L__BB4_17:
	ret;

}
	// .globl	cast_u32_bf16
.visible .entry cast_u32_bf16(
	.param .u64 cast_u32_bf16_param_0,
	.param .u64 cast_u32_bf16_param_1,
	.param .u64 cast_u32_bf16_param_2,
	.param .u64 cast_u32_bf16_param_3,
	.param .u64 cast_u32_bf16_param_4
)
{
	.reg .pred 	%p<20>;
	.reg .b16 	%rs<4>;
	.reg .f32 	%f<16>;
	.reg .b32 	%r<50>;
	.reg .b64 	%rd<66>;


	ld.param.u64 	%rd23, [cast_u32_bf16_param_0];
	ld.param.u64 	%rd24, [cast_u32_bf16_param_1];
	ld.param.u64 	%rd25, [cast_u32_bf16_param_2];
	ld.param.u64 	%rd26, [cast_u32_bf16_param_3];
	ld.param.u64 	%rd27, [cast_u32_bf16_param_4];
	cvta.to.global.u64 	%rd1, %rd27;
	cvta.to.global.u64 	%rd2, %rd26;
	cvta.to.global.u64 	%rd3, %rd25;
	setp.eq.s64 	%p3, %rd24, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p19, %p2;
	@%p3 bra 	$L__BB5_4;

	mov.u64 	%rd60, 1;
	mov.u32 	%r43, 0;

$L__BB5_2:
	not.b32 	%r20, %r43;
	cvt.u64.u32 	%rd29, %r20;
	add.s64 	%rd30, %rd29, %rd24;
	and.b64  	%rd5, %rd30, 4294967295;
	add.s64 	%rd31, %rd5, %rd24;
	shl.b64 	%rd32, %rd31, 3;
	add.s64 	%rd33, %rd3, %rd32;
	ld.global.u64 	%rd34, [%rd33];
	setp.ne.s64 	%p5, %rd60, %rd34;
	mov.pred 	%p19, -1;
	@%p5 bra 	$L__BB5_4;

	shl.b64 	%rd35, %rd5, 3;
	add.s64 	%rd36, %rd3, %rd35;
	ld.global.u64 	%rd37, [%rd36];
	mul.lo.s64 	%rd60, %rd37, %rd60;
	add.s32 	%r43, %r43, 1;
	cvt.u64.u32 	%rd38, %r43;
	setp.lt.u64 	%p7, %rd38, %rd24;
	mov.pred 	%p19, %p2;
	@%p7 bra 	$L__BB5_2;

$L__BB5_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r44, %r21, %r3, %r22;
	cvt.u64.u32 	%rd61, %r44;
	@%p19 bra 	$L__BB5_8;
	bra.uni 	$L__BB5_5;

$L__BB5_8:
	setp.ge.u64 	%p11, %rd61, %rd23;
	@%p11 bra 	$L__BB5_17;

	mov.u32 	%r27, %nctaid.x;
	mul.lo.s32 	%r8, %r3, %r27;
	@%p3 bra 	$L__BB5_16;

$L__BB5_10:
	mov.u32 	%r46, 0;
	mov.u32 	%r47, %r44;
	mov.u32 	%r48, %r46;

$L__BB5_11:
	not.b32 	%r30, %r46;
	cvt.u64.u32 	%rd43, %r30;
	add.s64 	%rd44, %rd43, %rd24;
	cvt.u64.u32 	%rd11, %r47;
	shl.b64 	%rd45, %rd44, 3;
	and.b64  	%rd46, %rd45, 34359738360;
	add.s64 	%rd12, %rd3, %rd46;
	ld.global.u64 	%rd13, [%rd12];
	and.b64  	%rd47, %rd13, -4294967296;
	setp.eq.s64 	%p13, %rd47, 0;
	@%p13 bra 	$L__BB5_13;

	div.u64 	%rd63, %rd11, %rd13;
	mul.lo.s64 	%rd48, %rd63, %rd13;
	sub.s64 	%rd64, %rd11, %rd48;
	bra.uni 	$L__BB5_14;

$L__BB5_13:
	cvt.u32.u64 	%r31, %rd13;
	cvt.u32.u64 	%r32, %rd11;
	div.u32 	%r33, %r32, %r31;
	mul.lo.s32 	%r34, %r33, %r31;
	sub.s32 	%r35, %r32, %r34;
	cvt.u64.u32 	%rd63, %r33;
	cvt.u64.u32 	%rd64, %r35;

$L__BB5_14:
	shl.b64 	%rd49, %rd24, 3;
	add.s64 	%rd50, %rd12, %rd49;
	ld.global.u64 	%rd51, [%rd50];
	mul.lo.s64 	%rd52, %rd51, %rd64;
	cvt.u32.u64 	%r36, %rd52;
	add.s32 	%r48, %r48, %r36;
	cvt.u32.u64 	%r47, %rd63;
	add.s32 	%r46, %r46, 1;
	cvt.u64.u32 	%rd53, %r46;
	setp.lt.u64 	%p14, %rd53, %rd24;
	@%p14 bra 	$L__BB5_11;

	mul.wide.u32 	%rd54, %r48, 4;
	add.s64 	%rd55, %rd2, %rd54;
	ld.global.u32 	%r37, [%rd55];
	cvt.rp.f32.u32 	%f7, %r37;
	cvt.rm.f32.u32 	%f8, %r37;
	cvt.rz.f32.u32 	%f9, %r37;
	setp.neu.f32 	%p15, %f7, %f8;
	mov.b32 	%r38, %f9;
	or.b32  	%r39, %r38, 1;
	mov.b32 	%f10, %r39;
	selp.f32 	%f6, %f10, %f9, %p15;
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs2, %f6;}

	// end inline asm
	shl.b64 	%rd56, %rd61, 1;
	add.s64 	%rd57, %rd1, %rd56;
	st.global.u16 	[%rd57], %rs2;
	add.s32 	%r44, %r44, %r8;
	cvt.u64.u32 	%rd61, %r44;
	setp.lt.u64 	%p16, %rd61, %rd23;
	@%p16 bra 	$L__BB5_10;
	bra.uni 	$L__BB5_17;

$L__BB5_16:
	ld.global.u32 	%r40, [%rd2];
	cvt.rp.f32.u32 	%f12, %r40;
	cvt.rm.f32.u32 	%f13, %r40;
	cvt.rz.f32.u32 	%f14, %r40;
	setp.neu.f32 	%p17, %f12, %f13;
	mov.b32 	%r41, %f14;
	or.b32  	%r42, %r41, 1;
	mov.b32 	%f15, %r42;
	selp.f32 	%f11, %f15, %f14, %p17;
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs3, %f11;}

	// end inline asm
	shl.b64 	%rd58, %rd61, 1;
	add.s64 	%rd59, %rd1, %rd58;
	st.global.u16 	[%rd59], %rs3;
	add.s32 	%r44, %r44, %r8;
	cvt.u64.u32 	%rd61, %r44;
	setp.lt.u64 	%p18, %rd61, %rd23;
	@%p18 bra 	$L__BB5_16;
	bra.uni 	$L__BB5_17;

$L__BB5_5:
	setp.ge.u64 	%p8, %rd61, %rd23;
	@%p8 bra 	$L__BB5_17;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;

$L__BB5_7:
	shl.b64 	%rd39, %rd61, 2;
	add.s64 	%rd40, %rd2, %rd39;
	ld.global.u32 	%r24, [%rd40];
	cvt.rp.f32.u32 	%f2, %r24;
	cvt.rm.f32.u32 	%f3, %r24;
	cvt.rz.f32.u32 	%f4, %r24;
	setp.neu.f32 	%p9, %f2, %f3;
	mov.b32 	%r25, %f4;
	or.b32  	%r26, %r25, 1;
	mov.b32 	%f5, %r26;
	selp.f32 	%f1, %f5, %f4, %p9;
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs1, %f1;}

	// end inline asm
	shl.b64 	%rd41, %rd61, 1;
	add.s64 	%rd42, %rd1, %rd41;
	st.global.u16 	[%rd42], %rs1;
	add.s32 	%r44, %r44, %r5;
	cvt.u64.u32 	%rd61, %r44;
	setp.lt.u64 	%p10, %rd61, %rd23;
	@%p10 bra 	$L__BB5_7;

$L__BB5_17:
	ret;

}
	// .globl	cast_f32_bf16
.visible .entry cast_f32_bf16(
	.param .u64 cast_f32_bf16_param_0,
	.param .u64 cast_f32_bf16_param_1,
	.param .u64 cast_f32_bf16_param_2,
	.param .u64 cast_f32_bf16_param_3,
	.param .u64 cast_f32_bf16_param_4
)
{
	.reg .pred 	%p<17>;
	.reg .b16 	%rs<4>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<41>;
	.reg .b64 	%rd<66>;


	ld.param.u64 	%rd23, [cast_f32_bf16_param_0];
	ld.param.u64 	%rd24, [cast_f32_bf16_param_1];
	ld.param.u64 	%rd25, [cast_f32_bf16_param_2];
	ld.param.u64 	%rd26, [cast_f32_bf16_param_3];
	ld.param.u64 	%rd27, [cast_f32_bf16_param_4];
	cvta.to.global.u64 	%rd1, %rd27;
	cvta.to.global.u64 	%rd2, %rd26;
	cvta.to.global.u64 	%rd3, %rd25;
	setp.eq.s64 	%p3, %rd24, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p16, %p2;
	@%p3 bra 	$L__BB6_4;

	mov.u64 	%rd60, 1;
	mov.u32 	%r34, 0;

$L__BB6_2:
	not.b32 	%r20, %r34;
	cvt.u64.u32 	%rd29, %r20;
	add.s64 	%rd30, %rd29, %rd24;
	and.b64  	%rd5, %rd30, 4294967295;
	add.s64 	%rd31, %rd5, %rd24;
	shl.b64 	%rd32, %rd31, 3;
	add.s64 	%rd33, %rd3, %rd32;
	ld.global.u64 	%rd34, [%rd33];
	setp.ne.s64 	%p5, %rd60, %rd34;
	mov.pred 	%p16, -1;
	@%p5 bra 	$L__BB6_4;

	shl.b64 	%rd35, %rd5, 3;
	add.s64 	%rd36, %rd3, %rd35;
	ld.global.u64 	%rd37, [%rd36];
	mul.lo.s64 	%rd60, %rd37, %rd60;
	add.s32 	%r34, %r34, 1;
	cvt.u64.u32 	%rd38, %r34;
	setp.lt.u64 	%p7, %rd38, %rd24;
	mov.pred 	%p16, %p2;
	@%p7 bra 	$L__BB6_2;

$L__BB6_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r35, %r21, %r3, %r22;
	cvt.u64.u32 	%rd61, %r35;
	@%p16 bra 	$L__BB6_8;
	bra.uni 	$L__BB6_5;

$L__BB6_8:
	setp.ge.u64 	%p10, %rd61, %rd23;
	@%p10 bra 	$L__BB6_17;

	mov.u32 	%r24, %nctaid.x;
	mul.lo.s32 	%r8, %r3, %r24;
	@%p3 bra 	$L__BB6_16;

$L__BB6_10:
	mov.u32 	%r37, 0;
	mov.u32 	%r38, %r35;
	mov.u32 	%r39, %r37;

$L__BB6_11:
	not.b32 	%r27, %r37;
	cvt.u64.u32 	%rd43, %r27;
	add.s64 	%rd44, %rd43, %rd24;
	cvt.u64.u32 	%rd11, %r38;
	shl.b64 	%rd45, %rd44, 3;
	and.b64  	%rd46, %rd45, 34359738360;
	add.s64 	%rd12, %rd3, %rd46;
	ld.global.u64 	%rd13, [%rd12];
	and.b64  	%rd47, %rd13, -4294967296;
	setp.eq.s64 	%p12, %rd47, 0;
	@%p12 bra 	$L__BB6_13;

	div.u64 	%rd63, %rd11, %rd13;
	mul.lo.s64 	%rd48, %rd63, %rd13;
	sub.s64 	%rd64, %rd11, %rd48;
	bra.uni 	$L__BB6_14;

$L__BB6_13:
	cvt.u32.u64 	%r28, %rd13;
	cvt.u32.u64 	%r29, %rd11;
	div.u32 	%r30, %r29, %r28;
	mul.lo.s32 	%r31, %r30, %r28;
	sub.s32 	%r32, %r29, %r31;
	cvt.u64.u32 	%rd63, %r30;
	cvt.u64.u32 	%rd64, %r32;

$L__BB6_14:
	shl.b64 	%rd49, %rd24, 3;
	add.s64 	%rd50, %rd12, %rd49;
	ld.global.u64 	%rd51, [%rd50];
	mul.lo.s64 	%rd52, %rd51, %rd64;
	cvt.u32.u64 	%r33, %rd52;
	add.s32 	%r39, %r39, %r33;
	cvt.u32.u64 	%r38, %rd63;
	add.s32 	%r37, %r37, 1;
	cvt.u64.u32 	%rd53, %r37;
	setp.lt.u64 	%p13, %rd53, %rd24;
	@%p13 bra 	$L__BB6_11;

	mul.wide.u32 	%rd54, %r39, 4;
	add.s64 	%rd55, %rd2, %rd54;
	ld.global.f32 	%f2, [%rd55];
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs2, %f2;}

	// end inline asm
	shl.b64 	%rd56, %rd61, 1;
	add.s64 	%rd57, %rd1, %rd56;
	st.global.u16 	[%rd57], %rs2;
	add.s32 	%r35, %r35, %r8;
	cvt.u64.u32 	%rd61, %r35;
	setp.lt.u64 	%p14, %rd61, %rd23;
	@%p14 bra 	$L__BB6_10;
	bra.uni 	$L__BB6_17;

$L__BB6_16:
	ld.global.f32 	%f3, [%rd2];
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs3, %f3;}

	// end inline asm
	shl.b64 	%rd58, %rd61, 1;
	add.s64 	%rd59, %rd1, %rd58;
	st.global.u16 	[%rd59], %rs3;
	add.s32 	%r35, %r35, %r8;
	cvt.u64.u32 	%rd61, %r35;
	setp.lt.u64 	%p15, %rd61, %rd23;
	@%p15 bra 	$L__BB6_16;
	bra.uni 	$L__BB6_17;

$L__BB6_5:
	setp.ge.u64 	%p8, %rd61, %rd23;
	@%p8 bra 	$L__BB6_17;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;

$L__BB6_7:
	shl.b64 	%rd39, %rd61, 2;
	add.s64 	%rd40, %rd2, %rd39;
	ld.global.f32 	%f1, [%rd40];
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs1, %f1;}

	// end inline asm
	shl.b64 	%rd41, %rd61, 1;
	add.s64 	%rd42, %rd1, %rd41;
	st.global.u16 	[%rd42], %rs1;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd61, %r35;
	setp.lt.u64 	%p9, %rd61, %rd23;
	@%p9 bra 	$L__BB6_7;

$L__BB6_17:
	ret;

}
	// .globl	cast_f64_bf16
.visible .entry cast_f64_bf16(
	.param .u64 cast_f64_bf16_param_0,
	.param .u64 cast_f64_bf16_param_1,
	.param .u64 cast_f64_bf16_param_2,
	.param .u64 cast_f64_bf16_param_3,
	.param .u64 cast_f64_bf16_param_4
)
{
	.reg .pred 	%p<44>;
	.reg .b16 	%rs<4>;
	.reg .f32 	%f<10>;
	.reg .b32 	%r<65>;
	.reg .f64 	%fd<7>;
	.reg .b64 	%rd<66>;


	ld.param.u64 	%rd23, [cast_f64_bf16_param_0];
	ld.param.u64 	%rd24, [cast_f64_bf16_param_1];
	ld.param.u64 	%rd25, [cast_f64_bf16_param_2];
	ld.param.u64 	%rd26, [cast_f64_bf16_param_3];
	ld.param.u64 	%rd27, [cast_f64_bf16_param_4];
	cvta.to.global.u64 	%rd1, %rd27;
	cvta.to.global.u64 	%rd2, %rd26;
	cvta.to.global.u64 	%rd3, %rd25;
	setp.eq.s64 	%p3, %rd24, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p43, %p2;
	@%p3 bra 	$L__BB7_4;

	mov.u64 	%rd60, 1;
	mov.u32 	%r58, 0;

$L__BB7_2:
	not.b32 	%r20, %r58;
	cvt.u64.u32 	%rd29, %r20;
	add.s64 	%rd30, %rd29, %rd24;
	and.b64  	%rd5, %rd30, 4294967295;
	add.s64 	%rd31, %rd5, %rd24;
	shl.b64 	%rd32, %rd31, 3;
	add.s64 	%rd33, %rd3, %rd32;
	ld.global.u64 	%rd34, [%rd33];
	setp.ne.s64 	%p5, %rd60, %rd34;
	mov.pred 	%p43, -1;
	@%p5 bra 	$L__BB7_4;

	shl.b64 	%rd35, %rd5, 3;
	add.s64 	%rd36, %rd3, %rd35;
	ld.global.u64 	%rd37, [%rd36];
	mul.lo.s64 	%rd60, %rd37, %rd60;
	add.s32 	%r58, %r58, 1;
	cvt.u64.u32 	%rd38, %r58;
	setp.lt.u64 	%p7, %rd38, %rd24;
	mov.pred 	%p43, %p2;
	@%p7 bra 	$L__BB7_2;

$L__BB7_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r59, %r21, %r3, %r22;
	cvt.u64.u32 	%rd61, %r59;
	@%p43 bra 	$L__BB7_8;
	bra.uni 	$L__BB7_5;

$L__BB7_8:
	setp.ge.u64 	%p19, %rd61, %rd23;
	@%p19 bra 	$L__BB7_17;

	mov.u32 	%r32, %nctaid.x;
	mul.lo.s32 	%r8, %r3, %r32;
	@%p3 bra 	$L__BB7_16;

$L__BB7_10:
	mov.u32 	%r61, 0;
	mov.u32 	%r62, %r59;
	mov.u32 	%r63, %r61;

$L__BB7_11:
	not.b32 	%r35, %r61;
	cvt.u64.u32 	%rd43, %r35;
	add.s64 	%rd44, %rd43, %rd24;
	cvt.u64.u32 	%rd11, %r62;
	shl.b64 	%rd45, %rd44, 3;
	and.b64  	%rd46, %rd45, 34359738360;
	add.s64 	%rd12, %rd3, %rd46;
	ld.global.u64 	%rd13, [%rd12];
	and.b64  	%rd47, %rd13, -4294967296;
	setp.eq.s64 	%p21, %rd47, 0;
	@%p21 bra 	$L__BB7_13;

	div.u64 	%rd63, %rd11, %rd13;
	mul.lo.s64 	%rd48, %rd63, %rd13;
	sub.s64 	%rd64, %rd11, %rd48;
	bra.uni 	$L__BB7_14;

$L__BB7_13:
	cvt.u32.u64 	%r36, %rd13;
	cvt.u32.u64 	%r37, %rd11;
	div.u32 	%r38, %r37, %r36;
	mul.lo.s32 	%r39, %r38, %r36;
	sub.s32 	%r40, %r37, %r39;
	cvt.u64.u32 	%rd63, %r38;
	cvt.u64.u32 	%rd64, %r40;

$L__BB7_14:
	shl.b64 	%rd49, %rd24, 3;
	add.s64 	%rd50, %rd12, %rd49;
	ld.global.u64 	%rd51, [%rd50];
	mul.lo.s64 	%rd52, %rd51, %rd64;
	cvt.u32.u64 	%r41, %rd52;
	add.s32 	%r63, %r63, %r41;
	cvt.u32.u64 	%r62, %rd63;
	add.s32 	%r61, %r61, 1;
	cvt.u64.u32 	%rd53, %r61;
	setp.lt.u64 	%p22, %rd53, %rd24;
	@%p22 bra 	$L__BB7_11;

	mul.wide.u32 	%rd54, %r63, 8;
	add.s64 	%rd55, %rd2, %rd54;
	ld.global.f64 	%fd3, [%rd55];
	// begin inline asm
	cvt.rn.f32.f64 %f4, %fd3;
	// end inline asm
	// begin inline asm
	cvt.f64.f32 %fd4, %f4;
	// end inline asm
	mov.b32 	%r42, %f4;
	shl.b32 	%r43, %r42, 1;
	setp.lt.u32 	%p23, %r43, -16777215;
	setp.gt.f64 	%p24, %fd3, 0d0000000000000000;
	setp.gt.f64 	%p25, %fd4, %fd3;
	and.pred  	%p26, %p24, %p25;
	selp.s32 	%r44, -1, 0, %p26;
	add.s32 	%r45, %r44, %r42;
	setp.lt.f64 	%p27, %fd3, 0d0000000000000000;
	setp.lt.f64 	%p28, %fd4, %fd3;
	and.pred  	%p29, %p27, %p28;
	selp.s32 	%r46, -1, 0, %p29;
	add.s32 	%r47, %r45, %r46;
	setp.neu.f64 	%p30, %fd4, %fd3;
	and.pred  	%p31, %p30, %p23;
	selp.u32 	%r48, 1, 0, %p31;
	or.b32  	%r49, %r47, %r48;
	mov.b32 	%f6, %r49;
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs2, %f6;}

	// end inline asm
	shl.b64 	%rd56, %rd61, 1;
	add.s64 	%rd57, %rd1, %rd56;
	st.global.u16 	[%rd57], %rs2;
	add.s32 	%r59, %r59, %r8;
	cvt.u64.u32 	%rd61, %r59;
	setp.lt.u64 	%p32, %rd61, %rd23;
	@%p32 bra 	$L__BB7_10;
	bra.uni 	$L__BB7_17;

$L__BB7_16:
	ld.global.f64 	%fd5, [%rd2];
	// begin inline asm
	cvt.rn.f32.f64 %f7, %fd5;
	// end inline asm
	// begin inline asm
	cvt.f64.f32 %fd6, %f7;
	// end inline asm
	mov.b32 	%r50, %f7;
	shl.b32 	%r51, %r50, 1;
	setp.lt.u32 	%p33, %r51, -16777215;
	setp.gt.f64 	%p34, %fd5, 0d0000000000000000;
	setp.gt.f64 	%p35, %fd6, %fd5;
	and.pred  	%p36, %p34, %p35;
	selp.s32 	%r52, -1, 0, %p36;
	add.s32 	%r53, %r52, %r50;
	setp.lt.f64 	%p37, %fd5, 0d0000000000000000;
	setp.lt.f64 	%p38, %fd6, %fd5;
	and.pred  	%p39, %p37, %p38;
	selp.s32 	%r54, -1, 0, %p39;
	add.s32 	%r55, %r53, %r54;
	setp.neu.f64 	%p40, %fd6, %fd5;
	and.pred  	%p41, %p40, %p33;
	selp.u32 	%r56, 1, 0, %p41;
	or.b32  	%r57, %r55, %r56;
	mov.b32 	%f9, %r57;
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs3, %f9;}

	// end inline asm
	shl.b64 	%rd58, %rd61, 1;
	add.s64 	%rd59, %rd1, %rd58;
	st.global.u16 	[%rd59], %rs3;
	add.s32 	%r59, %r59, %r8;
	cvt.u64.u32 	%rd61, %r59;
	setp.lt.u64 	%p42, %rd61, %rd23;
	@%p42 bra 	$L__BB7_16;
	bra.uni 	$L__BB7_17;

$L__BB7_5:
	setp.ge.u64 	%p8, %rd61, %rd23;
	@%p8 bra 	$L__BB7_17;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;

$L__BB7_7:
	shl.b64 	%rd39, %rd61, 3;
	add.s64 	%rd40, %rd2, %rd39;
	ld.global.f64 	%fd1, [%rd40];
	// begin inline asm
	cvt.rn.f32.f64 %f1, %fd1;
	// end inline asm
	// begin inline asm
	cvt.f64.f32 %fd2, %f1;
	// end inline asm
	mov.b32 	%r24, %f1;
	shl.b32 	%r25, %r24, 1;
	setp.lt.u32 	%p9, %r25, -16777215;
	setp.gt.f64 	%p10, %fd1, 0d0000000000000000;
	setp.gt.f64 	%p11, %fd2, %fd1;
	and.pred  	%p12, %p10, %p11;
	selp.s32 	%r26, -1, 0, %p12;
	add.s32 	%r27, %r26, %r24;
	setp.lt.f64 	%p13, %fd1, 0d0000000000000000;
	setp.lt.f64 	%p14, %fd2, %fd1;
	and.pred  	%p15, %p13, %p14;
	selp.s32 	%r28, -1, 0, %p15;
	add.s32 	%r29, %r27, %r28;
	setp.neu.f64 	%p16, %fd2, %fd1;
	and.pred  	%p17, %p16, %p9;
	selp.u32 	%r30, 1, 0, %p17;
	or.b32  	%r31, %r29, %r30;
	mov.b32 	%f3, %r31;
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs1, %f3;}

	// end inline asm
	shl.b64 	%rd41, %rd61, 1;
	add.s64 	%rd42, %rd1, %rd41;
	st.global.u16 	[%rd42], %rs1;
	add.s32 	%r59, %r59, %r5;
	cvt.u64.u32 	%rd61, %r59;
	setp.lt.u64 	%p18, %rd61, %rd23;
	@%p18 bra 	$L__BB7_7;

$L__BB7_17:
	ret;

}
	// .globl	cast_bf16_u8
.visible .entry cast_bf16_u8(
	.param .u64 cast_bf16_u8_param_0,
	.param .u64 cast_bf16_u8_param_1,
	.param .u64 cast_bf16_u8_param_2,
	.param .u64 cast_bf16_u8_param_3,
	.param .u64 cast_bf16_u8_param_4
)
{
	.reg .pred 	%p<17>;
	.reg .b16 	%rs<4>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<44>;
	.reg .b64 	%rd<63>;


	ld.param.u64 	%rd23, [cast_bf16_u8_param_0];
	ld.param.u64 	%rd24, [cast_bf16_u8_param_1];
	ld.param.u64 	%rd25, [cast_bf16_u8_param_2];
	ld.param.u64 	%rd26, [cast_bf16_u8_param_3];
	ld.param.u64 	%rd27, [cast_bf16_u8_param_4];
	cvta.to.global.u64 	%rd1, %rd27;
	cvta.to.global.u64 	%rd2, %rd26;
	cvta.to.global.u64 	%rd3, %rd25;
	setp.eq.s64 	%p3, %rd24, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p16, %p2;
	@%p3 bra 	$L__BB8_4;

	mov.u64 	%rd57, 1;
	mov.u32 	%r37, 0;

$L__BB8_2:
	not.b32 	%r20, %r37;
	cvt.u64.u32 	%rd29, %r20;
	add.s64 	%rd30, %rd29, %rd24;
	and.b64  	%rd5, %rd30, 4294967295;
	add.s64 	%rd31, %rd5, %rd24;
	shl.b64 	%rd32, %rd31, 3;
	add.s64 	%rd33, %rd3, %rd32;
	ld.global.u64 	%rd34, [%rd33];
	setp.ne.s64 	%p5, %rd57, %rd34;
	mov.pred 	%p16, -1;
	@%p5 bra 	$L__BB8_4;

	shl.b64 	%rd35, %rd5, 3;
	add.s64 	%rd36, %rd3, %rd35;
	ld.global.u64 	%rd37, [%rd36];
	mul.lo.s64 	%rd57, %rd37, %rd57;
	add.s32 	%r37, %r37, 1;
	cvt.u64.u32 	%rd38, %r37;
	setp.lt.u64 	%p7, %rd38, %rd24;
	mov.pred 	%p16, %p2;
	@%p7 bra 	$L__BB8_2;

$L__BB8_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r38, %r21, %r3, %r22;
	cvt.u64.u32 	%rd58, %r38;
	@%p16 bra 	$L__BB8_8;
	bra.uni 	$L__BB8_5;

$L__BB8_8:
	setp.ge.u64 	%p10, %rd58, %rd23;
	@%p10 bra 	$L__BB8_17;

	mov.u32 	%r25, %nctaid.x;
	mul.lo.s32 	%r8, %r3, %r25;
	@%p3 bra 	$L__BB8_16;

$L__BB8_10:
	mov.u32 	%r40, 0;
	mov.u32 	%r41, %r38;
	mov.u32 	%r42, %r40;

$L__BB8_11:
	not.b32 	%r28, %r40;
	cvt.u64.u32 	%rd42, %r28;
	add.s64 	%rd43, %rd42, %rd24;
	cvt.u64.u32 	%rd11, %r41;
	shl.b64 	%rd44, %rd43, 3;
	and.b64  	%rd45, %rd44, 34359738360;
	add.s64 	%rd12, %rd3, %rd45;
	ld.global.u64 	%rd13, [%rd12];
	and.b64  	%rd46, %rd13, -4294967296;
	setp.eq.s64 	%p12, %rd46, 0;
	@%p12 bra 	$L__BB8_13;

	div.u64 	%rd60, %rd11, %rd13;
	mul.lo.s64 	%rd47, %rd60, %rd13;
	sub.s64 	%rd61, %rd11, %rd47;
	bra.uni 	$L__BB8_14;

$L__BB8_13:
	cvt.u32.u64 	%r29, %rd13;
	cvt.u32.u64 	%r30, %rd11;
	div.u32 	%r31, %r30, %r29;
	mul.lo.s32 	%r32, %r31, %r29;
	sub.s32 	%r33, %r30, %r32;
	cvt.u64.u32 	%rd60, %r31;
	cvt.u64.u32 	%rd61, %r33;

$L__BB8_14:
	shl.b64 	%rd48, %rd24, 3;
	add.s64 	%rd49, %rd12, %rd48;
	ld.global.u64 	%rd50, [%rd49];
	mul.lo.s64 	%rd51, %rd50, %rd61;
	cvt.u32.u64 	%r34, %rd51;
	add.s32 	%r42, %r42, %r34;
	cvt.u32.u64 	%r41, %rd60;
	add.s32 	%r40, %r40, 1;
	cvt.u64.u32 	%rd52, %r40;
	setp.lt.u64 	%p13, %rd52, %rd24;
	@%p13 bra 	$L__BB8_11;

	mul.wide.u32 	%rd53, %r42, 2;
	add.s64 	%rd54, %rd2, %rd53;
	ld.global.u16 	%rs2, [%rd54];
	// begin inline asm
	{ mov.b32 %f2, {0,%rs2};}

	// end inline asm
	cvt.rzi.u32.f32 	%r35, %f2;
	add.s64 	%rd55, %rd1, %rd58;
	st.global.u8 	[%rd55], %r35;
	add.s32 	%r38, %r38, %r8;
	cvt.u64.u32 	%rd58, %r38;
	setp.lt.u64 	%p14, %rd58, %rd23;
	@%p14 bra 	$L__BB8_10;
	bra.uni 	$L__BB8_17;

$L__BB8_16:
	ld.global.u16 	%rs3, [%rd2];
	// begin inline asm
	{ mov.b32 %f3, {0,%rs3};}

	// end inline asm
	cvt.rzi.u32.f32 	%r36, %f3;
	add.s64 	%rd56, %rd1, %rd58;
	st.global.u8 	[%rd56], %r36;
	add.s32 	%r38, %r38, %r8;
	cvt.u64.u32 	%rd58, %r38;
	setp.lt.u64 	%p15, %rd58, %rd23;
	@%p15 bra 	$L__BB8_16;
	bra.uni 	$L__BB8_17;

$L__BB8_5:
	setp.ge.u64 	%p8, %rd58, %rd23;
	@%p8 bra 	$L__BB8_17;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;

$L__BB8_7:
	shl.b64 	%rd39, %rd58, 1;
	add.s64 	%rd40, %rd2, %rd39;
	ld.global.u16 	%rs1, [%rd40];
	// begin inline asm
	{ mov.b32 %f1, {0,%rs1};}

	// end inline asm
	cvt.rzi.u32.f32 	%r24, %f1;
	add.s64 	%rd41, %rd1, %rd58;
	st.global.u8 	[%rd41], %r24;
	add.s32 	%r38, %r38, %r5;
	cvt.u64.u32 	%rd58, %r38;
	setp.lt.u64 	%p9, %rd58, %rd23;
	@%p9 bra 	$L__BB8_7;

$L__BB8_17:
	ret;

}
	// .globl	cast_bf16_f16
.visible .entry cast_bf16_f16(
	.param .u64 cast_bf16_f16_param_0,
	.param .u64 cast_bf16_f16_param_1,
	.param .u64 cast_bf16_f16_param_2,
	.param .u64 cast_bf16_f16_param_3,
	.param .u64 cast_bf16_f16_param_4
)
{
	.reg .pred 	%p<17>;
	.reg .b16 	%rs<7>;
	.reg .f32 	%f<7>;
	.reg .b32 	%r<41>;
	.reg .b64 	%rd<65>;


	ld.param.u64 	%rd23, [cast_bf16_f16_param_0];
	ld.param.u64 	%rd24, [cast_bf16_f16_param_1];
	ld.param.u64 	%rd25, [cast_bf16_f16_param_2];
	ld.param.u64 	%rd26, [cast_bf16_f16_param_3];
	ld.param.u64 	%rd27, [cast_bf16_f16_param_4];
	cvta.to.global.u64 	%rd1, %rd27;
	cvta.to.global.u64 	%rd2, %rd26;
	cvta.to.global.u64 	%rd3, %rd25;
	setp.eq.s64 	%p3, %rd24, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p16, %p2;
	@%p3 bra 	$L__BB9_4;

	mov.u64 	%rd59, 1;
	mov.u32 	%r34, 0;

$L__BB9_2:
	not.b32 	%r20, %r34;
	cvt.u64.u32 	%rd29, %r20;
	add.s64 	%rd30, %rd29, %rd24;
	and.b64  	%rd5, %rd30, 4294967295;
	add.s64 	%rd31, %rd5, %rd24;
	shl.b64 	%rd32, %rd31, 3;
	add.s64 	%rd33, %rd3, %rd32;
	ld.global.u64 	%rd34, [%rd33];
	setp.ne.s64 	%p5, %rd59, %rd34;
	mov.pred 	%p16, -1;
	@%p5 bra 	$L__BB9_4;

	shl.b64 	%rd35, %rd5, 3;
	add.s64 	%rd36, %rd3, %rd35;
	ld.global.u64 	%rd37, [%rd36];
	mul.lo.s64 	%rd59, %rd37, %rd59;
	add.s32 	%r34, %r34, 1;
	cvt.u64.u32 	%rd38, %r34;
	setp.lt.u64 	%p7, %rd38, %rd24;
	mov.pred 	%p16, %p2;
	@%p7 bra 	$L__BB9_2;

$L__BB9_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r35, %r21, %r3, %r22;
	cvt.u64.u32 	%rd60, %r35;
	@%p16 bra 	$L__BB9_8;
	bra.uni 	$L__BB9_5;

$L__BB9_8:
	setp.ge.u64 	%p10, %rd60, %rd23;
	@%p10 bra 	$L__BB9_17;

	mov.u32 	%r24, %nctaid.x;
	mul.lo.s32 	%r8, %r3, %r24;
	@%p3 bra 	$L__BB9_16;

$L__BB9_10:
	mov.u32 	%r37, 0;
	mov.u32 	%r38, %r35;
	mov.u32 	%r39, %r37;

$L__BB9_11:
	not.b32 	%r27, %r37;
	cvt.u64.u32 	%rd42, %r27;
	add.s64 	%rd43, %rd42, %rd24;
	cvt.u64.u32 	%rd11, %r38;
	shl.b64 	%rd44, %rd43, 3;
	and.b64  	%rd45, %rd44, 34359738360;
	add.s64 	%rd12, %rd3, %rd45;
	ld.global.u64 	%rd13, [%rd12];
	and.b64  	%rd46, %rd13, -4294967296;
	setp.eq.s64 	%p12, %rd46, 0;
	@%p12 bra 	$L__BB9_13;

	div.u64 	%rd62, %rd11, %rd13;
	mul.lo.s64 	%rd47, %rd62, %rd13;
	sub.s64 	%rd63, %rd11, %rd47;
	bra.uni 	$L__BB9_14;

$L__BB9_13:
	cvt.u32.u64 	%r28, %rd13;
	cvt.u32.u64 	%r29, %rd11;
	div.u32 	%r30, %r29, %r28;
	mul.lo.s32 	%r31, %r30, %r28;
	sub.s32 	%r32, %r29, %r31;
	cvt.u64.u32 	%rd62, %r30;
	cvt.u64.u32 	%rd63, %r32;

$L__BB9_14:
	shl.b64 	%rd48, %rd24, 3;
	add.s64 	%rd49, %rd12, %rd48;
	ld.global.u64 	%rd50, [%rd49];
	mul.lo.s64 	%rd51, %rd50, %rd63;
	cvt.u32.u64 	%r33, %rd51;
	add.s32 	%r39, %r39, %r33;
	cvt.u32.u64 	%r38, %rd62;
	add.s32 	%r37, %r37, 1;
	cvt.u64.u32 	%rd52, %r37;
	setp.lt.u64 	%p13, %rd52, %rd24;
	@%p13 bra 	$L__BB9_11;

	mul.wide.u32 	%rd53, %r39, 2;
	add.s64 	%rd54, %rd2, %rd53;
	ld.global.u16 	%rs3, [%rd54];
	// begin inline asm
	{ mov.b32 %f3, {0,%rs3};}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs4, %f3;}

	// end inline asm
	shl.b64 	%rd55, %rd60, 1;
	add.s64 	%rd56, %rd1, %rd55;
	st.global.u16 	[%rd56], %rs4;
	add.s32 	%r35, %r35, %r8;
	cvt.u64.u32 	%rd60, %r35;
	setp.lt.u64 	%p14, %rd60, %rd23;
	@%p14 bra 	$L__BB9_10;
	bra.uni 	$L__BB9_17;

$L__BB9_16:
	ld.global.u16 	%rs5, [%rd2];
	// begin inline asm
	{ mov.b32 %f5, {0,%rs5};}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs6, %f5;}

	// end inline asm
	shl.b64 	%rd57, %rd60, 1;
	add.s64 	%rd58, %rd1, %rd57;
	st.global.u16 	[%rd58], %rs6;
	add.s32 	%r35, %r35, %r8;
	cvt.u64.u32 	%rd60, %r35;
	setp.lt.u64 	%p15, %rd60, %rd23;
	@%p15 bra 	$L__BB9_16;
	bra.uni 	$L__BB9_17;

$L__BB9_5:
	setp.ge.u64 	%p8, %rd60, %rd23;
	@%p8 bra 	$L__BB9_17;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;

$L__BB9_7:
	shl.b64 	%rd39, %rd60, 1;
	add.s64 	%rd40, %rd2, %rd39;
	ld.global.u16 	%rs1, [%rd40];
	// begin inline asm
	{ mov.b32 %f1, {0,%rs1};}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs2, %f1;}

	// end inline asm
	add.s64 	%rd41, %rd1, %rd39;
	st.global.u16 	[%rd41], %rs2;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd60, %r35;
	setp.lt.u64 	%p9, %rd60, %rd23;
	@%p9 bra 	$L__BB9_7;

$L__BB9_17:
	ret;

}
	// .globl	cast_f16_bf16
.visible .entry cast_f16_bf16(
	.param .u64 cast_f16_bf16_param_0,
	.param .u64 cast_f16_bf16_param_1,
	.param .u64 cast_f16_bf16_param_2,
	.param .u64 cast_f16_bf16_param_3,
	.param .u64 cast_f16_bf16_param_4
)
{
	.reg .pred 	%p<17>;
	.reg .b16 	%rs<7>;
	.reg .f32 	%f<7>;
	.reg .b32 	%r<41>;
	.reg .b64 	%rd<65>;


	ld.param.u64 	%rd23, [cast_f16_bf16_param_0];
	ld.param.u64 	%rd24, [cast_f16_bf16_param_1];
	ld.param.u64 	%rd25, [cast_f16_bf16_param_2];
	ld.param.u64 	%rd26, [cast_f16_bf16_param_3];
	ld.param.u64 	%rd27, [cast_f16_bf16_param_4];
	cvta.to.global.u64 	%rd1, %rd27;
	cvta.to.global.u64 	%rd2, %rd26;
	cvta.to.global.u64 	%rd3, %rd25;
	setp.eq.s64 	%p3, %rd24, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p16, %p2;
	@%p3 bra 	$L__BB10_4;

	mov.u64 	%rd59, 1;
	mov.u32 	%r34, 0;

$L__BB10_2:
	not.b32 	%r20, %r34;
	cvt.u64.u32 	%rd29, %r20;
	add.s64 	%rd30, %rd29, %rd24;
	and.b64  	%rd5, %rd30, 4294967295;
	add.s64 	%rd31, %rd5, %rd24;
	shl.b64 	%rd32, %rd31, 3;
	add.s64 	%rd33, %rd3, %rd32;
	ld.global.u64 	%rd34, [%rd33];
	setp.ne.s64 	%p5, %rd59, %rd34;
	mov.pred 	%p16, -1;
	@%p5 bra 	$L__BB10_4;

	shl.b64 	%rd35, %rd5, 3;
	add.s64 	%rd36, %rd3, %rd35;
	ld.global.u64 	%rd37, [%rd36];
	mul.lo.s64 	%rd59, %rd37, %rd59;
	add.s32 	%r34, %r34, 1;
	cvt.u64.u32 	%rd38, %r34;
	setp.lt.u64 	%p7, %rd38, %rd24;
	mov.pred 	%p16, %p2;
	@%p7 bra 	$L__BB10_2;

$L__BB10_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r35, %r21, %r3, %r22;
	cvt.u64.u32 	%rd60, %r35;
	@%p16 bra 	$L__BB10_8;
	bra.uni 	$L__BB10_5;

$L__BB10_8:
	setp.ge.u64 	%p10, %rd60, %rd23;
	@%p10 bra 	$L__BB10_17;

	mov.u32 	%r24, %nctaid.x;
	mul.lo.s32 	%r8, %r3, %r24;
	@%p3 bra 	$L__BB10_16;

$L__BB10_10:
	mov.u32 	%r37, 0;
	mov.u32 	%r38, %r35;
	mov.u32 	%r39, %r37;

$L__BB10_11:
	not.b32 	%r27, %r37;
	cvt.u64.u32 	%rd42, %r27;
	add.s64 	%rd43, %rd42, %rd24;
	cvt.u64.u32 	%rd11, %r38;
	shl.b64 	%rd44, %rd43, 3;
	and.b64  	%rd45, %rd44, 34359738360;
	add.s64 	%rd12, %rd3, %rd45;
	ld.global.u64 	%rd13, [%rd12];
	and.b64  	%rd46, %rd13, -4294967296;
	setp.eq.s64 	%p12, %rd46, 0;
	@%p12 bra 	$L__BB10_13;

	div.u64 	%rd62, %rd11, %rd13;
	mul.lo.s64 	%rd47, %rd62, %rd13;
	sub.s64 	%rd63, %rd11, %rd47;
	bra.uni 	$L__BB10_14;

$L__BB10_13:
	cvt.u32.u64 	%r28, %rd13;
	cvt.u32.u64 	%r29, %rd11;
	div.u32 	%r30, %r29, %r28;
	mul.lo.s32 	%r31, %r30, %r28;
	sub.s32 	%r32, %r29, %r31;
	cvt.u64.u32 	%rd62, %r30;
	cvt.u64.u32 	%rd63, %r32;

$L__BB10_14:
	shl.b64 	%rd48, %rd24, 3;
	add.s64 	%rd49, %rd12, %rd48;
	ld.global.u64 	%rd50, [%rd49];
	mul.lo.s64 	%rd51, %rd50, %rd63;
	cvt.u32.u64 	%r33, %rd51;
	add.s32 	%r39, %r39, %r33;
	cvt.u32.u64 	%r38, %rd62;
	add.s32 	%r37, %r37, 1;
	cvt.u64.u32 	%rd52, %r37;
	setp.lt.u64 	%p13, %rd52, %rd24;
	@%p13 bra 	$L__BB10_11;

	mul.wide.u32 	%rd53, %r39, 2;
	add.s64 	%rd54, %rd2, %rd53;
	ld.global.u16 	%rs3, [%rd54];
	// begin inline asm
	{  cvt.f32.f16 %f3, %rs3;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs4, %f3;}

	// end inline asm
	shl.b64 	%rd55, %rd60, 1;
	add.s64 	%rd56, %rd1, %rd55;
	st.global.u16 	[%rd56], %rs4;
	add.s32 	%r35, %r35, %r8;
	cvt.u64.u32 	%rd60, %r35;
	setp.lt.u64 	%p14, %rd60, %rd23;
	@%p14 bra 	$L__BB10_10;
	bra.uni 	$L__BB10_17;

$L__BB10_16:
	ld.global.u16 	%rs5, [%rd2];
	// begin inline asm
	{  cvt.f32.f16 %f5, %rs5;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs6, %f5;}

	// end inline asm
	shl.b64 	%rd57, %rd60, 1;
	add.s64 	%rd58, %rd1, %rd57;
	st.global.u16 	[%rd58], %rs6;
	add.s32 	%r35, %r35, %r8;
	cvt.u64.u32 	%rd60, %r35;
	setp.lt.u64 	%p15, %rd60, %rd23;
	@%p15 bra 	$L__BB10_16;
	bra.uni 	$L__BB10_17;

$L__BB10_5:
	setp.ge.u64 	%p8, %rd60, %rd23;
	@%p8 bra 	$L__BB10_17;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;

$L__BB10_7:
	shl.b64 	%rd39, %rd60, 1;
	add.s64 	%rd40, %rd2, %rd39;
	ld.global.u16 	%rs1, [%rd40];
	// begin inline asm
	{  cvt.f32.f16 %f1, %rs1;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.bf16.f32 %rs2, %f1;}

	// end inline asm
	add.s64 	%rd41, %rd1, %rd39;
	st.global.u16 	[%rd41], %rs2;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd60, %r35;
	setp.lt.u64 	%p9, %rd60, %rd23;
	@%p9 bra 	$L__BB10_7;

$L__BB10_17:
	ret;

}
	// .globl	cast_f16_f16
.visible .entry cast_f16_f16(
	.param .u64 cast_f16_f16_param_0,
	.param .u64 cast_f16_f16_param_1,
	.param .u64 cast_f16_f16_param_2,
	.param .u64 cast_f16_f16_param_3,
	.param .u64 cast_f16_f16_param_4
)
{
	.reg .pred 	%p<17>;
	.reg .b16 	%rs<4>;
	.reg .b32 	%r<41>;
	.reg .b64 	%rd<65>;


	ld.param.u64 	%rd23, [cast_f16_f16_param_0];
	ld.param.u64 	%rd24, [cast_f16_f16_param_1];
	ld.param.u64 	%rd25, [cast_f16_f16_param_2];
	ld.param.u64 	%rd26, [cast_f16_f16_param_3];
	ld.param.u64 	%rd27, [cast_f16_f16_param_4];
	cvta.to.global.u64 	%rd1, %rd27;
	cvta.to.global.u64 	%rd2, %rd26;
	cvta.to.global.u64 	%rd3, %rd25;
	setp.eq.s64 	%p3, %rd24, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p16, %p2;
	@%p3 bra 	$L__BB11_4;

	mov.u64 	%rd59, 1;
	mov.u32 	%r34, 0;

$L__BB11_2:
	not.b32 	%r20, %r34;
	cvt.u64.u32 	%rd29, %r20;
	add.s64 	%rd30, %rd29, %rd24;
	and.b64  	%rd5, %rd30, 4294967295;
	add.s64 	%rd31, %rd5, %rd24;
	shl.b64 	%rd32, %rd31, 3;
	add.s64 	%rd33, %rd3, %rd32;
	ld.global.u64 	%rd34, [%rd33];
	setp.ne.s64 	%p5, %rd59, %rd34;
	mov.pred 	%p16, -1;
	@%p5 bra 	$L__BB11_4;

	shl.b64 	%rd35, %rd5, 3;
	add.s64 	%rd36, %rd3, %rd35;
	ld.global.u64 	%rd37, [%rd36];
	mul.lo.s64 	%rd59, %rd37, %rd59;
	add.s32 	%r34, %r34, 1;
	cvt.u64.u32 	%rd38, %r34;
	setp.lt.u64 	%p7, %rd38, %rd24;
	mov.pred 	%p16, %p2;
	@%p7 bra 	$L__BB11_2;

$L__BB11_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r35, %r21, %r3, %r22;
	cvt.u64.u32 	%rd60, %r35;
	@%p16 bra 	$L__BB11_8;
	bra.uni 	$L__BB11_5;

$L__BB11_8:
	setp.ge.u64 	%p10, %rd60, %rd23;
	@%p10 bra 	$L__BB11_17;

	mov.u32 	%r24, %nctaid.x;
	mul.lo.s32 	%r8, %r3, %r24;
	@%p3 bra 	$L__BB11_16;

$L__BB11_10:
	mov.u32 	%r37, 0;
	mov.u32 	%r38, %r35;
	mov.u32 	%r39, %r37;

$L__BB11_11:
	not.b32 	%r27, %r37;
	cvt.u64.u32 	%rd42, %r27;
	add.s64 	%rd43, %rd42, %rd24;
	cvt.u64.u32 	%rd11, %r38;
	shl.b64 	%rd44, %rd43, 3;
	and.b64  	%rd45, %rd44, 34359738360;
	add.s64 	%rd12, %rd3, %rd45;
	ld.global.u64 	%rd13, [%rd12];
	and.b64  	%rd46, %rd13, -4294967296;
	setp.eq.s64 	%p12, %rd46, 0;
	@%p12 bra 	$L__BB11_13;

	div.u64 	%rd62, %rd11, %rd13;
	mul.lo.s64 	%rd47, %rd62, %rd13;
	sub.s64 	%rd63, %rd11, %rd47;
	bra.uni 	$L__BB11_14;

$L__BB11_13:
	cvt.u32.u64 	%r28, %rd13;
	cvt.u32.u64 	%r29, %rd11;
	div.u32 	%r30, %r29, %r28;
	mul.lo.s32 	%r31, %r30, %r28;
	sub.s32 	%r32, %r29, %r31;
	cvt.u64.u32 	%rd62, %r30;
	cvt.u64.u32 	%rd63, %r32;

$L__BB11_14:
	shl.b64 	%rd48, %rd24, 3;
	add.s64 	%rd49, %rd12, %rd48;
	ld.global.u64 	%rd50, [%rd49];
	mul.lo.s64 	%rd51, %rd50, %rd63;
	cvt.u32.u64 	%r33, %rd51;
	add.s32 	%r39, %r39, %r33;
	cvt.u32.u64 	%r38, %rd62;
	add.s32 	%r37, %r37, 1;
	cvt.u64.u32 	%rd52, %r37;
	setp.lt.u64 	%p13, %rd52, %rd24;
	@%p13 bra 	$L__BB11_11;

	mul.wide.u32 	%rd53, %r39, 2;
	add.s64 	%rd54, %rd2, %rd53;
	ld.global.u16 	%rs2, [%rd54];
	shl.b64 	%rd55, %rd60, 1;
	add.s64 	%rd56, %rd1, %rd55;
	st.global.u16 	[%rd56], %rs2;
	add.s32 	%r35, %r35, %r8;
	cvt.u64.u32 	%rd60, %r35;
	setp.lt.u64 	%p14, %rd60, %rd23;
	@%p14 bra 	$L__BB11_10;
	bra.uni 	$L__BB11_17;

$L__BB11_16:
	ld.global.u16 	%rs3, [%rd2];
	shl.b64 	%rd57, %rd60, 1;
	add.s64 	%rd58, %rd1, %rd57;
	st.global.u16 	[%rd58], %rs3;
	add.s32 	%r35, %r35, %r8;
	cvt.u64.u32 	%rd60, %r35;
	setp.lt.u64 	%p15, %rd60, %rd23;
	@%p15 bra 	$L__BB11_16;
	bra.uni 	$L__BB11_17;

$L__BB11_5:
	setp.ge.u64 	%p8, %rd60, %rd23;
	@%p8 bra 	$L__BB11_17;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;

$L__BB11_7:
	shl.b64 	%rd39, %rd60, 1;
	add.s64 	%rd40, %rd2, %rd39;
	ld.global.u16 	%rs1, [%rd40];
	add.s64 	%rd41, %rd1, %rd39;
	st.global.u16 	[%rd41], %rs1;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd60, %r35;
	setp.lt.u64 	%p9, %rd60, %rd23;
	@%p9 bra 	$L__BB11_7;

$L__BB11_17:
	ret;

}
	// .globl	cast_f16_u8
.visible .entry cast_f16_u8(
	.param .u64 cast_f16_u8_param_0,
	.param .u64 cast_f16_u8_param_1,
	.param .u64 cast_f16_u8_param_2,
	.param .u64 cast_f16_u8_param_3,
	.param .u64 cast_f16_u8_param_4
)
{
	.reg .pred 	%p<17>;
	.reg .b16 	%rs<4>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<44>;
	.reg .b64 	%rd<63>;


	ld.param.u64 	%rd23, [cast_f16_u8_param_0];
	ld.param.u64 	%rd24, [cast_f16_u8_param_1];
	ld.param.u64 	%rd25, [cast_f16_u8_param_2];
	ld.param.u64 	%rd26, [cast_f16_u8_param_3];
	ld.param.u64 	%rd27, [cast_f16_u8_param_4];
	cvta.to.global.u64 	%rd1, %rd27;
	cvta.to.global.u64 	%rd2, %rd26;
	cvta.to.global.u64 	%rd3, %rd25;
	setp.eq.s64 	%p3, %rd24, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p16, %p2;
	@%p3 bra 	$L__BB12_4;

	mov.u64 	%rd57, 1;
	mov.u32 	%r37, 0;

$L__BB12_2:
	not.b32 	%r20, %r37;
	cvt.u64.u32 	%rd29, %r20;
	add.s64 	%rd30, %rd29, %rd24;
	and.b64  	%rd5, %rd30, 4294967295;
	add.s64 	%rd31, %rd5, %rd24;
	shl.b64 	%rd32, %rd31, 3;
	add.s64 	%rd33, %rd3, %rd32;
	ld.global.u64 	%rd34, [%rd33];
	setp.ne.s64 	%p5, %rd57, %rd34;
	mov.pred 	%p16, -1;
	@%p5 bra 	$L__BB12_4;

	shl.b64 	%rd35, %rd5, 3;
	add.s64 	%rd36, %rd3, %rd35;
	ld.global.u64 	%rd37, [%rd36];
	mul.lo.s64 	%rd57, %rd37, %rd57;
	add.s32 	%r37, %r37, 1;
	cvt.u64.u32 	%rd38, %r37;
	setp.lt.u64 	%p7, %rd38, %rd24;
	mov.pred 	%p16, %p2;
	@%p7 bra 	$L__BB12_2;

$L__BB12_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r38, %r21, %r3, %r22;
	cvt.u64.u32 	%rd58, %r38;
	@%p16 bra 	$L__BB12_8;
	bra.uni 	$L__BB12_5;

$L__BB12_8:
	setp.ge.u64 	%p10, %rd58, %rd23;
	@%p10 bra 	$L__BB12_17;

	mov.u32 	%r25, %nctaid.x;
	mul.lo.s32 	%r8, %r3, %r25;
	@%p3 bra 	$L__BB12_16;

$L__BB12_10:
	mov.u32 	%r40, 0;
	mov.u32 	%r41, %r38;
	mov.u32 	%r42, %r40;

$L__BB12_11:
	not.b32 	%r28, %r40;
	cvt.u64.u32 	%rd42, %r28;
	add.s64 	%rd43, %rd42, %rd24;
	cvt.u64.u32 	%rd11, %r41;
	shl.b64 	%rd44, %rd43, 3;
	and.b64  	%rd45, %rd44, 34359738360;
	add.s64 	%rd12, %rd3, %rd45;
	ld.global.u64 	%rd13, [%rd12];
	and.b64  	%rd46, %rd13, -4294967296;
	setp.eq.s64 	%p12, %rd46, 0;
	@%p12 bra 	$L__BB12_13;

	div.u64 	%rd60, %rd11, %rd13;
	mul.lo.s64 	%rd47, %rd60, %rd13;
	sub.s64 	%rd61, %rd11, %rd47;
	bra.uni 	$L__BB12_14;

$L__BB12_13:
	cvt.u32.u64 	%r29, %rd13;
	cvt.u32.u64 	%r30, %rd11;
	div.u32 	%r31, %r30, %r29;
	mul.lo.s32 	%r32, %r31, %r29;
	sub.s32 	%r33, %r30, %r32;
	cvt.u64.u32 	%rd60, %r31;
	cvt.u64.u32 	%rd61, %r33;

$L__BB12_14:
	shl.b64 	%rd48, %rd24, 3;
	add.s64 	%rd49, %rd12, %rd48;
	ld.global.u64 	%rd50, [%rd49];
	mul.lo.s64 	%rd51, %rd50, %rd61;
	cvt.u32.u64 	%r34, %rd51;
	add.s32 	%r42, %r42, %r34;
	cvt.u32.u64 	%r41, %rd60;
	add.s32 	%r40, %r40, 1;
	cvt.u64.u32 	%rd52, %r40;
	setp.lt.u64 	%p13, %rd52, %rd24;
	@%p13 bra 	$L__BB12_11;

	mul.wide.u32 	%rd53, %r42, 2;
	add.s64 	%rd54, %rd2, %rd53;
	ld.global.u16 	%rs2, [%rd54];
	// begin inline asm
	{  cvt.f32.f16 %f2, %rs2;}

	// end inline asm
	cvt.rzi.u32.f32 	%r35, %f2;
	add.s64 	%rd55, %rd1, %rd58;
	st.global.u8 	[%rd55], %r35;
	add.s32 	%r38, %r38, %r8;
	cvt.u64.u32 	%rd58, %r38;
	setp.lt.u64 	%p14, %rd58, %rd23;
	@%p14 bra 	$L__BB12_10;
	bra.uni 	$L__BB12_17;

$L__BB12_16:
	ld.global.u16 	%rs3, [%rd2];
	// begin inline asm
	{  cvt.f32.f16 %f3, %rs3;}

	// end inline asm
	cvt.rzi.u32.f32 	%r36, %f3;
	add.s64 	%rd56, %rd1, %rd58;
	st.global.u8 	[%rd56], %r36;
	add.s32 	%r38, %r38, %r8;
	cvt.u64.u32 	%rd58, %r38;
	setp.lt.u64 	%p15, %rd58, %rd23;
	@%p15 bra 	$L__BB12_16;
	bra.uni 	$L__BB12_17;

$L__BB12_5:
	setp.ge.u64 	%p8, %rd58, %rd23;
	@%p8 bra 	$L__BB12_17;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;

$L__BB12_7:
	shl.b64 	%rd39, %rd58, 1;
	add.s64 	%rd40, %rd2, %rd39;
	ld.global.u16 	%rs1, [%rd40];
	// begin inline asm
	{  cvt.f32.f16 %f1, %rs1;}

	// end inline asm
	cvt.rzi.u32.f32 	%r24, %f1;
	add.s64 	%rd41, %rd1, %rd58;
	st.global.u8 	[%rd41], %r24;
	add.s32 	%r38, %r38, %r5;
	cvt.u64.u32 	%rd58, %r38;
	setp.lt.u64 	%p9, %rd58, %rd23;
	@%p9 bra 	$L__BB12_7;

$L__BB12_17:
	ret;

}
	// .globl	cast_f16_u32
.visible .entry cast_f16_u32(
	.param .u64 cast_f16_u32_param_0,
	.param .u64 cast_f16_u32_param_1,
	.param .u64 cast_f16_u32_param_2,
	.param .u64 cast_f16_u32_param_3,
	.param .u64 cast_f16_u32_param_4
)
{
	.reg .pred 	%p<17>;
	.reg .b16 	%rs<4>;
	.reg .b32 	%r<44>;
	.reg .b64 	%rd<66>;


	ld.param.u64 	%rd23, [cast_f16_u32_param_0];
	ld.param.u64 	%rd24, [cast_f16_u32_param_1];
	ld.param.u64 	%rd25, [cast_f16_u32_param_2];
	ld.param.u64 	%rd26, [cast_f16_u32_param_3];
	ld.param.u64 	%rd27, [cast_f16_u32_param_4];
	cvta.to.global.u64 	%rd1, %rd27;
	cvta.to.global.u64 	%rd2, %rd26;
	cvta.to.global.u64 	%rd3, %rd25;
	setp.eq.s64 	%p3, %rd24, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p16, %p2;
	@%p3 bra 	$L__BB13_4;

	mov.u64 	%rd60, 1;
	mov.u32 	%r37, 0;

$L__BB13_2:
	not.b32 	%r20, %r37;
	cvt.u64.u32 	%rd29, %r20;
	add.s64 	%rd30, %rd29, %rd24;
	and.b64  	%rd5, %rd30, 4294967295;
	add.s64 	%rd31, %rd5, %rd24;
	shl.b64 	%rd32, %rd31, 3;
	add.s64 	%rd33, %rd3, %rd32;
	ld.global.u64 	%rd34, [%rd33];
	setp.ne.s64 	%p5, %rd60, %rd34;
	mov.pred 	%p16, -1;
	@%p5 bra 	$L__BB13_4;

	shl.b64 	%rd35, %rd5, 3;
	add.s64 	%rd36, %rd3, %rd35;
	ld.global.u64 	%rd37, [%rd36];
	mul.lo.s64 	%rd60, %rd37, %rd60;
	add.s32 	%r37, %r37, 1;
	cvt.u64.u32 	%rd38, %r37;
	setp.lt.u64 	%p7, %rd38, %rd24;
	mov.pred 	%p16, %p2;
	@%p7 bra 	$L__BB13_2;

$L__BB13_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r38, %r21, %r3, %r22;
	cvt.u64.u32 	%rd61, %r38;
	@%p16 bra 	$L__BB13_8;
	bra.uni 	$L__BB13_5;

$L__BB13_8:
	setp.ge.u64 	%p10, %rd61, %rd23;
	@%p10 bra 	$L__BB13_17;

	mov.u32 	%r25, %nctaid.x;
	mul.lo.s32 	%r8, %r3, %r25;
	@%p3 bra 	$L__BB13_16;

$L__BB13_10:
	mov.u32 	%r40, 0;
	mov.u32 	%r41, %r38;
	mov.u32 	%r42, %r40;

$L__BB13_11:
	not.b32 	%r28, %r40;
	cvt.u64.u32 	%rd43, %r28;
	add.s64 	%rd44, %rd43, %rd24;
	cvt.u64.u32 	%rd11, %r41;
	shl.b64 	%rd45, %rd44, 3;
	and.b64  	%rd46, %rd45, 34359738360;
	add.s64 	%rd12, %rd3, %rd46;
	ld.global.u64 	%rd13, [%rd12];
	and.b64  	%rd47, %rd13, -4294967296;
	setp.eq.s64 	%p12, %rd47, 0;
	@%p12 bra 	$L__BB13_13;

	div.u64 	%rd63, %rd11, %rd13;
	mul.lo.s64 	%rd48, %rd63, %rd13;
	sub.s64 	%rd64, %rd11, %rd48;
	bra.uni 	$L__BB13_14;

$L__BB13_13:
	cvt.u32.u64 	%r29, %rd13;
	cvt.u32.u64 	%r30, %rd11;
	div.u32 	%r31, %r30, %r29;
	mul.lo.s32 	%r32, %r31, %r29;
	sub.s32 	%r33, %r30, %r32;
	cvt.u64.u32 	%rd63, %r31;
	cvt.u64.u32 	%rd64, %r33;

$L__BB13_14:
	shl.b64 	%rd49, %rd24, 3;
	add.s64 	%rd50, %rd12, %rd49;
	ld.global.u64 	%rd51, [%rd50];
	mul.lo.s64 	%rd52, %rd51, %rd64;
	cvt.u32.u64 	%r34, %rd52;
	add.s32 	%r42, %r42, %r34;
	cvt.u32.u64 	%r41, %rd63;
	add.s32 	%r40, %r40, 1;
	cvt.u64.u32 	%rd53, %r40;
	setp.lt.u64 	%p13, %rd53, %rd24;
	@%p13 bra 	$L__BB13_11;

	mul.wide.u32 	%rd54, %r42, 2;
	add.s64 	%rd55, %rd2, %rd54;
	ld.global.u16 	%rs2, [%rd55];
	// begin inline asm
	cvt.rzi.u32.f16 %r35, %rs2;
	// end inline asm
	shl.b64 	%rd56, %rd61, 2;
	add.s64 	%rd57, %rd1, %rd56;
	st.global.u32 	[%rd57], %r35;
	add.s32 	%r38, %r38, %r8;
	cvt.u64.u32 	%rd61, %r38;
	setp.lt.u64 	%p14, %rd61, %rd23;
	@%p14 bra 	$L__BB13_10;
	bra.uni 	$L__BB13_17;

$L__BB13_16:
	ld.global.u16 	%rs3, [%rd2];
	// begin inline asm
	cvt.rzi.u32.f16 %r36, %rs3;
	// end inline asm
	shl.b64 	%rd58, %rd61, 2;
	add.s64 	%rd59, %rd1, %rd58;
	st.global.u32 	[%rd59], %r36;
	add.s32 	%r38, %r38, %r8;
	cvt.u64.u32 	%rd61, %r38;
	setp.lt.u64 	%p15, %rd61, %rd23;
	@%p15 bra 	$L__BB13_16;
	bra.uni 	$L__BB13_17;

$L__BB13_5:
	setp.ge.u64 	%p8, %rd61, %rd23;
	@%p8 bra 	$L__BB13_17;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;

$L__BB13_7:
	shl.b64 	%rd39, %rd61, 1;
	add.s64 	%rd40, %rd2, %rd39;
	ld.global.u16 	%rs1, [%rd40];
	// begin inline asm
	cvt.rzi.u32.f16 %r24, %rs1;
	// end inline asm
	shl.b64 	%rd41, %rd61, 2;
	add.s64 	%rd42, %rd1, %rd41;
	st.global.u32 	[%rd42], %r24;
	add.s32 	%r38, %r38, %r5;
	cvt.u64.u32 	%rd61, %r38;
	setp.lt.u64 	%p9, %rd61, %rd23;
	@%p9 bra 	$L__BB13_7;

$L__BB13_17:
	ret;

}
	// .globl	cast_f16_f32
.visible .entry cast_f16_f32(
	.param .u64 cast_f16_f32_param_0,
	.param .u64 cast_f16_f32_param_1,
	.param .u64 cast_f16_f32_param_2,
	.param .u64 cast_f16_f32_param_3,
	.param .u64 cast_f16_f32_param_4
)
{
	.reg .pred 	%p<17>;
	.reg .b16 	%rs<4>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<41>;
	.reg .b64 	%rd<66>;


	ld.param.u64 	%rd23, [cast_f16_f32_param_0];
	ld.param.u64 	%rd24, [cast_f16_f32_param_1];
	ld.param.u64 	%rd25, [cast_f16_f32_param_2];
	ld.param.u64 	%rd26, [cast_f16_f32_param_3];
	ld.param.u64 	%rd27, [cast_f16_f32_param_4];
	cvta.to.global.u64 	%rd1, %rd27;
	cvta.to.global.u64 	%rd2, %rd26;
	cvta.to.global.u64 	%rd3, %rd25;
	setp.eq.s64 	%p3, %rd24, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p16, %p2;
	@%p3 bra 	$L__BB14_4;

	mov.u64 	%rd60, 1;
	mov.u32 	%r34, 0;

$L__BB14_2:
	not.b32 	%r20, %r34;
	cvt.u64.u32 	%rd29, %r20;
	add.s64 	%rd30, %rd29, %rd24;
	and.b64  	%rd5, %rd30, 4294967295;
	add.s64 	%rd31, %rd5, %rd24;
	shl.b64 	%rd32, %rd31, 3;
	add.s64 	%rd33, %rd3, %rd32;
	ld.global.u64 	%rd34, [%rd33];
	setp.ne.s64 	%p5, %rd60, %rd34;
	mov.pred 	%p16, -1;
	@%p5 bra 	$L__BB14_4;

	shl.b64 	%rd35, %rd5, 3;
	add.s64 	%rd36, %rd3, %rd35;
	ld.global.u64 	%rd37, [%rd36];
	mul.lo.s64 	%rd60, %rd37, %rd60;
	add.s32 	%r34, %r34, 1;
	cvt.u64.u32 	%rd38, %r34;
	setp.lt.u64 	%p7, %rd38, %rd24;
	mov.pred 	%p16, %p2;
	@%p7 bra 	$L__BB14_2;

$L__BB14_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r35, %r21, %r3, %r22;
	cvt.u64.u32 	%rd61, %r35;
	@%p16 bra 	$L__BB14_8;
	bra.uni 	$L__BB14_5;

$L__BB14_8:
	setp.ge.u64 	%p10, %rd61, %rd23;
	@%p10 bra 	$L__BB14_17;

	mov.u32 	%r24, %nctaid.x;
	mul.lo.s32 	%r8, %r3, %r24;
	@%p3 bra 	$L__BB14_16;

$L__BB14_10:
	mov.u32 	%r37, 0;
	mov.u32 	%r38, %r35;
	mov.u32 	%r39, %r37;

$L__BB14_11:
	not.b32 	%r27, %r37;
	cvt.u64.u32 	%rd43, %r27;
	add.s64 	%rd44, %rd43, %rd24;
	cvt.u64.u32 	%rd11, %r38;
	shl.b64 	%rd45, %rd44, 3;
	and.b64  	%rd46, %rd45, 34359738360;
	add.s64 	%rd12, %rd3, %rd46;
	ld.global.u64 	%rd13, [%rd12];
	and.b64  	%rd47, %rd13, -4294967296;
	setp.eq.s64 	%p12, %rd47, 0;
	@%p12 bra 	$L__BB14_13;

	div.u64 	%rd63, %rd11, %rd13;
	mul.lo.s64 	%rd48, %rd63, %rd13;
	sub.s64 	%rd64, %rd11, %rd48;
	bra.uni 	$L__BB14_14;

$L__BB14_13:
	cvt.u32.u64 	%r28, %rd13;
	cvt.u32.u64 	%r29, %rd11;
	div.u32 	%r30, %r29, %r28;
	mul.lo.s32 	%r31, %r30, %r28;
	sub.s32 	%r32, %r29, %r31;
	cvt.u64.u32 	%rd63, %r30;
	cvt.u64.u32 	%rd64, %r32;

$L__BB14_14:
	shl.b64 	%rd49, %rd24, 3;
	add.s64 	%rd50, %rd12, %rd49;
	ld.global.u64 	%rd51, [%rd50];
	mul.lo.s64 	%rd52, %rd51, %rd64;
	cvt.u32.u64 	%r33, %rd52;
	add.s32 	%r39, %r39, %r33;
	cvt.u32.u64 	%r38, %rd63;
	add.s32 	%r37, %r37, 1;
	cvt.u64.u32 	%rd53, %r37;
	setp.lt.u64 	%p13, %rd53, %rd24;
	@%p13 bra 	$L__BB14_11;

	mul.wide.u32 	%rd54, %r39, 2;
	add.s64 	%rd55, %rd2, %rd54;
	ld.global.u16 	%rs2, [%rd55];
	// begin inline asm
	{  cvt.f32.f16 %f2, %rs2;}

	// end inline asm
	shl.b64 	%rd56, %rd61, 2;
	add.s64 	%rd57, %rd1, %rd56;
	st.global.f32 	[%rd57], %f2;
	add.s32 	%r35, %r35, %r8;
	cvt.u64.u32 	%rd61, %r35;
	setp.lt.u64 	%p14, %rd61, %rd23;
	@%p14 bra 	$L__BB14_10;
	bra.uni 	$L__BB14_17;

$L__BB14_16:
	ld.global.u16 	%rs3, [%rd2];
	// begin inline asm
	{  cvt.f32.f16 %f3, %rs3;}

	// end inline asm
	shl.b64 	%rd58, %rd61, 2;
	add.s64 	%rd59, %rd1, %rd58;
	st.global.f32 	[%rd59], %f3;
	add.s32 	%r35, %r35, %r8;
	cvt.u64.u32 	%rd61, %r35;
	setp.lt.u64 	%p15, %rd61, %rd23;
	@%p15 bra 	$L__BB14_16;
	bra.uni 	$L__BB14_17;

$L__BB14_5:
	setp.ge.u64 	%p8, %rd61, %rd23;
	@%p8 bra 	$L__BB14_17;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;

$L__BB14_7:
	shl.b64 	%rd39, %rd61, 1;
	add.s64 	%rd40, %rd2, %rd39;
	ld.global.u16 	%rs1, [%rd40];
	// begin inline asm
	{  cvt.f32.f16 %f1, %rs1;}

	// end inline asm
	shl.b64 	%rd41, %rd61, 2;
	add.s64 	%rd42, %rd1, %rd41;
	st.global.f32 	[%rd42], %f1;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd61, %r35;
	setp.lt.u64 	%p9, %rd61, %rd23;
	@%p9 bra 	$L__BB14_7;

$L__BB14_17:
	ret;

}
	// .globl	cast_f16_f64
.visible .entry cast_f16_f64(
	.param .u64 cast_f16_f64_param_0,
	.param .u64 cast_f16_f64_param_1,
	.param .u64 cast_f16_f64_param_2,
	.param .u64 cast_f16_f64_param_3,
	.param .u64 cast_f16_f64_param_4
)
{
	.reg .pred 	%p<17>;
	.reg .b16 	%rs<4>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<41>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<66>;


	ld.param.u64 	%rd23, [cast_f16_f64_param_0];
	ld.param.u64 	%rd24, [cast_f16_f64_param_1];
	ld.param.u64 	%rd25, [cast_f16_f64_param_2];
	ld.param.u64 	%rd26, [cast_f16_f64_param_3];
	ld.param.u64 	%rd27, [cast_f16_f64_param_4];
	cvta.to.global.u64 	%rd1, %rd27;
	cvta.to.global.u64 	%rd2, %rd26;
	cvta.to.global.u64 	%rd3, %rd25;
	setp.eq.s64 	%p3, %rd24, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p16, %p2;
	@%p3 bra 	$L__BB15_4;

	mov.u64 	%rd60, 1;
	mov.u32 	%r34, 0;

$L__BB15_2:
	not.b32 	%r20, %r34;
	cvt.u64.u32 	%rd29, %r20;
	add.s64 	%rd30, %rd29, %rd24;
	and.b64  	%rd5, %rd30, 4294967295;
	add.s64 	%rd31, %rd5, %rd24;
	shl.b64 	%rd32, %rd31, 3;
	add.s64 	%rd33, %rd3, %rd32;
	ld.global.u64 	%rd34, [%rd33];
	setp.ne.s64 	%p5, %rd60, %rd34;
	mov.pred 	%p16, -1;
	@%p5 bra 	$L__BB15_4;

	shl.b64 	%rd35, %rd5, 3;
	add.s64 	%rd36, %rd3, %rd35;
	ld.global.u64 	%rd37, [%rd36];
	mul.lo.s64 	%rd60, %rd37, %rd60;
	add.s32 	%r34, %r34, 1;
	cvt.u64.u32 	%rd38, %r34;
	setp.lt.u64 	%p7, %rd38, %rd24;
	mov.pred 	%p16, %p2;
	@%p7 bra 	$L__BB15_2;

$L__BB15_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r35, %r21, %r3, %r22;
	cvt.u64.u32 	%rd61, %r35;
	@%p16 bra 	$L__BB15_8;
	bra.uni 	$L__BB15_5;

$L__BB15_8:
	setp.ge.u64 	%p10, %rd61, %rd23;
	@%p10 bra 	$L__BB15_17;

	mov.u32 	%r24, %nctaid.x;
	mul.lo.s32 	%r8, %r3, %r24;
	@%p3 bra 	$L__BB15_16;

$L__BB15_10:
	mov.u32 	%r37, 0;
	mov.u32 	%r38, %r35;
	mov.u32 	%r39, %r37;

$L__BB15_11:
	not.b32 	%r27, %r37;
	cvt.u64.u32 	%rd43, %r27;
	add.s64 	%rd44, %rd43, %rd24;
	cvt.u64.u32 	%rd11, %r38;
	shl.b64 	%rd45, %rd44, 3;
	and.b64  	%rd46, %rd45, 34359738360;
	add.s64 	%rd12, %rd3, %rd46;
	ld.global.u64 	%rd13, [%rd12];
	and.b64  	%rd47, %rd13, -4294967296;
	setp.eq.s64 	%p12, %rd47, 0;
	@%p12 bra 	$L__BB15_13;

	div.u64 	%rd63, %rd11, %rd13;
	mul.lo.s64 	%rd48, %rd63, %rd13;
	sub.s64 	%rd64, %rd11, %rd48;
	bra.uni 	$L__BB15_14;

$L__BB15_13:
	cvt.u32.u64 	%r28, %rd13;
	cvt.u32.u64 	%r29, %rd11;
	div.u32 	%r30, %r29, %r28;
	mul.lo.s32 	%r31, %r30, %r28;
	sub.s32 	%r32, %r29, %r31;
	cvt.u64.u32 	%rd63, %r30;
	cvt.u64.u32 	%rd64, %r32;

$L__BB15_14:
	shl.b64 	%rd49, %rd24, 3;
	add.s64 	%rd50, %rd12, %rd49;
	ld.global.u64 	%rd51, [%rd50];
	mul.lo.s64 	%rd52, %rd51, %rd64;
	cvt.u32.u64 	%r33, %rd52;
	add.s32 	%r39, %r39, %r33;
	cvt.u32.u64 	%r38, %rd63;
	add.s32 	%r37, %r37, 1;
	cvt.u64.u32 	%rd53, %r37;
	setp.lt.u64 	%p13, %rd53, %rd24;
	@%p13 bra 	$L__BB15_11;

	mul.wide.u32 	%rd54, %r39, 2;
	add.s64 	%rd55, %rd2, %rd54;
	ld.global.u16 	%rs2, [%rd55];
	// begin inline asm
	{  cvt.f32.f16 %f2, %rs2;}

	// end inline asm
	cvt.f64.f32 	%fd2, %f2;
	shl.b64 	%rd56, %rd61, 3;
	add.s64 	%rd57, %rd1, %rd56;
	st.global.f64 	[%rd57], %fd2;
	add.s32 	%r35, %r35, %r8;
	cvt.u64.u32 	%rd61, %r35;
	setp.lt.u64 	%p14, %rd61, %rd23;
	@%p14 bra 	$L__BB15_10;
	bra.uni 	$L__BB15_17;

$L__BB15_16:
	ld.global.u16 	%rs3, [%rd2];
	// begin inline asm
	{  cvt.f32.f16 %f3, %rs3;}

	// end inline asm
	cvt.f64.f32 	%fd3, %f3;
	shl.b64 	%rd58, %rd61, 3;
	add.s64 	%rd59, %rd1, %rd58;
	st.global.f64 	[%rd59], %fd3;
	add.s32 	%r35, %r35, %r8;
	cvt.u64.u32 	%rd61, %r35;
	setp.lt.u64 	%p15, %rd61, %rd23;
	@%p15 bra 	$L__BB15_16;
	bra.uni 	$L__BB15_17;

$L__BB15_5:
	setp.ge.u64 	%p8, %rd61, %rd23;
	@%p8 bra 	$L__BB15_17;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;

$L__BB15_7:
	shl.b64 	%rd39, %rd61, 1;
	add.s64 	%rd40, %rd2, %rd39;
	ld.global.u16 	%rs1, [%rd40];
	// begin inline asm
	{  cvt.f32.f16 %f1, %rs1;}

	// end inline asm
	cvt.f64.f32 	%fd1, %f1;
	shl.b64 	%rd41, %rd61, 3;
	add.s64 	%rd42, %rd1, %rd41;
	st.global.f64 	[%rd42], %fd1;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd61, %r35;
	setp.lt.u64 	%p9, %rd61, %rd23;
	@%p9 bra 	$L__BB15_7;

$L__BB15_17:
	ret;

}
	// .globl	cast_u8_f16
.visible .entry cast_u8_f16(
	.param .u64 cast_u8_f16_param_0,
	.param .u64 cast_u8_f16_param_1,
	.param .u64 cast_u8_f16_param_2,
	.param .u64 cast_u8_f16_param_3,
	.param .u64 cast_u8_f16_param_4
)
{
	.reg .pred 	%p<17>;
	.reg .b16 	%rs<4>;
	.reg .b32 	%r<44>;
	.reg .b64 	%rd<65>;


	ld.param.u64 	%rd23, [cast_u8_f16_param_0];
	ld.param.u64 	%rd24, [cast_u8_f16_param_1];
	ld.param.u64 	%rd25, [cast_u8_f16_param_2];
	ld.param.u64 	%rd26, [cast_u8_f16_param_3];
	ld.param.u64 	%rd27, [cast_u8_f16_param_4];
	cvta.to.global.u64 	%rd1, %rd27;
	cvta.to.global.u64 	%rd2, %rd26;
	cvta.to.global.u64 	%rd3, %rd25;
	setp.eq.s64 	%p3, %rd24, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p16, %p2;
	@%p3 bra 	$L__BB16_4;

	mov.u64 	%rd59, 1;
	mov.u32 	%r37, 0;

$L__BB16_2:
	not.b32 	%r20, %r37;
	cvt.u64.u32 	%rd29, %r20;
	add.s64 	%rd30, %rd29, %rd24;
	and.b64  	%rd5, %rd30, 4294967295;
	add.s64 	%rd31, %rd5, %rd24;
	shl.b64 	%rd32, %rd31, 3;
	add.s64 	%rd33, %rd3, %rd32;
	ld.global.u64 	%rd34, [%rd33];
	setp.ne.s64 	%p5, %rd59, %rd34;
	mov.pred 	%p16, -1;
	@%p5 bra 	$L__BB16_4;

	shl.b64 	%rd35, %rd5, 3;
	add.s64 	%rd36, %rd3, %rd35;
	ld.global.u64 	%rd37, [%rd36];
	mul.lo.s64 	%rd59, %rd37, %rd59;
	add.s32 	%r37, %r37, 1;
	cvt.u64.u32 	%rd38, %r37;
	setp.lt.u64 	%p7, %rd38, %rd24;
	mov.pred 	%p16, %p2;
	@%p7 bra 	$L__BB16_2;

$L__BB16_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r38, %r21, %r3, %r22;
	cvt.u64.u32 	%rd60, %r38;
	@%p16 bra 	$L__BB16_8;
	bra.uni 	$L__BB16_5;

$L__BB16_8:
	setp.ge.u64 	%p10, %rd60, %rd23;
	@%p10 bra 	$L__BB16_17;

	mov.u32 	%r25, %nctaid.x;
	mul.lo.s32 	%r8, %r3, %r25;
	@%p3 bra 	$L__BB16_16;

$L__BB16_10:
	mov.u32 	%r40, 0;
	mov.u32 	%r41, %r38;
	mov.u32 	%r42, %r40;

$L__BB16_11:
	not.b32 	%r28, %r40;
	cvt.u64.u32 	%rd42, %r28;
	add.s64 	%rd43, %rd42, %rd24;
	cvt.u64.u32 	%rd11, %r41;
	shl.b64 	%rd44, %rd43, 3;
	and.b64  	%rd45, %rd44, 34359738360;
	add.s64 	%rd12, %rd3, %rd45;
	ld.global.u64 	%rd13, [%rd12];
	and.b64  	%rd46, %rd13, -4294967296;
	setp.eq.s64 	%p12, %rd46, 0;
	@%p12 bra 	$L__BB16_13;

	div.u64 	%rd62, %rd11, %rd13;
	mul.lo.s64 	%rd47, %rd62, %rd13;
	sub.s64 	%rd63, %rd11, %rd47;
	bra.uni 	$L__BB16_14;

$L__BB16_13:
	cvt.u32.u64 	%r29, %rd13;
	cvt.u32.u64 	%r30, %rd11;
	div.u32 	%r31, %r30, %r29;
	mul.lo.s32 	%r32, %r31, %r29;
	sub.s32 	%r33, %r30, %r32;
	cvt.u64.u32 	%rd62, %r31;
	cvt.u64.u32 	%rd63, %r33;

$L__BB16_14:
	shl.b64 	%rd48, %rd24, 3;
	add.s64 	%rd49, %rd12, %rd48;
	ld.global.u64 	%rd50, [%rd49];
	mul.lo.s64 	%rd51, %rd50, %rd63;
	cvt.u32.u64 	%r34, %rd51;
	add.s32 	%r42, %r42, %r34;
	cvt.u32.u64 	%r41, %rd62;
	add.s32 	%r40, %r40, 1;
	cvt.u64.u32 	%rd52, %r40;
	setp.lt.u64 	%p13, %rd52, %rd24;
	@%p13 bra 	$L__BB16_11;

	cvt.u64.u32 	%rd53, %r42;
	add.s64 	%rd54, %rd2, %rd53;
	ld.global.u8 	%r35, [%rd54];
	// begin inline asm
	cvt.rn.f16.s32 %rs2, %r35;
	// end inline asm
	shl.b64 	%rd55, %rd60, 1;
	add.s64 	%rd56, %rd1, %rd55;
	st.global.u16 	[%rd56], %rs2;
	add.s32 	%r38, %r38, %r8;
	cvt.u64.u32 	%rd60, %r38;
	setp.lt.u64 	%p14, %rd60, %rd23;
	@%p14 bra 	$L__BB16_10;
	bra.uni 	$L__BB16_17;

$L__BB16_16:
	ld.global.u8 	%r36, [%rd2];
	// begin inline asm
	cvt.rn.f16.s32 %rs3, %r36;
	// end inline asm
	shl.b64 	%rd57, %rd60, 1;
	add.s64 	%rd58, %rd1, %rd57;
	st.global.u16 	[%rd58], %rs3;
	add.s32 	%r38, %r38, %r8;
	cvt.u64.u32 	%rd60, %r38;
	setp.lt.u64 	%p15, %rd60, %rd23;
	@%p15 bra 	$L__BB16_16;
	bra.uni 	$L__BB16_17;

$L__BB16_5:
	setp.ge.u64 	%p8, %rd60, %rd23;
	@%p8 bra 	$L__BB16_17;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;

$L__BB16_7:
	add.s64 	%rd39, %rd2, %rd60;
	ld.global.u8 	%r24, [%rd39];
	// begin inline asm
	cvt.rn.f16.s32 %rs1, %r24;
	// end inline asm
	shl.b64 	%rd40, %rd60, 1;
	add.s64 	%rd41, %rd1, %rd40;
	st.global.u16 	[%rd41], %rs1;
	add.s32 	%r38, %r38, %r5;
	cvt.u64.u32 	%rd60, %r38;
	setp.lt.u64 	%p9, %rd60, %rd23;
	@%p9 bra 	$L__BB16_7;

$L__BB16_17:
	ret;

}
	// .globl	cast_u32_f16
.visible .entry cast_u32_f16(
	.param .u64 cast_u32_f16_param_0,
	.param .u64 cast_u32_f16_param_1,
	.param .u64 cast_u32_f16_param_2,
	.param .u64 cast_u32_f16_param_3,
	.param .u64 cast_u32_f16_param_4
)
{
	.reg .pred 	%p<17>;
	.reg .b16 	%rs<4>;
	.reg .b32 	%r<44>;
	.reg .b64 	%rd<66>;


	ld.param.u64 	%rd23, [cast_u32_f16_param_0];
	ld.param.u64 	%rd24, [cast_u32_f16_param_1];
	ld.param.u64 	%rd25, [cast_u32_f16_param_2];
	ld.param.u64 	%rd26, [cast_u32_f16_param_3];
	ld.param.u64 	%rd27, [cast_u32_f16_param_4];
	cvta.to.global.u64 	%rd1, %rd27;
	cvta.to.global.u64 	%rd2, %rd26;
	cvta.to.global.u64 	%rd3, %rd25;
	setp.eq.s64 	%p3, %rd24, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p16, %p2;
	@%p3 bra 	$L__BB17_4;

	mov.u64 	%rd60, 1;
	mov.u32 	%r37, 0;

$L__BB17_2:
	not.b32 	%r20, %r37;
	cvt.u64.u32 	%rd29, %r20;
	add.s64 	%rd30, %rd29, %rd24;
	and.b64  	%rd5, %rd30, 4294967295;
	add.s64 	%rd31, %rd5, %rd24;
	shl.b64 	%rd32, %rd31, 3;
	add.s64 	%rd33, %rd3, %rd32;
	ld.global.u64 	%rd34, [%rd33];
	setp.ne.s64 	%p5, %rd60, %rd34;
	mov.pred 	%p16, -1;
	@%p5 bra 	$L__BB17_4;

	shl.b64 	%rd35, %rd5, 3;
	add.s64 	%rd36, %rd3, %rd35;
	ld.global.u64 	%rd37, [%rd36];
	mul.lo.s64 	%rd60, %rd37, %rd60;
	add.s32 	%r37, %r37, 1;
	cvt.u64.u32 	%rd38, %r37;
	setp.lt.u64 	%p7, %rd38, %rd24;
	mov.pred 	%p16, %p2;
	@%p7 bra 	$L__BB17_2;

$L__BB17_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r38, %r21, %r3, %r22;
	cvt.u64.u32 	%rd61, %r38;
	@%p16 bra 	$L__BB17_8;
	bra.uni 	$L__BB17_5;

$L__BB17_8:
	setp.ge.u64 	%p10, %rd61, %rd23;
	@%p10 bra 	$L__BB17_17;

	mov.u32 	%r25, %nctaid.x;
	mul.lo.s32 	%r8, %r3, %r25;
	@%p3 bra 	$L__BB17_16;

$L__BB17_10:
	mov.u32 	%r40, 0;
	mov.u32 	%r41, %r38;
	mov.u32 	%r42, %r40;

$L__BB17_11:
	not.b32 	%r28, %r40;
	cvt.u64.u32 	%rd43, %r28;
	add.s64 	%rd44, %rd43, %rd24;
	cvt.u64.u32 	%rd11, %r41;
	shl.b64 	%rd45, %rd44, 3;
	and.b64  	%rd46, %rd45, 34359738360;
	add.s64 	%rd12, %rd3, %rd46;
	ld.global.u64 	%rd13, [%rd12];
	and.b64  	%rd47, %rd13, -4294967296;
	setp.eq.s64 	%p12, %rd47, 0;
	@%p12 bra 	$L__BB17_13;

	div.u64 	%rd63, %rd11, %rd13;
	mul.lo.s64 	%rd48, %rd63, %rd13;
	sub.s64 	%rd64, %rd11, %rd48;
	bra.uni 	$L__BB17_14;

$L__BB17_13:
	cvt.u32.u64 	%r29, %rd13;
	cvt.u32.u64 	%r30, %rd11;
	div.u32 	%r31, %r30, %r29;
	mul.lo.s32 	%r32, %r31, %r29;
	sub.s32 	%r33, %r30, %r32;
	cvt.u64.u32 	%rd63, %r31;
	cvt.u64.u32 	%rd64, %r33;

$L__BB17_14:
	shl.b64 	%rd49, %rd24, 3;
	add.s64 	%rd50, %rd12, %rd49;
	ld.global.u64 	%rd51, [%rd50];
	mul.lo.s64 	%rd52, %rd51, %rd64;
	cvt.u32.u64 	%r34, %rd52;
	add.s32 	%r42, %r42, %r34;
	cvt.u32.u64 	%r41, %rd63;
	add.s32 	%r40, %r40, 1;
	cvt.u64.u32 	%rd53, %r40;
	setp.lt.u64 	%p13, %rd53, %rd24;
	@%p13 bra 	$L__BB17_11;

	mul.wide.u32 	%rd54, %r42, 4;
	add.s64 	%rd55, %rd2, %rd54;
	ld.global.u32 	%r35, [%rd55];
	// begin inline asm
	cvt.rn.f16.u32 %rs2, %r35;
	// end inline asm
	shl.b64 	%rd56, %rd61, 1;
	add.s64 	%rd57, %rd1, %rd56;
	st.global.u16 	[%rd57], %rs2;
	add.s32 	%r38, %r38, %r8;
	cvt.u64.u32 	%rd61, %r38;
	setp.lt.u64 	%p14, %rd61, %rd23;
	@%p14 bra 	$L__BB17_10;
	bra.uni 	$L__BB17_17;

$L__BB17_16:
	ld.global.u32 	%r36, [%rd2];
	// begin inline asm
	cvt.rn.f16.u32 %rs3, %r36;
	// end inline asm
	shl.b64 	%rd58, %rd61, 1;
	add.s64 	%rd59, %rd1, %rd58;
	st.global.u16 	[%rd59], %rs3;
	add.s32 	%r38, %r38, %r8;
	cvt.u64.u32 	%rd61, %r38;
	setp.lt.u64 	%p15, %rd61, %rd23;
	@%p15 bra 	$L__BB17_16;
	bra.uni 	$L__BB17_17;

$L__BB17_5:
	setp.ge.u64 	%p8, %rd61, %rd23;
	@%p8 bra 	$L__BB17_17;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;

$L__BB17_7:
	shl.b64 	%rd39, %rd61, 2;
	add.s64 	%rd40, %rd2, %rd39;
	ld.global.u32 	%r24, [%rd40];
	// begin inline asm
	cvt.rn.f16.u32 %rs1, %r24;
	// end inline asm
	shl.b64 	%rd41, %rd61, 1;
	add.s64 	%rd42, %rd1, %rd41;
	st.global.u16 	[%rd42], %rs1;
	add.s32 	%r38, %r38, %r5;
	cvt.u64.u32 	%rd61, %r38;
	setp.lt.u64 	%p9, %rd61, %rd23;
	@%p9 bra 	$L__BB17_7;

$L__BB17_17:
	ret;

}
	// .globl	cast_f32_f16
.visible .entry cast_f32_f16(
	.param .u64 cast_f32_f16_param_0,
	.param .u64 cast_f32_f16_param_1,
	.param .u64 cast_f32_f16_param_2,
	.param .u64 cast_f32_f16_param_3,
	.param .u64 cast_f32_f16_param_4
)
{
	.reg .pred 	%p<17>;
	.reg .b16 	%rs<4>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<41>;
	.reg .b64 	%rd<66>;


	ld.param.u64 	%rd23, [cast_f32_f16_param_0];
	ld.param.u64 	%rd24, [cast_f32_f16_param_1];
	ld.param.u64 	%rd25, [cast_f32_f16_param_2];
	ld.param.u64 	%rd26, [cast_f32_f16_param_3];
	ld.param.u64 	%rd27, [cast_f32_f16_param_4];
	cvta.to.global.u64 	%rd1, %rd27;
	cvta.to.global.u64 	%rd2, %rd26;
	cvta.to.global.u64 	%rd3, %rd25;
	setp.eq.s64 	%p3, %rd24, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p16, %p2;
	@%p3 bra 	$L__BB18_4;

	mov.u64 	%rd60, 1;
	mov.u32 	%r34, 0;

$L__BB18_2:
	not.b32 	%r20, %r34;
	cvt.u64.u32 	%rd29, %r20;
	add.s64 	%rd30, %rd29, %rd24;
	and.b64  	%rd5, %rd30, 4294967295;
	add.s64 	%rd31, %rd5, %rd24;
	shl.b64 	%rd32, %rd31, 3;
	add.s64 	%rd33, %rd3, %rd32;
	ld.global.u64 	%rd34, [%rd33];
	setp.ne.s64 	%p5, %rd60, %rd34;
	mov.pred 	%p16, -1;
	@%p5 bra 	$L__BB18_4;

	shl.b64 	%rd35, %rd5, 3;
	add.s64 	%rd36, %rd3, %rd35;
	ld.global.u64 	%rd37, [%rd36];
	mul.lo.s64 	%rd60, %rd37, %rd60;
	add.s32 	%r34, %r34, 1;
	cvt.u64.u32 	%rd38, %r34;
	setp.lt.u64 	%p7, %rd38, %rd24;
	mov.pred 	%p16, %p2;
	@%p7 bra 	$L__BB18_2;

$L__BB18_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r35, %r21, %r3, %r22;
	cvt.u64.u32 	%rd61, %r35;
	@%p16 bra 	$L__BB18_8;
	bra.uni 	$L__BB18_5;

$L__BB18_8:
	setp.ge.u64 	%p10, %rd61, %rd23;
	@%p10 bra 	$L__BB18_17;

	mov.u32 	%r24, %nctaid.x;
	mul.lo.s32 	%r8, %r3, %r24;
	@%p3 bra 	$L__BB18_16;

$L__BB18_10:
	mov.u32 	%r37, 0;
	mov.u32 	%r38, %r35;
	mov.u32 	%r39, %r37;

$L__BB18_11:
	not.b32 	%r27, %r37;
	cvt.u64.u32 	%rd43, %r27;
	add.s64 	%rd44, %rd43, %rd24;
	cvt.u64.u32 	%rd11, %r38;
	shl.b64 	%rd45, %rd44, 3;
	and.b64  	%rd46, %rd45, 34359738360;
	add.s64 	%rd12, %rd3, %rd46;
	ld.global.u64 	%rd13, [%rd12];
	and.b64  	%rd47, %rd13, -4294967296;
	setp.eq.s64 	%p12, %rd47, 0;
	@%p12 bra 	$L__BB18_13;

	div.u64 	%rd63, %rd11, %rd13;
	mul.lo.s64 	%rd48, %rd63, %rd13;
	sub.s64 	%rd64, %rd11, %rd48;
	bra.uni 	$L__BB18_14;

$L__BB18_13:
	cvt.u32.u64 	%r28, %rd13;
	cvt.u32.u64 	%r29, %rd11;
	div.u32 	%r30, %r29, %r28;
	mul.lo.s32 	%r31, %r30, %r28;
	sub.s32 	%r32, %r29, %r31;
	cvt.u64.u32 	%rd63, %r30;
	cvt.u64.u32 	%rd64, %r32;

$L__BB18_14:
	shl.b64 	%rd49, %rd24, 3;
	add.s64 	%rd50, %rd12, %rd49;
	ld.global.u64 	%rd51, [%rd50];
	mul.lo.s64 	%rd52, %rd51, %rd64;
	cvt.u32.u64 	%r33, %rd52;
	add.s32 	%r39, %r39, %r33;
	cvt.u32.u64 	%r38, %rd63;
	add.s32 	%r37, %r37, 1;
	cvt.u64.u32 	%rd53, %r37;
	setp.lt.u64 	%p13, %rd53, %rd24;
	@%p13 bra 	$L__BB18_11;

	mul.wide.u32 	%rd54, %r39, 4;
	add.s64 	%rd55, %rd2, %rd54;
	ld.global.f32 	%f2, [%rd55];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs2, %f2;}

	// end inline asm
	shl.b64 	%rd56, %rd61, 1;
	add.s64 	%rd57, %rd1, %rd56;
	st.global.u16 	[%rd57], %rs2;
	add.s32 	%r35, %r35, %r8;
	cvt.u64.u32 	%rd61, %r35;
	setp.lt.u64 	%p14, %rd61, %rd23;
	@%p14 bra 	$L__BB18_10;
	bra.uni 	$L__BB18_17;

$L__BB18_16:
	ld.global.f32 	%f3, [%rd2];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs3, %f3;}

	// end inline asm
	shl.b64 	%rd58, %rd61, 1;
	add.s64 	%rd59, %rd1, %rd58;
	st.global.u16 	[%rd59], %rs3;
	add.s32 	%r35, %r35, %r8;
	cvt.u64.u32 	%rd61, %r35;
	setp.lt.u64 	%p15, %rd61, %rd23;
	@%p15 bra 	$L__BB18_16;
	bra.uni 	$L__BB18_17;

$L__BB18_5:
	setp.ge.u64 	%p8, %rd61, %rd23;
	@%p8 bra 	$L__BB18_17;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;

$L__BB18_7:
	shl.b64 	%rd39, %rd61, 2;
	add.s64 	%rd40, %rd2, %rd39;
	ld.global.f32 	%f1, [%rd40];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f1;}

	// end inline asm
	shl.b64 	%rd41, %rd61, 1;
	add.s64 	%rd42, %rd1, %rd41;
	st.global.u16 	[%rd42], %rs1;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd61, %r35;
	setp.lt.u64 	%p9, %rd61, %rd23;
	@%p9 bra 	$L__BB18_7;

$L__BB18_17:
	ret;

}
	// .globl	cast_f64_f16
.visible .entry cast_f64_f16(
	.param .u64 cast_f64_f16_param_0,
	.param .u64 cast_f64_f16_param_1,
	.param .u64 cast_f64_f16_param_2,
	.param .u64 cast_f64_f16_param_3,
	.param .u64 cast_f64_f16_param_4
)
{
	.reg .pred 	%p<17>;
	.reg .b16 	%rs<4>;
	.reg .b32 	%r<41>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<66>;


	ld.param.u64 	%rd23, [cast_f64_f16_param_0];
	ld.param.u64 	%rd24, [cast_f64_f16_param_1];
	ld.param.u64 	%rd25, [cast_f64_f16_param_2];
	ld.param.u64 	%rd26, [cast_f64_f16_param_3];
	ld.param.u64 	%rd27, [cast_f64_f16_param_4];
	cvta.to.global.u64 	%rd1, %rd27;
	cvta.to.global.u64 	%rd2, %rd26;
	cvta.to.global.u64 	%rd3, %rd25;
	setp.eq.s64 	%p3, %rd24, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p16, %p2;
	@%p3 bra 	$L__BB19_4;

	mov.u64 	%rd60, 1;
	mov.u32 	%r34, 0;

$L__BB19_2:
	not.b32 	%r20, %r34;
	cvt.u64.u32 	%rd29, %r20;
	add.s64 	%rd30, %rd29, %rd24;
	and.b64  	%rd5, %rd30, 4294967295;
	add.s64 	%rd31, %rd5, %rd24;
	shl.b64 	%rd32, %rd31, 3;
	add.s64 	%rd33, %rd3, %rd32;
	ld.global.u64 	%rd34, [%rd33];
	setp.ne.s64 	%p5, %rd60, %rd34;
	mov.pred 	%p16, -1;
	@%p5 bra 	$L__BB19_4;

	shl.b64 	%rd35, %rd5, 3;
	add.s64 	%rd36, %rd3, %rd35;
	ld.global.u64 	%rd37, [%rd36];
	mul.lo.s64 	%rd60, %rd37, %rd60;
	add.s32 	%r34, %r34, 1;
	cvt.u64.u32 	%rd38, %r34;
	setp.lt.u64 	%p7, %rd38, %rd24;
	mov.pred 	%p16, %p2;
	@%p7 bra 	$L__BB19_2;

$L__BB19_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r35, %r21, %r3, %r22;
	cvt.u64.u32 	%rd61, %r35;
	@%p16 bra 	$L__BB19_8;
	bra.uni 	$L__BB19_5;

$L__BB19_8:
	setp.ge.u64 	%p10, %rd61, %rd23;
	@%p10 bra 	$L__BB19_17;

	mov.u32 	%r24, %nctaid.x;
	mul.lo.s32 	%r8, %r3, %r24;
	@%p3 bra 	$L__BB19_16;

$L__BB19_10:
	mov.u32 	%r37, 0;
	mov.u32 	%r38, %r35;
	mov.u32 	%r39, %r37;

$L__BB19_11:
	not.b32 	%r27, %r37;
	cvt.u64.u32 	%rd43, %r27;
	add.s64 	%rd44, %rd43, %rd24;
	cvt.u64.u32 	%rd11, %r38;
	shl.b64 	%rd45, %rd44, 3;
	and.b64  	%rd46, %rd45, 34359738360;
	add.s64 	%rd12, %rd3, %rd46;
	ld.global.u64 	%rd13, [%rd12];
	and.b64  	%rd47, %rd13, -4294967296;
	setp.eq.s64 	%p12, %rd47, 0;
	@%p12 bra 	$L__BB19_13;

	div.u64 	%rd63, %rd11, %rd13;
	mul.lo.s64 	%rd48, %rd63, %rd13;
	sub.s64 	%rd64, %rd11, %rd48;
	bra.uni 	$L__BB19_14;

$L__BB19_13:
	cvt.u32.u64 	%r28, %rd13;
	cvt.u32.u64 	%r29, %rd11;
	div.u32 	%r30, %r29, %r28;
	mul.lo.s32 	%r31, %r30, %r28;
	sub.s32 	%r32, %r29, %r31;
	cvt.u64.u32 	%rd63, %r30;
	cvt.u64.u32 	%rd64, %r32;

$L__BB19_14:
	shl.b64 	%rd49, %rd24, 3;
	add.s64 	%rd50, %rd12, %rd49;
	ld.global.u64 	%rd51, [%rd50];
	mul.lo.s64 	%rd52, %rd51, %rd64;
	cvt.u32.u64 	%r33, %rd52;
	add.s32 	%r39, %r39, %r33;
	cvt.u32.u64 	%r38, %rd63;
	add.s32 	%r37, %r37, 1;
	cvt.u64.u32 	%rd53, %r37;
	setp.lt.u64 	%p13, %rd53, %rd24;
	@%p13 bra 	$L__BB19_11;

	mul.wide.u32 	%rd54, %r39, 8;
	add.s64 	%rd55, %rd2, %rd54;
	ld.global.f64 	%fd2, [%rd55];
	// begin inline asm
	{  cvt.rn.f16.f64 %rs2, %fd2;}

	// end inline asm
	shl.b64 	%rd56, %rd61, 1;
	add.s64 	%rd57, %rd1, %rd56;
	st.global.u16 	[%rd57], %rs2;
	add.s32 	%r35, %r35, %r8;
	cvt.u64.u32 	%rd61, %r35;
	setp.lt.u64 	%p14, %rd61, %rd23;
	@%p14 bra 	$L__BB19_10;
	bra.uni 	$L__BB19_17;

$L__BB19_16:
	ld.global.f64 	%fd3, [%rd2];
	// begin inline asm
	{  cvt.rn.f16.f64 %rs3, %fd3;}

	// end inline asm
	shl.b64 	%rd58, %rd61, 1;
	add.s64 	%rd59, %rd1, %rd58;
	st.global.u16 	[%rd59], %rs3;
	add.s32 	%r35, %r35, %r8;
	cvt.u64.u32 	%rd61, %r35;
	setp.lt.u64 	%p15, %rd61, %rd23;
	@%p15 bra 	$L__BB19_16;
	bra.uni 	$L__BB19_17;

$L__BB19_5:
	setp.ge.u64 	%p8, %rd61, %rd23;
	@%p8 bra 	$L__BB19_17;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;

$L__BB19_7:
	shl.b64 	%rd39, %rd61, 3;
	add.s64 	%rd40, %rd2, %rd39;
	ld.global.f64 	%fd1, [%rd40];
	// begin inline asm
	{  cvt.rn.f16.f64 %rs1, %fd1;}

	// end inline asm
	shl.b64 	%rd41, %rd61, 1;
	add.s64 	%rd42, %rd1, %rd41;
	st.global.u16 	[%rd42], %rs1;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd61, %r35;
	setp.lt.u64 	%p9, %rd61, %rd23;
	@%p9 bra 	$L__BB19_7;

$L__BB19_17:
	ret;

}
	// .globl	cast_u32_u32
.visible .entry cast_u32_u32(
	.param .u64 cast_u32_u32_param_0,
	.param .u64 cast_u32_u32_param_1,
	.param .u64 cast_u32_u32_param_2,
	.param .u64 cast_u32_u32_param_3,
	.param .u64 cast_u32_u32_param_4
)
{
	.reg .pred 	%p<17>;
	.reg .b32 	%r<44>;
	.reg .b64 	%rd<65>;


	ld.param.u64 	%rd23, [cast_u32_u32_param_0];
	ld.param.u64 	%rd24, [cast_u32_u32_param_1];
	ld.param.u64 	%rd25, [cast_u32_u32_param_2];
	ld.param.u64 	%rd26, [cast_u32_u32_param_3];
	ld.param.u64 	%rd27, [cast_u32_u32_param_4];
	cvta.to.global.u64 	%rd1, %rd27;
	cvta.to.global.u64 	%rd2, %rd26;
	cvta.to.global.u64 	%rd3, %rd25;
	setp.eq.s64 	%p3, %rd24, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p16, %p2;
	@%p3 bra 	$L__BB20_4;

	mov.u64 	%rd59, 1;
	mov.u32 	%r37, 0;

$L__BB20_2:
	not.b32 	%r20, %r37;
	cvt.u64.u32 	%rd29, %r20;
	add.s64 	%rd30, %rd29, %rd24;
	and.b64  	%rd5, %rd30, 4294967295;
	add.s64 	%rd31, %rd5, %rd24;
	shl.b64 	%rd32, %rd31, 3;
	add.s64 	%rd33, %rd3, %rd32;
	ld.global.u64 	%rd34, [%rd33];
	setp.ne.s64 	%p5, %rd59, %rd34;
	mov.pred 	%p16, -1;
	@%p5 bra 	$L__BB20_4;

	shl.b64 	%rd35, %rd5, 3;
	add.s64 	%rd36, %rd3, %rd35;
	ld.global.u64 	%rd37, [%rd36];
	mul.lo.s64 	%rd59, %rd37, %rd59;
	add.s32 	%r37, %r37, 1;
	cvt.u64.u32 	%rd38, %r37;
	setp.lt.u64 	%p7, %rd38, %rd24;
	mov.pred 	%p16, %p2;
	@%p7 bra 	$L__BB20_2;

$L__BB20_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r38, %r21, %r3, %r22;
	cvt.u64.u32 	%rd60, %r38;
	@%p16 bra 	$L__BB20_8;
	bra.uni 	$L__BB20_5;

$L__BB20_8:
	setp.ge.u64 	%p10, %rd60, %rd23;
	@%p10 bra 	$L__BB20_17;

	mov.u32 	%r25, %nctaid.x;
	mul.lo.s32 	%r8, %r3, %r25;
	@%p3 bra 	$L__BB20_16;

$L__BB20_10:
	mov.u32 	%r40, 0;
	mov.u32 	%r41, %r38;
	mov.u32 	%r42, %r40;

$L__BB20_11:
	not.b32 	%r28, %r40;
	cvt.u64.u32 	%rd42, %r28;
	add.s64 	%rd43, %rd42, %rd24;
	cvt.u64.u32 	%rd11, %r41;
	shl.b64 	%rd44, %rd43, 3;
	and.b64  	%rd45, %rd44, 34359738360;
	add.s64 	%rd12, %rd3, %rd45;
	ld.global.u64 	%rd13, [%rd12];
	and.b64  	%rd46, %rd13, -4294967296;
	setp.eq.s64 	%p12, %rd46, 0;
	@%p12 bra 	$L__BB20_13;

	div.u64 	%rd62, %rd11, %rd13;
	mul.lo.s64 	%rd47, %rd62, %rd13;
	sub.s64 	%rd63, %rd11, %rd47;
	bra.uni 	$L__BB20_14;

$L__BB20_13:
	cvt.u32.u64 	%r29, %rd13;
	cvt.u32.u64 	%r30, %rd11;
	div.u32 	%r31, %r30, %r29;
	mul.lo.s32 	%r32, %r31, %r29;
	sub.s32 	%r33, %r30, %r32;
	cvt.u64.u32 	%rd62, %r31;
	cvt.u64.u32 	%rd63, %r33;

$L__BB20_14:
	shl.b64 	%rd48, %rd24, 3;
	add.s64 	%rd49, %rd12, %rd48;
	ld.global.u64 	%rd50, [%rd49];
	mul.lo.s64 	%rd51, %rd50, %rd63;
	cvt.u32.u64 	%r34, %rd51;
	add.s32 	%r42, %r42, %r34;
	cvt.u32.u64 	%r41, %rd62;
	add.s32 	%r40, %r40, 1;
	cvt.u64.u32 	%rd52, %r40;
	setp.lt.u64 	%p13, %rd52, %rd24;
	@%p13 bra 	$L__BB20_11;

	mul.wide.u32 	%rd53, %r42, 4;
	add.s64 	%rd54, %rd2, %rd53;
	ld.global.u32 	%r35, [%rd54];
	shl.b64 	%rd55, %rd60, 2;
	add.s64 	%rd56, %rd1, %rd55;
	st.global.u32 	[%rd56], %r35;
	add.s32 	%r38, %r38, %r8;
	cvt.u64.u32 	%rd60, %r38;
	setp.lt.u64 	%p14, %rd60, %rd23;
	@%p14 bra 	$L__BB20_10;
	bra.uni 	$L__BB20_17;

$L__BB20_16:
	ld.global.u32 	%r36, [%rd2];
	shl.b64 	%rd57, %rd60, 2;
	add.s64 	%rd58, %rd1, %rd57;
	st.global.u32 	[%rd58], %r36;
	add.s32 	%r38, %r38, %r8;
	cvt.u64.u32 	%rd60, %r38;
	setp.lt.u64 	%p15, %rd60, %rd23;
	@%p15 bra 	$L__BB20_16;
	bra.uni 	$L__BB20_17;

$L__BB20_5:
	setp.ge.u64 	%p8, %rd60, %rd23;
	@%p8 bra 	$L__BB20_17;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;

$L__BB20_7:
	shl.b64 	%rd39, %rd60, 2;
	add.s64 	%rd40, %rd2, %rd39;
	ld.global.u32 	%r24, [%rd40];
	add.s64 	%rd41, %rd1, %rd39;
	st.global.u32 	[%rd41], %r24;
	add.s32 	%r38, %r38, %r5;
	cvt.u64.u32 	%rd60, %r38;
	setp.lt.u64 	%p9, %rd60, %rd23;
	@%p9 bra 	$L__BB20_7;

$L__BB20_17:
	ret;

}
	// .globl	cast_u32_u8
.visible .entry cast_u32_u8(
	.param .u64 cast_u32_u8_param_0,
	.param .u64 cast_u32_u8_param_1,
	.param .u64 cast_u32_u8_param_2,
	.param .u64 cast_u32_u8_param_3,
	.param .u64 cast_u32_u8_param_4
)
{
	.reg .pred 	%p<17>;
	.reg .b32 	%r<44>;
	.reg .b64 	%rd<63>;


	ld.param.u64 	%rd23, [cast_u32_u8_param_0];
	ld.param.u64 	%rd24, [cast_u32_u8_param_1];
	ld.param.u64 	%rd25, [cast_u32_u8_param_2];
	ld.param.u64 	%rd26, [cast_u32_u8_param_3];
	ld.param.u64 	%rd27, [cast_u32_u8_param_4];
	cvta.to.global.u64 	%rd1, %rd27;
	cvta.to.global.u64 	%rd2, %rd26;
	cvta.to.global.u64 	%rd3, %rd25;
	setp.eq.s64 	%p3, %rd24, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p16, %p2;
	@%p3 bra 	$L__BB21_4;

	mov.u64 	%rd57, 1;
	mov.u32 	%r37, 0;

$L__BB21_2:
	not.b32 	%r20, %r37;
	cvt.u64.u32 	%rd29, %r20;
	add.s64 	%rd30, %rd29, %rd24;
	and.b64  	%rd5, %rd30, 4294967295;
	add.s64 	%rd31, %rd5, %rd24;
	shl.b64 	%rd32, %rd31, 3;
	add.s64 	%rd33, %rd3, %rd32;
	ld.global.u64 	%rd34, [%rd33];
	setp.ne.s64 	%p5, %rd57, %rd34;
	mov.pred 	%p16, -1;
	@%p5 bra 	$L__BB21_4;

	shl.b64 	%rd35, %rd5, 3;
	add.s64 	%rd36, %rd3, %rd35;
	ld.global.u64 	%rd37, [%rd36];
	mul.lo.s64 	%rd57, %rd37, %rd57;
	add.s32 	%r37, %r37, 1;
	cvt.u64.u32 	%rd38, %r37;
	setp.lt.u64 	%p7, %rd38, %rd24;
	mov.pred 	%p16, %p2;
	@%p7 bra 	$L__BB21_2;

$L__BB21_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r38, %r21, %r3, %r22;
	cvt.u64.u32 	%rd58, %r38;
	@%p16 bra 	$L__BB21_8;
	bra.uni 	$L__BB21_5;

$L__BB21_8:
	setp.ge.u64 	%p10, %rd58, %rd23;
	@%p10 bra 	$L__BB21_17;

	mov.u32 	%r25, %nctaid.x;
	mul.lo.s32 	%r8, %r3, %r25;
	@%p3 bra 	$L__BB21_16;

$L__BB21_10:
	mov.u32 	%r40, 0;
	mov.u32 	%r41, %r38;
	mov.u32 	%r42, %r40;

$L__BB21_11:
	not.b32 	%r28, %r40;
	cvt.u64.u32 	%rd42, %r28;
	add.s64 	%rd43, %rd42, %rd24;
	cvt.u64.u32 	%rd11, %r41;
	shl.b64 	%rd44, %rd43, 3;
	and.b64  	%rd45, %rd44, 34359738360;
	add.s64 	%rd12, %rd3, %rd45;
	ld.global.u64 	%rd13, [%rd12];
	and.b64  	%rd46, %rd13, -4294967296;
	setp.eq.s64 	%p12, %rd46, 0;
	@%p12 bra 	$L__BB21_13;

	div.u64 	%rd60, %rd11, %rd13;
	mul.lo.s64 	%rd47, %rd60, %rd13;
	sub.s64 	%rd61, %rd11, %rd47;
	bra.uni 	$L__BB21_14;

$L__BB21_13:
	cvt.u32.u64 	%r29, %rd13;
	cvt.u32.u64 	%r30, %rd11;
	div.u32 	%r31, %r30, %r29;
	mul.lo.s32 	%r32, %r31, %r29;
	sub.s32 	%r33, %r30, %r32;
	cvt.u64.u32 	%rd60, %r31;
	cvt.u64.u32 	%rd61, %r33;

$L__BB21_14:
	shl.b64 	%rd48, %rd24, 3;
	add.s64 	%rd49, %rd12, %rd48;
	ld.global.u64 	%rd50, [%rd49];
	mul.lo.s64 	%rd51, %rd50, %rd61;
	cvt.u32.u64 	%r34, %rd51;
	add.s32 	%r42, %r42, %r34;
	cvt.u32.u64 	%r41, %rd60;
	add.s32 	%r40, %r40, 1;
	cvt.u64.u32 	%rd52, %r40;
	setp.lt.u64 	%p13, %rd52, %rd24;
	@%p13 bra 	$L__BB21_11;

	mul.wide.u32 	%rd53, %r42, 4;
	add.s64 	%rd54, %rd2, %rd53;
	ld.global.u32 	%r35, [%rd54];
	add.s64 	%rd55, %rd1, %rd58;
	st.global.u8 	[%rd55], %r35;
	add.s32 	%r38, %r38, %r8;
	cvt.u64.u32 	%rd58, %r38;
	setp.lt.u64 	%p14, %rd58, %rd23;
	@%p14 bra 	$L__BB21_10;
	bra.uni 	$L__BB21_17;

$L__BB21_16:
	ld.global.u32 	%r36, [%rd2];
	add.s64 	%rd56, %rd1, %rd58;
	st.global.u8 	[%rd56], %r36;
	add.s32 	%r38, %r38, %r8;
	cvt.u64.u32 	%rd58, %r38;
	setp.lt.u64 	%p15, %rd58, %rd23;
	@%p15 bra 	$L__BB21_16;
	bra.uni 	$L__BB21_17;

$L__BB21_5:
	setp.ge.u64 	%p8, %rd58, %rd23;
	@%p8 bra 	$L__BB21_17;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;

$L__BB21_7:
	shl.b64 	%rd39, %rd58, 2;
	add.s64 	%rd40, %rd2, %rd39;
	ld.global.u32 	%r24, [%rd40];
	add.s64 	%rd41, %rd1, %rd58;
	st.global.u8 	[%rd41], %r24;
	add.s32 	%r38, %r38, %r5;
	cvt.u64.u32 	%rd58, %r38;
	setp.lt.u64 	%p9, %rd58, %rd23;
	@%p9 bra 	$L__BB21_7;

$L__BB21_17:
	ret;

}
	// .globl	cast_u32_i64
.visible .entry cast_u32_i64(
	.param .u64 cast_u32_i64_param_0,
	.param .u64 cast_u32_i64_param_1,
	.param .u64 cast_u32_i64_param_2,
	.param .u64 cast_u32_i64_param_3,
	.param .u64 cast_u32_i64_param_4
)
{
	.reg .pred 	%p<17>;
	.reg .b32 	%r<41>;
	.reg .b64 	%rd<69>;


	ld.param.u64 	%rd23, [cast_u32_i64_param_0];
	ld.param.u64 	%rd24, [cast_u32_i64_param_1];
	ld.param.u64 	%rd25, [cast_u32_i64_param_2];
	ld.param.u64 	%rd26, [cast_u32_i64_param_3];
	ld.param.u64 	%rd27, [cast_u32_i64_param_4];
	cvta.to.global.u64 	%rd1, %rd27;
	cvta.to.global.u64 	%rd2, %rd26;
	cvta.to.global.u64 	%rd3, %rd25;
	setp.eq.s64 	%p3, %rd24, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p16, %p2;
	@%p3 bra 	$L__BB22_4;

	mov.u64 	%rd63, 1;
	mov.u32 	%r34, 0;

$L__BB22_2:
	not.b32 	%r20, %r34;
	cvt.u64.u32 	%rd29, %r20;
	add.s64 	%rd30, %rd29, %rd24;
	and.b64  	%rd5, %rd30, 4294967295;
	add.s64 	%rd31, %rd5, %rd24;
	shl.b64 	%rd32, %rd31, 3;
	add.s64 	%rd33, %rd3, %rd32;
	ld.global.u64 	%rd34, [%rd33];
	setp.ne.s64 	%p5, %rd63, %rd34;
	mov.pred 	%p16, -1;
	@%p5 bra 	$L__BB22_4;

	shl.b64 	%rd35, %rd5, 3;
	add.s64 	%rd36, %rd3, %rd35;
	ld.global.u64 	%rd37, [%rd36];
	mul.lo.s64 	%rd63, %rd37, %rd63;
	add.s32 	%r34, %r34, 1;
	cvt.u64.u32 	%rd38, %r34;
	setp.lt.u64 	%p7, %rd38, %rd24;
	mov.pred 	%p16, %p2;
	@%p7 bra 	$L__BB22_2;

$L__BB22_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r35, %r21, %r3, %r22;
	cvt.u64.u32 	%rd64, %r35;
	@%p16 bra 	$L__BB22_8;
	bra.uni 	$L__BB22_5;

$L__BB22_8:
	setp.ge.u64 	%p10, %rd64, %rd23;
	@%p10 bra 	$L__BB22_17;

	mov.u32 	%r24, %nctaid.x;
	mul.lo.s32 	%r8, %r3, %r24;
	@%p3 bra 	$L__BB22_16;

$L__BB22_10:
	mov.u32 	%r37, 0;
	mov.u32 	%r38, %r35;
	mov.u32 	%r39, %r37;

$L__BB22_11:
	not.b32 	%r27, %r37;
	cvt.u64.u32 	%rd44, %r27;
	add.s64 	%rd45, %rd44, %rd24;
	cvt.u64.u32 	%rd11, %r38;
	shl.b64 	%rd46, %rd45, 3;
	and.b64  	%rd47, %rd46, 34359738360;
	add.s64 	%rd12, %rd3, %rd47;
	ld.global.u64 	%rd13, [%rd12];
	and.b64  	%rd48, %rd13, -4294967296;
	setp.eq.s64 	%p12, %rd48, 0;
	@%p12 bra 	$L__BB22_13;

	div.u64 	%rd66, %rd11, %rd13;
	mul.lo.s64 	%rd49, %rd66, %rd13;
	sub.s64 	%rd67, %rd11, %rd49;
	bra.uni 	$L__BB22_14;

$L__BB22_13:
	cvt.u32.u64 	%r28, %rd13;
	cvt.u32.u64 	%r29, %rd11;
	div.u32 	%r30, %r29, %r28;
	mul.lo.s32 	%r31, %r30, %r28;
	sub.s32 	%r32, %r29, %r31;
	cvt.u64.u32 	%rd66, %r30;
	cvt.u64.u32 	%rd67, %r32;

$L__BB22_14:
	shl.b64 	%rd50, %rd24, 3;
	add.s64 	%rd51, %rd12, %rd50;
	ld.global.u64 	%rd52, [%rd51];
	mul.lo.s64 	%rd53, %rd52, %rd67;
	cvt.u32.u64 	%r33, %rd53;
	add.s32 	%r39, %r39, %r33;
	cvt.u32.u64 	%r38, %rd66;
	add.s32 	%r37, %r37, 1;
	cvt.u64.u32 	%rd54, %r37;
	setp.lt.u64 	%p13, %rd54, %rd24;
	@%p13 bra 	$L__BB22_11;

	mul.wide.u32 	%rd55, %r39, 4;
	add.s64 	%rd56, %rd2, %rd55;
	ld.global.u32 	%rd57, [%rd56];
	shl.b64 	%rd58, %rd64, 3;
	add.s64 	%rd59, %rd1, %rd58;
	st.global.u64 	[%rd59], %rd57;
	add.s32 	%r35, %r35, %r8;
	cvt.u64.u32 	%rd64, %r35;
	setp.lt.u64 	%p14, %rd64, %rd23;
	@%p14 bra 	$L__BB22_10;
	bra.uni 	$L__BB22_17;

$L__BB22_16:
	ld.global.u32 	%rd60, [%rd2];
	shl.b64 	%rd61, %rd64, 3;
	add.s64 	%rd62, %rd1, %rd61;
	st.global.u64 	[%rd62], %rd60;
	add.s32 	%r35, %r35, %r8;
	cvt.u64.u32 	%rd64, %r35;
	setp.lt.u64 	%p15, %rd64, %rd23;
	@%p15 bra 	$L__BB22_16;
	bra.uni 	$L__BB22_17;

$L__BB22_5:
	setp.ge.u64 	%p8, %rd64, %rd23;
	@%p8 bra 	$L__BB22_17;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;

$L__BB22_7:
	shl.b64 	%rd39, %rd64, 2;
	add.s64 	%rd40, %rd2, %rd39;
	ld.global.u32 	%rd41, [%rd40];
	shl.b64 	%rd42, %rd64, 3;
	add.s64 	%rd43, %rd1, %rd42;
	st.global.u64 	[%rd43], %rd41;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd64, %r35;
	setp.lt.u64 	%p9, %rd64, %rd23;
	@%p9 bra 	$L__BB22_7;

$L__BB22_17:
	ret;

}
	// .globl	cast_u32_f32
.visible .entry cast_u32_f32(
	.param .u64 cast_u32_f32_param_0,
	.param .u64 cast_u32_f32_param_1,
	.param .u64 cast_u32_f32_param_2,
	.param .u64 cast_u32_f32_param_3,
	.param .u64 cast_u32_f32_param_4
)
{
	.reg .pred 	%p<17>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<44>;
	.reg .b64 	%rd<65>;


	ld.param.u64 	%rd23, [cast_u32_f32_param_0];
	ld.param.u64 	%rd24, [cast_u32_f32_param_1];
	ld.param.u64 	%rd25, [cast_u32_f32_param_2];
	ld.param.u64 	%rd26, [cast_u32_f32_param_3];
	ld.param.u64 	%rd27, [cast_u32_f32_param_4];
	cvta.to.global.u64 	%rd1, %rd27;
	cvta.to.global.u64 	%rd2, %rd26;
	cvta.to.global.u64 	%rd3, %rd25;
	setp.eq.s64 	%p3, %rd24, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p16, %p2;
	@%p3 bra 	$L__BB23_4;

	mov.u64 	%rd59, 1;
	mov.u32 	%r37, 0;

$L__BB23_2:
	not.b32 	%r20, %r37;
	cvt.u64.u32 	%rd29, %r20;
	add.s64 	%rd30, %rd29, %rd24;
	and.b64  	%rd5, %rd30, 4294967295;
	add.s64 	%rd31, %rd5, %rd24;
	shl.b64 	%rd32, %rd31, 3;
	add.s64 	%rd33, %rd3, %rd32;
	ld.global.u64 	%rd34, [%rd33];
	setp.ne.s64 	%p5, %rd59, %rd34;
	mov.pred 	%p16, -1;
	@%p5 bra 	$L__BB23_4;

	shl.b64 	%rd35, %rd5, 3;
	add.s64 	%rd36, %rd3, %rd35;
	ld.global.u64 	%rd37, [%rd36];
	mul.lo.s64 	%rd59, %rd37, %rd59;
	add.s32 	%r37, %r37, 1;
	cvt.u64.u32 	%rd38, %r37;
	setp.lt.u64 	%p7, %rd38, %rd24;
	mov.pred 	%p16, %p2;
	@%p7 bra 	$L__BB23_2;

$L__BB23_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r38, %r21, %r3, %r22;
	cvt.u64.u32 	%rd60, %r38;
	@%p16 bra 	$L__BB23_8;
	bra.uni 	$L__BB23_5;

$L__BB23_8:
	setp.ge.u64 	%p10, %rd60, %rd23;
	@%p10 bra 	$L__BB23_17;

	mov.u32 	%r25, %nctaid.x;
	mul.lo.s32 	%r8, %r3, %r25;
	@%p3 bra 	$L__BB23_16;

$L__BB23_10:
	mov.u32 	%r40, 0;
	mov.u32 	%r41, %r38;
	mov.u32 	%r42, %r40;

$L__BB23_11:
	not.b32 	%r28, %r40;
	cvt.u64.u32 	%rd42, %r28;
	add.s64 	%rd43, %rd42, %rd24;
	cvt.u64.u32 	%rd11, %r41;
	shl.b64 	%rd44, %rd43, 3;
	and.b64  	%rd45, %rd44, 34359738360;
	add.s64 	%rd12, %rd3, %rd45;
	ld.global.u64 	%rd13, [%rd12];
	and.b64  	%rd46, %rd13, -4294967296;
	setp.eq.s64 	%p12, %rd46, 0;
	@%p12 bra 	$L__BB23_13;

	div.u64 	%rd62, %rd11, %rd13;
	mul.lo.s64 	%rd47, %rd62, %rd13;
	sub.s64 	%rd63, %rd11, %rd47;
	bra.uni 	$L__BB23_14;

$L__BB23_13:
	cvt.u32.u64 	%r29, %rd13;
	cvt.u32.u64 	%r30, %rd11;
	div.u32 	%r31, %r30, %r29;
	mul.lo.s32 	%r32, %r31, %r29;
	sub.s32 	%r33, %r30, %r32;
	cvt.u64.u32 	%rd62, %r31;
	cvt.u64.u32 	%rd63, %r33;

$L__BB23_14:
	shl.b64 	%rd48, %rd24, 3;
	add.s64 	%rd49, %rd12, %rd48;
	ld.global.u64 	%rd50, [%rd49];
	mul.lo.s64 	%rd51, %rd50, %rd63;
	cvt.u32.u64 	%r34, %rd51;
	add.s32 	%r42, %r42, %r34;
	cvt.u32.u64 	%r41, %rd62;
	add.s32 	%r40, %r40, 1;
	cvt.u64.u32 	%rd52, %r40;
	setp.lt.u64 	%p13, %rd52, %rd24;
	@%p13 bra 	$L__BB23_11;

	mul.wide.u32 	%rd53, %r42, 4;
	add.s64 	%rd54, %rd2, %rd53;
	ld.global.u32 	%r35, [%rd54];
	cvt.rn.f32.u32 	%f2, %r35;
	shl.b64 	%rd55, %rd60, 2;
	add.s64 	%rd56, %rd1, %rd55;
	st.global.f32 	[%rd56], %f2;
	add.s32 	%r38, %r38, %r8;
	cvt.u64.u32 	%rd60, %r38;
	setp.lt.u64 	%p14, %rd60, %rd23;
	@%p14 bra 	$L__BB23_10;
	bra.uni 	$L__BB23_17;

$L__BB23_16:
	ld.global.u32 	%r36, [%rd2];
	cvt.rn.f32.u32 	%f3, %r36;
	shl.b64 	%rd57, %rd60, 2;
	add.s64 	%rd58, %rd1, %rd57;
	st.global.f32 	[%rd58], %f3;
	add.s32 	%r38, %r38, %r8;
	cvt.u64.u32 	%rd60, %r38;
	setp.lt.u64 	%p15, %rd60, %rd23;
	@%p15 bra 	$L__BB23_16;
	bra.uni 	$L__BB23_17;

$L__BB23_5:
	setp.ge.u64 	%p8, %rd60, %rd23;
	@%p8 bra 	$L__BB23_17;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;

$L__BB23_7:
	shl.b64 	%rd39, %rd60, 2;
	add.s64 	%rd40, %rd2, %rd39;
	ld.global.u32 	%r24, [%rd40];
	cvt.rn.f32.u32 	%f1, %r24;
	add.s64 	%rd41, %rd1, %rd39;
	st.global.f32 	[%rd41], %f1;
	add.s32 	%r38, %r38, %r5;
	cvt.u64.u32 	%rd60, %r38;
	setp.lt.u64 	%p9, %rd60, %rd23;
	@%p9 bra 	$L__BB23_7;

$L__BB23_17:
	ret;

}
	// .globl	cast_u32_f64
.visible .entry cast_u32_f64(
	.param .u64 cast_u32_f64_param_0,
	.param .u64 cast_u32_f64_param_1,
	.param .u64 cast_u32_f64_param_2,
	.param .u64 cast_u32_f64_param_3,
	.param .u64 cast_u32_f64_param_4
)
{
	.reg .pred 	%p<17>;
	.reg .b32 	%r<44>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<66>;


	ld.param.u64 	%rd23, [cast_u32_f64_param_0];
	ld.param.u64 	%rd24, [cast_u32_f64_param_1];
	ld.param.u64 	%rd25, [cast_u32_f64_param_2];
	ld.param.u64 	%rd26, [cast_u32_f64_param_3];
	ld.param.u64 	%rd27, [cast_u32_f64_param_4];
	cvta.to.global.u64 	%rd1, %rd27;
	cvta.to.global.u64 	%rd2, %rd26;
	cvta.to.global.u64 	%rd3, %rd25;
	setp.eq.s64 	%p3, %rd24, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p16, %p2;
	@%p3 bra 	$L__BB24_4;

	mov.u64 	%rd60, 1;
	mov.u32 	%r37, 0;

$L__BB24_2:
	not.b32 	%r20, %r37;
	cvt.u64.u32 	%rd29, %r20;
	add.s64 	%rd30, %rd29, %rd24;
	and.b64  	%rd5, %rd30, 4294967295;
	add.s64 	%rd31, %rd5, %rd24;
	shl.b64 	%rd32, %rd31, 3;
	add.s64 	%rd33, %rd3, %rd32;
	ld.global.u64 	%rd34, [%rd33];
	setp.ne.s64 	%p5, %rd60, %rd34;
	mov.pred 	%p16, -1;
	@%p5 bra 	$L__BB24_4;

	shl.b64 	%rd35, %rd5, 3;
	add.s64 	%rd36, %rd3, %rd35;
	ld.global.u64 	%rd37, [%rd36];
	mul.lo.s64 	%rd60, %rd37, %rd60;
	add.s32 	%r37, %r37, 1;
	cvt.u64.u32 	%rd38, %r37;
	setp.lt.u64 	%p7, %rd38, %rd24;
	mov.pred 	%p16, %p2;
	@%p7 bra 	$L__BB24_2;

$L__BB24_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r38, %r21, %r3, %r22;
	cvt.u64.u32 	%rd61, %r38;
	@%p16 bra 	$L__BB24_8;
	bra.uni 	$L__BB24_5;

$L__BB24_8:
	setp.ge.u64 	%p10, %rd61, %rd23;
	@%p10 bra 	$L__BB24_17;

	mov.u32 	%r25, %nctaid.x;
	mul.lo.s32 	%r8, %r3, %r25;
	@%p3 bra 	$L__BB24_16;

$L__BB24_10:
	mov.u32 	%r40, 0;
	mov.u32 	%r41, %r38;
	mov.u32 	%r42, %r40;

$L__BB24_11:
	not.b32 	%r28, %r40;
	cvt.u64.u32 	%rd43, %r28;
	add.s64 	%rd44, %rd43, %rd24;
	cvt.u64.u32 	%rd11, %r41;
	shl.b64 	%rd45, %rd44, 3;
	and.b64  	%rd46, %rd45, 34359738360;
	add.s64 	%rd12, %rd3, %rd46;
	ld.global.u64 	%rd13, [%rd12];
	and.b64  	%rd47, %rd13, -4294967296;
	setp.eq.s64 	%p12, %rd47, 0;
	@%p12 bra 	$L__BB24_13;

	div.u64 	%rd63, %rd11, %rd13;
	mul.lo.s64 	%rd48, %rd63, %rd13;
	sub.s64 	%rd64, %rd11, %rd48;
	bra.uni 	$L__BB24_14;

$L__BB24_13:
	cvt.u32.u64 	%r29, %rd13;
	cvt.u32.u64 	%r30, %rd11;
	div.u32 	%r31, %r30, %r29;
	mul.lo.s32 	%r32, %r31, %r29;
	sub.s32 	%r33, %r30, %r32;
	cvt.u64.u32 	%rd63, %r31;
	cvt.u64.u32 	%rd64, %r33;

$L__BB24_14:
	shl.b64 	%rd49, %rd24, 3;
	add.s64 	%rd50, %rd12, %rd49;
	ld.global.u64 	%rd51, [%rd50];
	mul.lo.s64 	%rd52, %rd51, %rd64;
	cvt.u32.u64 	%r34, %rd52;
	add.s32 	%r42, %r42, %r34;
	cvt.u32.u64 	%r41, %rd63;
	add.s32 	%r40, %r40, 1;
	cvt.u64.u32 	%rd53, %r40;
	setp.lt.u64 	%p13, %rd53, %rd24;
	@%p13 bra 	$L__BB24_11;

	mul.wide.u32 	%rd54, %r42, 4;
	add.s64 	%rd55, %rd2, %rd54;
	ld.global.u32 	%r35, [%rd55];
	cvt.rn.f64.u32 	%fd2, %r35;
	shl.b64 	%rd56, %rd61, 3;
	add.s64 	%rd57, %rd1, %rd56;
	st.global.f64 	[%rd57], %fd2;
	add.s32 	%r38, %r38, %r8;
	cvt.u64.u32 	%rd61, %r38;
	setp.lt.u64 	%p14, %rd61, %rd23;
	@%p14 bra 	$L__BB24_10;
	bra.uni 	$L__BB24_17;

$L__BB24_16:
	ld.global.u32 	%r36, [%rd2];
	cvt.rn.f64.u32 	%fd3, %r36;
	shl.b64 	%rd58, %rd61, 3;
	add.s64 	%rd59, %rd1, %rd58;
	st.global.f64 	[%rd59], %fd3;
	add.s32 	%r38, %r38, %r8;
	cvt.u64.u32 	%rd61, %r38;
	setp.lt.u64 	%p15, %rd61, %rd23;
	@%p15 bra 	$L__BB24_16;
	bra.uni 	$L__BB24_17;

$L__BB24_5:
	setp.ge.u64 	%p8, %rd61, %rd23;
	@%p8 bra 	$L__BB24_17;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;

$L__BB24_7:
	shl.b64 	%rd39, %rd61, 2;
	add.s64 	%rd40, %rd2, %rd39;
	ld.global.u32 	%r24, [%rd40];
	cvt.rn.f64.u32 	%fd1, %r24;
	shl.b64 	%rd41, %rd61, 3;
	add.s64 	%rd42, %rd1, %rd41;
	st.global.f64 	[%rd42], %fd1;
	add.s32 	%r38, %r38, %r5;
	cvt.u64.u32 	%rd61, %r38;
	setp.lt.u64 	%p9, %rd61, %rd23;
	@%p9 bra 	$L__BB24_7;

$L__BB24_17:
	ret;

}
	// .globl	cast_u8_u32
.visible .entry cast_u8_u32(
	.param .u64 cast_u8_u32_param_0,
	.param .u64 cast_u8_u32_param_1,
	.param .u64 cast_u8_u32_param_2,
	.param .u64 cast_u8_u32_param_3,
	.param .u64 cast_u8_u32_param_4
)
{
	.reg .pred 	%p<17>;
	.reg .b32 	%r<44>;
	.reg .b64 	%rd<65>;


	ld.param.u64 	%rd23, [cast_u8_u32_param_0];
	ld.param.u64 	%rd24, [cast_u8_u32_param_1];
	ld.param.u64 	%rd25, [cast_u8_u32_param_2];
	ld.param.u64 	%rd26, [cast_u8_u32_param_3];
	ld.param.u64 	%rd27, [cast_u8_u32_param_4];
	cvta.to.global.u64 	%rd1, %rd27;
	cvta.to.global.u64 	%rd2, %rd26;
	cvta.to.global.u64 	%rd3, %rd25;
	setp.eq.s64 	%p3, %rd24, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p16, %p2;
	@%p3 bra 	$L__BB25_4;

	mov.u64 	%rd59, 1;
	mov.u32 	%r37, 0;

$L__BB25_2:
	not.b32 	%r20, %r37;
	cvt.u64.u32 	%rd29, %r20;
	add.s64 	%rd30, %rd29, %rd24;
	and.b64  	%rd5, %rd30, 4294967295;
	add.s64 	%rd31, %rd5, %rd24;
	shl.b64 	%rd32, %rd31, 3;
	add.s64 	%rd33, %rd3, %rd32;
	ld.global.u64 	%rd34, [%rd33];
	setp.ne.s64 	%p5, %rd59, %rd34;
	mov.pred 	%p16, -1;
	@%p5 bra 	$L__BB25_4;

	shl.b64 	%rd35, %rd5, 3;
	add.s64 	%rd36, %rd3, %rd35;
	ld.global.u64 	%rd37, [%rd36];
	mul.lo.s64 	%rd59, %rd37, %rd59;
	add.s32 	%r37, %r37, 1;
	cvt.u64.u32 	%rd38, %r37;
	setp.lt.u64 	%p7, %rd38, %rd24;
	mov.pred 	%p16, %p2;
	@%p7 bra 	$L__BB25_2;

$L__BB25_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r38, %r21, %r3, %r22;
	cvt.u64.u32 	%rd60, %r38;
	@%p16 bra 	$L__BB25_8;
	bra.uni 	$L__BB25_5;

$L__BB25_8:
	setp.ge.u64 	%p10, %rd60, %rd23;
	@%p10 bra 	$L__BB25_17;

	mov.u32 	%r25, %nctaid.x;
	mul.lo.s32 	%r8, %r3, %r25;
	@%p3 bra 	$L__BB25_16;

$L__BB25_10:
	mov.u32 	%r40, 0;
	mov.u32 	%r41, %r38;
	mov.u32 	%r42, %r40;

$L__BB25_11:
	not.b32 	%r28, %r40;
	cvt.u64.u32 	%rd42, %r28;
	add.s64 	%rd43, %rd42, %rd24;
	cvt.u64.u32 	%rd11, %r41;
	shl.b64 	%rd44, %rd43, 3;
	and.b64  	%rd45, %rd44, 34359738360;
	add.s64 	%rd12, %rd3, %rd45;
	ld.global.u64 	%rd13, [%rd12];
	and.b64  	%rd46, %rd13, -4294967296;
	setp.eq.s64 	%p12, %rd46, 0;
	@%p12 bra 	$L__BB25_13;

	div.u64 	%rd62, %rd11, %rd13;
	mul.lo.s64 	%rd47, %rd62, %rd13;
	sub.s64 	%rd63, %rd11, %rd47;
	bra.uni 	$L__BB25_14;

$L__BB25_13:
	cvt.u32.u64 	%r29, %rd13;
	cvt.u32.u64 	%r30, %rd11;
	div.u32 	%r31, %r30, %r29;
	mul.lo.s32 	%r32, %r31, %r29;
	sub.s32 	%r33, %r30, %r32;
	cvt.u64.u32 	%rd62, %r31;
	cvt.u64.u32 	%rd63, %r33;

$L__BB25_14:
	shl.b64 	%rd48, %rd24, 3;
	add.s64 	%rd49, %rd12, %rd48;
	ld.global.u64 	%rd50, [%rd49];
	mul.lo.s64 	%rd51, %rd50, %rd63;
	cvt.u32.u64 	%r34, %rd51;
	add.s32 	%r42, %r42, %r34;
	cvt.u32.u64 	%r41, %rd62;
	add.s32 	%r40, %r40, 1;
	cvt.u64.u32 	%rd52, %r40;
	setp.lt.u64 	%p13, %rd52, %rd24;
	@%p13 bra 	$L__BB25_11;

	cvt.u64.u32 	%rd53, %r42;
	add.s64 	%rd54, %rd2, %rd53;
	ld.global.u8 	%r35, [%rd54];
	shl.b64 	%rd55, %rd60, 2;
	add.s64 	%rd56, %rd1, %rd55;
	st.global.u32 	[%rd56], %r35;
	add.s32 	%r38, %r38, %r8;
	cvt.u64.u32 	%rd60, %r38;
	setp.lt.u64 	%p14, %rd60, %rd23;
	@%p14 bra 	$L__BB25_10;
	bra.uni 	$L__BB25_17;

$L__BB25_16:
	ld.global.u8 	%r36, [%rd2];
	shl.b64 	%rd57, %rd60, 2;
	add.s64 	%rd58, %rd1, %rd57;
	st.global.u32 	[%rd58], %r36;
	add.s32 	%r38, %r38, %r8;
	cvt.u64.u32 	%rd60, %r38;
	setp.lt.u64 	%p15, %rd60, %rd23;
	@%p15 bra 	$L__BB25_16;
	bra.uni 	$L__BB25_17;

$L__BB25_5:
	setp.ge.u64 	%p8, %rd60, %rd23;
	@%p8 bra 	$L__BB25_17;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;

$L__BB25_7:
	add.s64 	%rd39, %rd2, %rd60;
	ld.global.u8 	%r24, [%rd39];
	shl.b64 	%rd40, %rd60, 2;
	add.s64 	%rd41, %rd1, %rd40;
	st.global.u32 	[%rd41], %r24;
	add.s32 	%r38, %r38, %r5;
	cvt.u64.u32 	%rd60, %r38;
	setp.lt.u64 	%p9, %rd60, %rd23;
	@%p9 bra 	$L__BB25_7;

$L__BB25_17:
	ret;

}
	// .globl	cast_u8_u8
.visible .entry cast_u8_u8(
	.param .u64 cast_u8_u8_param_0,
	.param .u64 cast_u8_u8_param_1,
	.param .u64 cast_u8_u8_param_2,
	.param .u64 cast_u8_u8_param_3,
	.param .u64 cast_u8_u8_param_4
)
{
	.reg .pred 	%p<17>;
	.reg .b16 	%rs<4>;
	.reg .b32 	%r<41>;
	.reg .b64 	%rd<62>;


	ld.param.u64 	%rd23, [cast_u8_u8_param_0];
	ld.param.u64 	%rd24, [cast_u8_u8_param_1];
	ld.param.u64 	%rd25, [cast_u8_u8_param_2];
	ld.param.u64 	%rd26, [cast_u8_u8_param_3];
	ld.param.u64 	%rd27, [cast_u8_u8_param_4];
	cvta.to.global.u64 	%rd1, %rd27;
	cvta.to.global.u64 	%rd2, %rd26;
	cvta.to.global.u64 	%rd3, %rd25;
	setp.eq.s64 	%p3, %rd24, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p16, %p2;
	@%p3 bra 	$L__BB26_4;

	mov.u64 	%rd56, 1;
	mov.u32 	%r34, 0;

$L__BB26_2:
	not.b32 	%r20, %r34;
	cvt.u64.u32 	%rd29, %r20;
	add.s64 	%rd30, %rd29, %rd24;
	and.b64  	%rd5, %rd30, 4294967295;
	add.s64 	%rd31, %rd5, %rd24;
	shl.b64 	%rd32, %rd31, 3;
	add.s64 	%rd33, %rd3, %rd32;
	ld.global.u64 	%rd34, [%rd33];
	setp.ne.s64 	%p5, %rd56, %rd34;
	mov.pred 	%p16, -1;
	@%p5 bra 	$L__BB26_4;

	shl.b64 	%rd35, %rd5, 3;
	add.s64 	%rd36, %rd3, %rd35;
	ld.global.u64 	%rd37, [%rd36];
	mul.lo.s64 	%rd56, %rd37, %rd56;
	add.s32 	%r34, %r34, 1;
	cvt.u64.u32 	%rd38, %r34;
	setp.lt.u64 	%p7, %rd38, %rd24;
	mov.pred 	%p16, %p2;
	@%p7 bra 	$L__BB26_2;

$L__BB26_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r35, %r21, %r3, %r22;
	cvt.u64.u32 	%rd57, %r35;
	@%p16 bra 	$L__BB26_8;
	bra.uni 	$L__BB26_5;

$L__BB26_8:
	setp.ge.u64 	%p10, %rd57, %rd23;
	@%p10 bra 	$L__BB26_17;

	mov.u32 	%r24, %nctaid.x;
	mul.lo.s32 	%r8, %r3, %r24;
	@%p3 bra 	$L__BB26_16;

$L__BB26_10:
	mov.u32 	%r37, 0;
	mov.u32 	%r38, %r35;
	mov.u32 	%r39, %r37;

$L__BB26_11:
	not.b32 	%r27, %r37;
	cvt.u64.u32 	%rd41, %r27;
	add.s64 	%rd42, %rd41, %rd24;
	cvt.u64.u32 	%rd11, %r38;
	shl.b64 	%rd43, %rd42, 3;
	and.b64  	%rd44, %rd43, 34359738360;
	add.s64 	%rd12, %rd3, %rd44;
	ld.global.u64 	%rd13, [%rd12];
	and.b64  	%rd45, %rd13, -4294967296;
	setp.eq.s64 	%p12, %rd45, 0;
	@%p12 bra 	$L__BB26_13;

	div.u64 	%rd59, %rd11, %rd13;
	mul.lo.s64 	%rd46, %rd59, %rd13;
	sub.s64 	%rd60, %rd11, %rd46;
	bra.uni 	$L__BB26_14;

$L__BB26_13:
	cvt.u32.u64 	%r28, %rd13;
	cvt.u32.u64 	%r29, %rd11;
	div.u32 	%r30, %r29, %r28;
	mul.lo.s32 	%r31, %r30, %r28;
	sub.s32 	%r32, %r29, %r31;
	cvt.u64.u32 	%rd59, %r30;
	cvt.u64.u32 	%rd60, %r32;

$L__BB26_14:
	shl.b64 	%rd47, %rd24, 3;
	add.s64 	%rd48, %rd12, %rd47;
	ld.global.u64 	%rd49, [%rd48];
	mul.lo.s64 	%rd50, %rd49, %rd60;
	cvt.u32.u64 	%r33, %rd50;
	add.s32 	%r39, %r39, %r33;
	cvt.u32.u64 	%r38, %rd59;
	add.s32 	%r37, %r37, 1;
	cvt.u64.u32 	%rd51, %r37;
	setp.lt.u64 	%p13, %rd51, %rd24;
	@%p13 bra 	$L__BB26_11;

	cvt.u64.u32 	%rd52, %r39;
	add.s64 	%rd53, %rd2, %rd52;
	ld.global.u8 	%rs2, [%rd53];
	add.s64 	%rd54, %rd1, %rd57;
	st.global.u8 	[%rd54], %rs2;
	add.s32 	%r35, %r35, %r8;
	cvt.u64.u32 	%rd57, %r35;
	setp.lt.u64 	%p14, %rd57, %rd23;
	@%p14 bra 	$L__BB26_10;
	bra.uni 	$L__BB26_17;

$L__BB26_16:
	ld.global.u8 	%rs3, [%rd2];
	add.s64 	%rd55, %rd1, %rd57;
	st.global.u8 	[%rd55], %rs3;
	add.s32 	%r35, %r35, %r8;
	cvt.u64.u32 	%rd57, %r35;
	setp.lt.u64 	%p15, %rd57, %rd23;
	@%p15 bra 	$L__BB26_16;
	bra.uni 	$L__BB26_17;

$L__BB26_5:
	setp.ge.u64 	%p8, %rd57, %rd23;
	@%p8 bra 	$L__BB26_17;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;

$L__BB26_7:
	add.s64 	%rd39, %rd2, %rd57;
	ld.global.u8 	%rs1, [%rd39];
	add.s64 	%rd40, %rd1, %rd57;
	st.global.u8 	[%rd40], %rs1;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd57, %r35;
	setp.lt.u64 	%p9, %rd57, %rd23;
	@%p9 bra 	$L__BB26_7;

$L__BB26_17:
	ret;

}
	// .globl	cast_u8_i64
.visible .entry cast_u8_i64(
	.param .u64 cast_u8_i64_param_0,
	.param .u64 cast_u8_i64_param_1,
	.param .u64 cast_u8_i64_param_2,
	.param .u64 cast_u8_i64_param_3,
	.param .u64 cast_u8_i64_param_4
)
{
	.reg .pred 	%p<17>;
	.reg .b32 	%r<41>;
	.reg .b64 	%rd<68>;


	ld.param.u64 	%rd23, [cast_u8_i64_param_0];
	ld.param.u64 	%rd24, [cast_u8_i64_param_1];
	ld.param.u64 	%rd25, [cast_u8_i64_param_2];
	ld.param.u64 	%rd26, [cast_u8_i64_param_3];
	ld.param.u64 	%rd27, [cast_u8_i64_param_4];
	cvta.to.global.u64 	%rd1, %rd27;
	cvta.to.global.u64 	%rd2, %rd26;
	cvta.to.global.u64 	%rd3, %rd25;
	setp.eq.s64 	%p3, %rd24, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p16, %p2;
	@%p3 bra 	$L__BB27_4;

	mov.u64 	%rd62, 1;
	mov.u32 	%r34, 0;

$L__BB27_2:
	not.b32 	%r20, %r34;
	cvt.u64.u32 	%rd29, %r20;
	add.s64 	%rd30, %rd29, %rd24;
	and.b64  	%rd5, %rd30, 4294967295;
	add.s64 	%rd31, %rd5, %rd24;
	shl.b64 	%rd32, %rd31, 3;
	add.s64 	%rd33, %rd3, %rd32;
	ld.global.u64 	%rd34, [%rd33];
	setp.ne.s64 	%p5, %rd62, %rd34;
	mov.pred 	%p16, -1;
	@%p5 bra 	$L__BB27_4;

	shl.b64 	%rd35, %rd5, 3;
	add.s64 	%rd36, %rd3, %rd35;
	ld.global.u64 	%rd37, [%rd36];
	mul.lo.s64 	%rd62, %rd37, %rd62;
	add.s32 	%r34, %r34, 1;
	cvt.u64.u32 	%rd38, %r34;
	setp.lt.u64 	%p7, %rd38, %rd24;
	mov.pred 	%p16, %p2;
	@%p7 bra 	$L__BB27_2;

$L__BB27_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r35, %r21, %r3, %r22;
	cvt.u64.u32 	%rd63, %r35;
	@%p16 bra 	$L__BB27_8;
	bra.uni 	$L__BB27_5;

$L__BB27_8:
	setp.ge.u64 	%p10, %rd63, %rd23;
	@%p10 bra 	$L__BB27_17;

	mov.u32 	%r24, %nctaid.x;
	mul.lo.s32 	%r8, %r3, %r24;
	@%p3 bra 	$L__BB27_16;

$L__BB27_10:
	mov.u32 	%r37, 0;
	mov.u32 	%r38, %r35;
	mov.u32 	%r39, %r37;

$L__BB27_11:
	not.b32 	%r27, %r37;
	cvt.u64.u32 	%rd43, %r27;
	add.s64 	%rd44, %rd43, %rd24;
	cvt.u64.u32 	%rd11, %r38;
	shl.b64 	%rd45, %rd44, 3;
	and.b64  	%rd46, %rd45, 34359738360;
	add.s64 	%rd12, %rd3, %rd46;
	ld.global.u64 	%rd13, [%rd12];
	and.b64  	%rd47, %rd13, -4294967296;
	setp.eq.s64 	%p12, %rd47, 0;
	@%p12 bra 	$L__BB27_13;

	div.u64 	%rd65, %rd11, %rd13;
	mul.lo.s64 	%rd48, %rd65, %rd13;
	sub.s64 	%rd66, %rd11, %rd48;
	bra.uni 	$L__BB27_14;

$L__BB27_13:
	cvt.u32.u64 	%r28, %rd13;
	cvt.u32.u64 	%r29, %rd11;
	div.u32 	%r30, %r29, %r28;
	mul.lo.s32 	%r31, %r30, %r28;
	sub.s32 	%r32, %r29, %r31;
	cvt.u64.u32 	%rd65, %r30;
	cvt.u64.u32 	%rd66, %r32;

$L__BB27_14:
	shl.b64 	%rd49, %rd24, 3;
	add.s64 	%rd50, %rd12, %rd49;
	ld.global.u64 	%rd51, [%rd50];
	mul.lo.s64 	%rd52, %rd51, %rd66;
	cvt.u32.u64 	%r33, %rd52;
	add.s32 	%r39, %r39, %r33;
	cvt.u32.u64 	%r38, %rd65;
	add.s32 	%r37, %r37, 1;
	cvt.u64.u32 	%rd53, %r37;
	setp.lt.u64 	%p13, %rd53, %rd24;
	@%p13 bra 	$L__BB27_11;

	cvt.u64.u32 	%rd54, %r39;
	add.s64 	%rd55, %rd2, %rd54;
	ld.global.u8 	%rd56, [%rd55];
	shl.b64 	%rd57, %rd63, 3;
	add.s64 	%rd58, %rd1, %rd57;
	st.global.u64 	[%rd58], %rd56;
	add.s32 	%r35, %r35, %r8;
	cvt.u64.u32 	%rd63, %r35;
	setp.lt.u64 	%p14, %rd63, %rd23;
	@%p14 bra 	$L__BB27_10;
	bra.uni 	$L__BB27_17;

$L__BB27_16:
	ld.global.u8 	%rd59, [%rd2];
	shl.b64 	%rd60, %rd63, 3;
	add.s64 	%rd61, %rd1, %rd60;
	st.global.u64 	[%rd61], %rd59;
	add.s32 	%r35, %r35, %r8;
	cvt.u64.u32 	%rd63, %r35;
	setp.lt.u64 	%p15, %rd63, %rd23;
	@%p15 bra 	$L__BB27_16;
	bra.uni 	$L__BB27_17;

$L__BB27_5:
	setp.ge.u64 	%p8, %rd63, %rd23;
	@%p8 bra 	$L__BB27_17;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;

$L__BB27_7:
	add.s64 	%rd39, %rd2, %rd63;
	ld.global.u8 	%rd40, [%rd39];
	shl.b64 	%rd41, %rd63, 3;
	add.s64 	%rd42, %rd1, %rd41;
	st.global.u64 	[%rd42], %rd40;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd63, %r35;
	setp.lt.u64 	%p9, %rd63, %rd23;
	@%p9 bra 	$L__BB27_7;

$L__BB27_17:
	ret;

}
	// .globl	cast_u8_f32
.visible .entry cast_u8_f32(
	.param .u64 cast_u8_f32_param_0,
	.param .u64 cast_u8_f32_param_1,
	.param .u64 cast_u8_f32_param_2,
	.param .u64 cast_u8_f32_param_3,
	.param .u64 cast_u8_f32_param_4
)
{
	.reg .pred 	%p<17>;
	.reg .b16 	%rs<4>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<41>;
	.reg .b64 	%rd<65>;


	ld.param.u64 	%rd23, [cast_u8_f32_param_0];
	ld.param.u64 	%rd24, [cast_u8_f32_param_1];
	ld.param.u64 	%rd25, [cast_u8_f32_param_2];
	ld.param.u64 	%rd26, [cast_u8_f32_param_3];
	ld.param.u64 	%rd27, [cast_u8_f32_param_4];
	cvta.to.global.u64 	%rd1, %rd27;
	cvta.to.global.u64 	%rd2, %rd26;
	cvta.to.global.u64 	%rd3, %rd25;
	setp.eq.s64 	%p3, %rd24, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p16, %p2;
	@%p3 bra 	$L__BB28_4;

	mov.u64 	%rd59, 1;
	mov.u32 	%r34, 0;

$L__BB28_2:
	not.b32 	%r20, %r34;
	cvt.u64.u32 	%rd29, %r20;
	add.s64 	%rd30, %rd29, %rd24;
	and.b64  	%rd5, %rd30, 4294967295;
	add.s64 	%rd31, %rd5, %rd24;
	shl.b64 	%rd32, %rd31, 3;
	add.s64 	%rd33, %rd3, %rd32;
	ld.global.u64 	%rd34, [%rd33];
	setp.ne.s64 	%p5, %rd59, %rd34;
	mov.pred 	%p16, -1;
	@%p5 bra 	$L__BB28_4;

	shl.b64 	%rd35, %rd5, 3;
	add.s64 	%rd36, %rd3, %rd35;
	ld.global.u64 	%rd37, [%rd36];
	mul.lo.s64 	%rd59, %rd37, %rd59;
	add.s32 	%r34, %r34, 1;
	cvt.u64.u32 	%rd38, %r34;
	setp.lt.u64 	%p7, %rd38, %rd24;
	mov.pred 	%p16, %p2;
	@%p7 bra 	$L__BB28_2;

$L__BB28_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r35, %r21, %r3, %r22;
	cvt.u64.u32 	%rd60, %r35;
	@%p16 bra 	$L__BB28_8;
	bra.uni 	$L__BB28_5;

$L__BB28_8:
	setp.ge.u64 	%p10, %rd60, %rd23;
	@%p10 bra 	$L__BB28_17;

	mov.u32 	%r24, %nctaid.x;
	mul.lo.s32 	%r8, %r3, %r24;
	@%p3 bra 	$L__BB28_16;

$L__BB28_10:
	mov.u32 	%r37, 0;
	mov.u32 	%r38, %r35;
	mov.u32 	%r39, %r37;

$L__BB28_11:
	not.b32 	%r27, %r37;
	cvt.u64.u32 	%rd42, %r27;
	add.s64 	%rd43, %rd42, %rd24;
	cvt.u64.u32 	%rd11, %r38;
	shl.b64 	%rd44, %rd43, 3;
	and.b64  	%rd45, %rd44, 34359738360;
	add.s64 	%rd12, %rd3, %rd45;
	ld.global.u64 	%rd13, [%rd12];
	and.b64  	%rd46, %rd13, -4294967296;
	setp.eq.s64 	%p12, %rd46, 0;
	@%p12 bra 	$L__BB28_13;

	div.u64 	%rd62, %rd11, %rd13;
	mul.lo.s64 	%rd47, %rd62, %rd13;
	sub.s64 	%rd63, %rd11, %rd47;
	bra.uni 	$L__BB28_14;

$L__BB28_13:
	cvt.u32.u64 	%r28, %rd13;
	cvt.u32.u64 	%r29, %rd11;
	div.u32 	%r30, %r29, %r28;
	mul.lo.s32 	%r31, %r30, %r28;
	sub.s32 	%r32, %r29, %r31;
	cvt.u64.u32 	%rd62, %r30;
	cvt.u64.u32 	%rd63, %r32;

$L__BB28_14:
	shl.b64 	%rd48, %rd24, 3;
	add.s64 	%rd49, %rd12, %rd48;
	ld.global.u64 	%rd50, [%rd49];
	mul.lo.s64 	%rd51, %rd50, %rd63;
	cvt.u32.u64 	%r33, %rd51;
	add.s32 	%r39, %r39, %r33;
	cvt.u32.u64 	%r38, %rd62;
	add.s32 	%r37, %r37, 1;
	cvt.u64.u32 	%rd52, %r37;
	setp.lt.u64 	%p13, %rd52, %rd24;
	@%p13 bra 	$L__BB28_11;

	cvt.u64.u32 	%rd53, %r39;
	add.s64 	%rd54, %rd2, %rd53;
	ld.global.u8 	%rs2, [%rd54];
	cvt.rn.f32.u16 	%f2, %rs2;
	shl.b64 	%rd55, %rd60, 2;
	add.s64 	%rd56, %rd1, %rd55;
	st.global.f32 	[%rd56], %f2;
	add.s32 	%r35, %r35, %r8;
	cvt.u64.u32 	%rd60, %r35;
	setp.lt.u64 	%p14, %rd60, %rd23;
	@%p14 bra 	$L__BB28_10;
	bra.uni 	$L__BB28_17;

$L__BB28_16:
	ld.global.u8 	%rs3, [%rd2];
	cvt.rn.f32.u16 	%f3, %rs3;
	shl.b64 	%rd57, %rd60, 2;
	add.s64 	%rd58, %rd1, %rd57;
	st.global.f32 	[%rd58], %f3;
	add.s32 	%r35, %r35, %r8;
	cvt.u64.u32 	%rd60, %r35;
	setp.lt.u64 	%p15, %rd60, %rd23;
	@%p15 bra 	$L__BB28_16;
	bra.uni 	$L__BB28_17;

$L__BB28_5:
	setp.ge.u64 	%p8, %rd60, %rd23;
	@%p8 bra 	$L__BB28_17;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;

$L__BB28_7:
	add.s64 	%rd39, %rd2, %rd60;
	ld.global.u8 	%rs1, [%rd39];
	cvt.rn.f32.u16 	%f1, %rs1;
	shl.b64 	%rd40, %rd60, 2;
	add.s64 	%rd41, %rd1, %rd40;
	st.global.f32 	[%rd41], %f1;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd60, %r35;
	setp.lt.u64 	%p9, %rd60, %rd23;
	@%p9 bra 	$L__BB28_7;

$L__BB28_17:
	ret;

}
	// .globl	cast_u8_f64
.visible .entry cast_u8_f64(
	.param .u64 cast_u8_f64_param_0,
	.param .u64 cast_u8_f64_param_1,
	.param .u64 cast_u8_f64_param_2,
	.param .u64 cast_u8_f64_param_3,
	.param .u64 cast_u8_f64_param_4
)
{
	.reg .pred 	%p<17>;
	.reg .b16 	%rs<4>;
	.reg .b32 	%r<41>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<65>;


	ld.param.u64 	%rd23, [cast_u8_f64_param_0];
	ld.param.u64 	%rd24, [cast_u8_f64_param_1];
	ld.param.u64 	%rd25, [cast_u8_f64_param_2];
	ld.param.u64 	%rd26, [cast_u8_f64_param_3];
	ld.param.u64 	%rd27, [cast_u8_f64_param_4];
	cvta.to.global.u64 	%rd1, %rd27;
	cvta.to.global.u64 	%rd2, %rd26;
	cvta.to.global.u64 	%rd3, %rd25;
	setp.eq.s64 	%p3, %rd24, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p16, %p2;
	@%p3 bra 	$L__BB29_4;

	mov.u64 	%rd59, 1;
	mov.u32 	%r34, 0;

$L__BB29_2:
	not.b32 	%r20, %r34;
	cvt.u64.u32 	%rd29, %r20;
	add.s64 	%rd30, %rd29, %rd24;
	and.b64  	%rd5, %rd30, 4294967295;
	add.s64 	%rd31, %rd5, %rd24;
	shl.b64 	%rd32, %rd31, 3;
	add.s64 	%rd33, %rd3, %rd32;
	ld.global.u64 	%rd34, [%rd33];
	setp.ne.s64 	%p5, %rd59, %rd34;
	mov.pred 	%p16, -1;
	@%p5 bra 	$L__BB29_4;

	shl.b64 	%rd35, %rd5, 3;
	add.s64 	%rd36, %rd3, %rd35;
	ld.global.u64 	%rd37, [%rd36];
	mul.lo.s64 	%rd59, %rd37, %rd59;
	add.s32 	%r34, %r34, 1;
	cvt.u64.u32 	%rd38, %r34;
	setp.lt.u64 	%p7, %rd38, %rd24;
	mov.pred 	%p16, %p2;
	@%p7 bra 	$L__BB29_2;

$L__BB29_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r35, %r21, %r3, %r22;
	cvt.u64.u32 	%rd60, %r35;
	@%p16 bra 	$L__BB29_8;
	bra.uni 	$L__BB29_5;

$L__BB29_8:
	setp.ge.u64 	%p10, %rd60, %rd23;
	@%p10 bra 	$L__BB29_17;

	mov.u32 	%r24, %nctaid.x;
	mul.lo.s32 	%r8, %r3, %r24;
	@%p3 bra 	$L__BB29_16;

$L__BB29_10:
	mov.u32 	%r37, 0;
	mov.u32 	%r38, %r35;
	mov.u32 	%r39, %r37;

$L__BB29_11:
	not.b32 	%r27, %r37;
	cvt.u64.u32 	%rd42, %r27;
	add.s64 	%rd43, %rd42, %rd24;
	cvt.u64.u32 	%rd11, %r38;
	shl.b64 	%rd44, %rd43, 3;
	and.b64  	%rd45, %rd44, 34359738360;
	add.s64 	%rd12, %rd3, %rd45;
	ld.global.u64 	%rd13, [%rd12];
	and.b64  	%rd46, %rd13, -4294967296;
	setp.eq.s64 	%p12, %rd46, 0;
	@%p12 bra 	$L__BB29_13;

	div.u64 	%rd62, %rd11, %rd13;
	mul.lo.s64 	%rd47, %rd62, %rd13;
	sub.s64 	%rd63, %rd11, %rd47;
	bra.uni 	$L__BB29_14;

$L__BB29_13:
	cvt.u32.u64 	%r28, %rd13;
	cvt.u32.u64 	%r29, %rd11;
	div.u32 	%r30, %r29, %r28;
	mul.lo.s32 	%r31, %r30, %r28;
	sub.s32 	%r32, %r29, %r31;
	cvt.u64.u32 	%rd62, %r30;
	cvt.u64.u32 	%rd63, %r32;

$L__BB29_14:
	shl.b64 	%rd48, %rd24, 3;
	add.s64 	%rd49, %rd12, %rd48;
	ld.global.u64 	%rd50, [%rd49];
	mul.lo.s64 	%rd51, %rd50, %rd63;
	cvt.u32.u64 	%r33, %rd51;
	add.s32 	%r39, %r39, %r33;
	cvt.u32.u64 	%r38, %rd62;
	add.s32 	%r37, %r37, 1;
	cvt.u64.u32 	%rd52, %r37;
	setp.lt.u64 	%p13, %rd52, %rd24;
	@%p13 bra 	$L__BB29_11;

	cvt.u64.u32 	%rd53, %r39;
	add.s64 	%rd54, %rd2, %rd53;
	ld.global.u8 	%rs2, [%rd54];
	cvt.rn.f64.u16 	%fd2, %rs2;
	shl.b64 	%rd55, %rd60, 3;
	add.s64 	%rd56, %rd1, %rd55;
	st.global.f64 	[%rd56], %fd2;
	add.s32 	%r35, %r35, %r8;
	cvt.u64.u32 	%rd60, %r35;
	setp.lt.u64 	%p14, %rd60, %rd23;
	@%p14 bra 	$L__BB29_10;
	bra.uni 	$L__BB29_17;

$L__BB29_16:
	ld.global.u8 	%rs3, [%rd2];
	cvt.rn.f64.u16 	%fd3, %rs3;
	shl.b64 	%rd57, %rd60, 3;
	add.s64 	%rd58, %rd1, %rd57;
	st.global.f64 	[%rd58], %fd3;
	add.s32 	%r35, %r35, %r8;
	cvt.u64.u32 	%rd60, %r35;
	setp.lt.u64 	%p15, %rd60, %rd23;
	@%p15 bra 	$L__BB29_16;
	bra.uni 	$L__BB29_17;

$L__BB29_5:
	setp.ge.u64 	%p8, %rd60, %rd23;
	@%p8 bra 	$L__BB29_17;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;

$L__BB29_7:
	add.s64 	%rd39, %rd2, %rd60;
	ld.global.u8 	%rs1, [%rd39];
	cvt.rn.f64.u16 	%fd1, %rs1;
	shl.b64 	%rd40, %rd60, 3;
	add.s64 	%rd41, %rd1, %rd40;
	st.global.f64 	[%rd41], %fd1;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd60, %r35;
	setp.lt.u64 	%p9, %rd60, %rd23;
	@%p9 bra 	$L__BB29_7;

$L__BB29_17:
	ret;

}
	// .globl	cast_i64_u32
.visible .entry cast_i64_u32(
	.param .u64 cast_i64_u32_param_0,
	.param .u64 cast_i64_u32_param_1,
	.param .u64 cast_i64_u32_param_2,
	.param .u64 cast_i64_u32_param_3,
	.param .u64 cast_i64_u32_param_4
)
{
	.reg .pred 	%p<17>;
	.reg .b32 	%r<41>;
	.reg .b64 	%rd<69>;


	ld.param.u64 	%rd23, [cast_i64_u32_param_0];
	ld.param.u64 	%rd24, [cast_i64_u32_param_1];
	ld.param.u64 	%rd25, [cast_i64_u32_param_2];
	ld.param.u64 	%rd26, [cast_i64_u32_param_3];
	ld.param.u64 	%rd27, [cast_i64_u32_param_4];
	cvta.to.global.u64 	%rd1, %rd27;
	cvta.to.global.u64 	%rd2, %rd26;
	cvta.to.global.u64 	%rd3, %rd25;
	setp.eq.s64 	%p3, %rd24, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p16, %p2;
	@%p3 bra 	$L__BB30_4;

	mov.u64 	%rd63, 1;
	mov.u32 	%r34, 0;

$L__BB30_2:
	not.b32 	%r20, %r34;
	cvt.u64.u32 	%rd29, %r20;
	add.s64 	%rd30, %rd29, %rd24;
	and.b64  	%rd5, %rd30, 4294967295;
	add.s64 	%rd31, %rd5, %rd24;
	shl.b64 	%rd32, %rd31, 3;
	add.s64 	%rd33, %rd3, %rd32;
	ld.global.u64 	%rd34, [%rd33];
	setp.ne.s64 	%p5, %rd63, %rd34;
	mov.pred 	%p16, -1;
	@%p5 bra 	$L__BB30_4;

	shl.b64 	%rd35, %rd5, 3;
	add.s64 	%rd36, %rd3, %rd35;
	ld.global.u64 	%rd37, [%rd36];
	mul.lo.s64 	%rd63, %rd37, %rd63;
	add.s32 	%r34, %r34, 1;
	cvt.u64.u32 	%rd38, %r34;
	setp.lt.u64 	%p7, %rd38, %rd24;
	mov.pred 	%p16, %p2;
	@%p7 bra 	$L__BB30_2;

$L__BB30_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r35, %r21, %r3, %r22;
	cvt.u64.u32 	%rd64, %r35;
	@%p16 bra 	$L__BB30_8;
	bra.uni 	$L__BB30_5;

$L__BB30_8:
	setp.ge.u64 	%p10, %rd64, %rd23;
	@%p10 bra 	$L__BB30_17;

	mov.u32 	%r24, %nctaid.x;
	mul.lo.s32 	%r8, %r3, %r24;
	@%p3 bra 	$L__BB30_16;

$L__BB30_10:
	mov.u32 	%r37, 0;
	mov.u32 	%r38, %r35;
	mov.u32 	%r39, %r37;

$L__BB30_11:
	not.b32 	%r27, %r37;
	cvt.u64.u32 	%rd44, %r27;
	add.s64 	%rd45, %rd44, %rd24;
	cvt.u64.u32 	%rd11, %r38;
	shl.b64 	%rd46, %rd45, 3;
	and.b64  	%rd47, %rd46, 34359738360;
	add.s64 	%rd12, %rd3, %rd47;
	ld.global.u64 	%rd13, [%rd12];
	and.b64  	%rd48, %rd13, -4294967296;
	setp.eq.s64 	%p12, %rd48, 0;
	@%p12 bra 	$L__BB30_13;

	div.u64 	%rd66, %rd11, %rd13;
	mul.lo.s64 	%rd49, %rd66, %rd13;
	sub.s64 	%rd67, %rd11, %rd49;
	bra.uni 	$L__BB30_14;

$L__BB30_13:
	cvt.u32.u64 	%r28, %rd13;
	cvt.u32.u64 	%r29, %rd11;
	div.u32 	%r30, %r29, %r28;
	mul.lo.s32 	%r31, %r30, %r28;
	sub.s32 	%r32, %r29, %r31;
	cvt.u64.u32 	%rd66, %r30;
	cvt.u64.u32 	%rd67, %r32;

$L__BB30_14:
	shl.b64 	%rd50, %rd24, 3;
	add.s64 	%rd51, %rd12, %rd50;
	ld.global.u64 	%rd52, [%rd51];
	mul.lo.s64 	%rd53, %rd52, %rd67;
	cvt.u32.u64 	%r33, %rd53;
	add.s32 	%r39, %r39, %r33;
	cvt.u32.u64 	%r38, %rd66;
	add.s32 	%r37, %r37, 1;
	cvt.u64.u32 	%rd54, %r37;
	setp.lt.u64 	%p13, %rd54, %rd24;
	@%p13 bra 	$L__BB30_11;

	mul.wide.u32 	%rd55, %r39, 8;
	add.s64 	%rd56, %rd2, %rd55;
	ld.global.u64 	%rd57, [%rd56];
	shl.b64 	%rd58, %rd64, 2;
	add.s64 	%rd59, %rd1, %rd58;
	st.global.u32 	[%rd59], %rd57;
	add.s32 	%r35, %r35, %r8;
	cvt.u64.u32 	%rd64, %r35;
	setp.lt.u64 	%p14, %rd64, %rd23;
	@%p14 bra 	$L__BB30_10;
	bra.uni 	$L__BB30_17;

$L__BB30_16:
	ld.global.u64 	%rd60, [%rd2];
	shl.b64 	%rd61, %rd64, 2;
	add.s64 	%rd62, %rd1, %rd61;
	st.global.u32 	[%rd62], %rd60;
	add.s32 	%r35, %r35, %r8;
	cvt.u64.u32 	%rd64, %r35;
	setp.lt.u64 	%p15, %rd64, %rd23;
	@%p15 bra 	$L__BB30_16;
	bra.uni 	$L__BB30_17;

$L__BB30_5:
	setp.ge.u64 	%p8, %rd64, %rd23;
	@%p8 bra 	$L__BB30_17;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;

$L__BB30_7:
	shl.b64 	%rd39, %rd64, 3;
	add.s64 	%rd40, %rd2, %rd39;
	ld.global.u64 	%rd41, [%rd40];
	shl.b64 	%rd42, %rd64, 2;
	add.s64 	%rd43, %rd1, %rd42;
	st.global.u32 	[%rd43], %rd41;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd64, %r35;
	setp.lt.u64 	%p9, %rd64, %rd23;
	@%p9 bra 	$L__BB30_7;

$L__BB30_17:
	ret;

}
	// .globl	cast_i64_u8
.visible .entry cast_i64_u8(
	.param .u64 cast_i64_u8_param_0,
	.param .u64 cast_i64_u8_param_1,
	.param .u64 cast_i64_u8_param_2,
	.param .u64 cast_i64_u8_param_3,
	.param .u64 cast_i64_u8_param_4
)
{
	.reg .pred 	%p<17>;
	.reg .b32 	%r<41>;
	.reg .b64 	%rd<66>;


	ld.param.u64 	%rd23, [cast_i64_u8_param_0];
	ld.param.u64 	%rd24, [cast_i64_u8_param_1];
	ld.param.u64 	%rd25, [cast_i64_u8_param_2];
	ld.param.u64 	%rd26, [cast_i64_u8_param_3];
	ld.param.u64 	%rd27, [cast_i64_u8_param_4];
	cvta.to.global.u64 	%rd1, %rd27;
	cvta.to.global.u64 	%rd2, %rd26;
	cvta.to.global.u64 	%rd3, %rd25;
	setp.eq.s64 	%p3, %rd24, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p16, %p2;
	@%p3 bra 	$L__BB31_4;

	mov.u64 	%rd60, 1;
	mov.u32 	%r34, 0;

$L__BB31_2:
	not.b32 	%r20, %r34;
	cvt.u64.u32 	%rd29, %r20;
	add.s64 	%rd30, %rd29, %rd24;
	and.b64  	%rd5, %rd30, 4294967295;
	add.s64 	%rd31, %rd5, %rd24;
	shl.b64 	%rd32, %rd31, 3;
	add.s64 	%rd33, %rd3, %rd32;
	ld.global.u64 	%rd34, [%rd33];
	setp.ne.s64 	%p5, %rd60, %rd34;
	mov.pred 	%p16, -1;
	@%p5 bra 	$L__BB31_4;

	shl.b64 	%rd35, %rd5, 3;
	add.s64 	%rd36, %rd3, %rd35;
	ld.global.u64 	%rd37, [%rd36];
	mul.lo.s64 	%rd60, %rd37, %rd60;
	add.s32 	%r34, %r34, 1;
	cvt.u64.u32 	%rd38, %r34;
	setp.lt.u64 	%p7, %rd38, %rd24;
	mov.pred 	%p16, %p2;
	@%p7 bra 	$L__BB31_2;

$L__BB31_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r35, %r21, %r3, %r22;
	cvt.u64.u32 	%rd61, %r35;
	@%p16 bra 	$L__BB31_8;
	bra.uni 	$L__BB31_5;

$L__BB31_8:
	setp.ge.u64 	%p10, %rd61, %rd23;
	@%p10 bra 	$L__BB31_17;

	mov.u32 	%r24, %nctaid.x;
	mul.lo.s32 	%r8, %r3, %r24;
	@%p3 bra 	$L__BB31_16;

$L__BB31_10:
	mov.u32 	%r37, 0;
	mov.u32 	%r38, %r35;
	mov.u32 	%r39, %r37;

$L__BB31_11:
	not.b32 	%r27, %r37;
	cvt.u64.u32 	%rd43, %r27;
	add.s64 	%rd44, %rd43, %rd24;
	cvt.u64.u32 	%rd11, %r38;
	shl.b64 	%rd45, %rd44, 3;
	and.b64  	%rd46, %rd45, 34359738360;
	add.s64 	%rd12, %rd3, %rd46;
	ld.global.u64 	%rd13, [%rd12];
	and.b64  	%rd47, %rd13, -4294967296;
	setp.eq.s64 	%p12, %rd47, 0;
	@%p12 bra 	$L__BB31_13;

	div.u64 	%rd63, %rd11, %rd13;
	mul.lo.s64 	%rd48, %rd63, %rd13;
	sub.s64 	%rd64, %rd11, %rd48;
	bra.uni 	$L__BB31_14;

$L__BB31_13:
	cvt.u32.u64 	%r28, %rd13;
	cvt.u32.u64 	%r29, %rd11;
	div.u32 	%r30, %r29, %r28;
	mul.lo.s32 	%r31, %r30, %r28;
	sub.s32 	%r32, %r29, %r31;
	cvt.u64.u32 	%rd63, %r30;
	cvt.u64.u32 	%rd64, %r32;

$L__BB31_14:
	shl.b64 	%rd49, %rd24, 3;
	add.s64 	%rd50, %rd12, %rd49;
	ld.global.u64 	%rd51, [%rd50];
	mul.lo.s64 	%rd52, %rd51, %rd64;
	cvt.u32.u64 	%r33, %rd52;
	add.s32 	%r39, %r39, %r33;
	cvt.u32.u64 	%r38, %rd63;
	add.s32 	%r37, %r37, 1;
	cvt.u64.u32 	%rd53, %r37;
	setp.lt.u64 	%p13, %rd53, %rd24;
	@%p13 bra 	$L__BB31_11;

	mul.wide.u32 	%rd54, %r39, 8;
	add.s64 	%rd55, %rd2, %rd54;
	ld.global.u64 	%rd56, [%rd55];
	add.s64 	%rd57, %rd1, %rd61;
	st.global.u8 	[%rd57], %rd56;
	add.s32 	%r35, %r35, %r8;
	cvt.u64.u32 	%rd61, %r35;
	setp.lt.u64 	%p14, %rd61, %rd23;
	@%p14 bra 	$L__BB31_10;
	bra.uni 	$L__BB31_17;

$L__BB31_16:
	ld.global.u64 	%rd58, [%rd2];
	add.s64 	%rd59, %rd1, %rd61;
	st.global.u8 	[%rd59], %rd58;
	add.s32 	%r35, %r35, %r8;
	cvt.u64.u32 	%rd61, %r35;
	setp.lt.u64 	%p15, %rd61, %rd23;
	@%p15 bra 	$L__BB31_16;
	bra.uni 	$L__BB31_17;

$L__BB31_5:
	setp.ge.u64 	%p8, %rd61, %rd23;
	@%p8 bra 	$L__BB31_17;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;

$L__BB31_7:
	shl.b64 	%rd39, %rd61, 3;
	add.s64 	%rd40, %rd2, %rd39;
	ld.global.u64 	%rd41, [%rd40];
	add.s64 	%rd42, %rd1, %rd61;
	st.global.u8 	[%rd42], %rd41;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd61, %r35;
	setp.lt.u64 	%p9, %rd61, %rd23;
	@%p9 bra 	$L__BB31_7;

$L__BB31_17:
	ret;

}
	// .globl	cast_i64_i64
.visible .entry cast_i64_i64(
	.param .u64 cast_i64_i64_param_0,
	.param .u64 cast_i64_i64_param_1,
	.param .u64 cast_i64_i64_param_2,
	.param .u64 cast_i64_i64_param_3,
	.param .u64 cast_i64_i64_param_4
)
{
	.reg .pred 	%p<17>;
	.reg .b32 	%r<41>;
	.reg .b64 	%rd<68>;


	ld.param.u64 	%rd23, [cast_i64_i64_param_0];
	ld.param.u64 	%rd24, [cast_i64_i64_param_1];
	ld.param.u64 	%rd25, [cast_i64_i64_param_2];
	ld.param.u64 	%rd26, [cast_i64_i64_param_3];
	ld.param.u64 	%rd27, [cast_i64_i64_param_4];
	cvta.to.global.u64 	%rd1, %rd27;
	cvta.to.global.u64 	%rd2, %rd26;
	cvta.to.global.u64 	%rd3, %rd25;
	setp.eq.s64 	%p3, %rd24, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p16, %p2;
	@%p3 bra 	$L__BB32_4;

	mov.u64 	%rd62, 1;
	mov.u32 	%r34, 0;

$L__BB32_2:
	not.b32 	%r20, %r34;
	cvt.u64.u32 	%rd29, %r20;
	add.s64 	%rd30, %rd29, %rd24;
	and.b64  	%rd5, %rd30, 4294967295;
	add.s64 	%rd31, %rd5, %rd24;
	shl.b64 	%rd32, %rd31, 3;
	add.s64 	%rd33, %rd3, %rd32;
	ld.global.u64 	%rd34, [%rd33];
	setp.ne.s64 	%p5, %rd62, %rd34;
	mov.pred 	%p16, -1;
	@%p5 bra 	$L__BB32_4;

	shl.b64 	%rd35, %rd5, 3;
	add.s64 	%rd36, %rd3, %rd35;
	ld.global.u64 	%rd37, [%rd36];
	mul.lo.s64 	%rd62, %rd37, %rd62;
	add.s32 	%r34, %r34, 1;
	cvt.u64.u32 	%rd38, %r34;
	setp.lt.u64 	%p7, %rd38, %rd24;
	mov.pred 	%p16, %p2;
	@%p7 bra 	$L__BB32_2;

$L__BB32_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r35, %r21, %r3, %r22;
	cvt.u64.u32 	%rd63, %r35;
	@%p16 bra 	$L__BB32_8;
	bra.uni 	$L__BB32_5;

$L__BB32_8:
	setp.ge.u64 	%p10, %rd63, %rd23;
	@%p10 bra 	$L__BB32_17;

	mov.u32 	%r24, %nctaid.x;
	mul.lo.s32 	%r8, %r3, %r24;
	@%p3 bra 	$L__BB32_16;

$L__BB32_10:
	mov.u32 	%r37, 0;
	mov.u32 	%r38, %r35;
	mov.u32 	%r39, %r37;

$L__BB32_11:
	not.b32 	%r27, %r37;
	cvt.u64.u32 	%rd43, %r27;
	add.s64 	%rd44, %rd43, %rd24;
	cvt.u64.u32 	%rd11, %r38;
	shl.b64 	%rd45, %rd44, 3;
	and.b64  	%rd46, %rd45, 34359738360;
	add.s64 	%rd12, %rd3, %rd46;
	ld.global.u64 	%rd13, [%rd12];
	and.b64  	%rd47, %rd13, -4294967296;
	setp.eq.s64 	%p12, %rd47, 0;
	@%p12 bra 	$L__BB32_13;

	div.u64 	%rd65, %rd11, %rd13;
	mul.lo.s64 	%rd48, %rd65, %rd13;
	sub.s64 	%rd66, %rd11, %rd48;
	bra.uni 	$L__BB32_14;

$L__BB32_13:
	cvt.u32.u64 	%r28, %rd13;
	cvt.u32.u64 	%r29, %rd11;
	div.u32 	%r30, %r29, %r28;
	mul.lo.s32 	%r31, %r30, %r28;
	sub.s32 	%r32, %r29, %r31;
	cvt.u64.u32 	%rd65, %r30;
	cvt.u64.u32 	%rd66, %r32;

$L__BB32_14:
	shl.b64 	%rd49, %rd24, 3;
	add.s64 	%rd50, %rd12, %rd49;
	ld.global.u64 	%rd51, [%rd50];
	mul.lo.s64 	%rd52, %rd51, %rd66;
	cvt.u32.u64 	%r33, %rd52;
	add.s32 	%r39, %r39, %r33;
	cvt.u32.u64 	%r38, %rd65;
	add.s32 	%r37, %r37, 1;
	cvt.u64.u32 	%rd53, %r37;
	setp.lt.u64 	%p13, %rd53, %rd24;
	@%p13 bra 	$L__BB32_11;

	mul.wide.u32 	%rd54, %r39, 8;
	add.s64 	%rd55, %rd2, %rd54;
	ld.global.u64 	%rd56, [%rd55];
	shl.b64 	%rd57, %rd63, 3;
	add.s64 	%rd58, %rd1, %rd57;
	st.global.u64 	[%rd58], %rd56;
	add.s32 	%r35, %r35, %r8;
	cvt.u64.u32 	%rd63, %r35;
	setp.lt.u64 	%p14, %rd63, %rd23;
	@%p14 bra 	$L__BB32_10;
	bra.uni 	$L__BB32_17;

$L__BB32_16:
	ld.global.u64 	%rd59, [%rd2];
	shl.b64 	%rd60, %rd63, 3;
	add.s64 	%rd61, %rd1, %rd60;
	st.global.u64 	[%rd61], %rd59;
	add.s32 	%r35, %r35, %r8;
	cvt.u64.u32 	%rd63, %r35;
	setp.lt.u64 	%p15, %rd63, %rd23;
	@%p15 bra 	$L__BB32_16;
	bra.uni 	$L__BB32_17;

$L__BB32_5:
	setp.ge.u64 	%p8, %rd63, %rd23;
	@%p8 bra 	$L__BB32_17;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;

$L__BB32_7:
	shl.b64 	%rd39, %rd63, 3;
	add.s64 	%rd40, %rd2, %rd39;
	ld.global.u64 	%rd41, [%rd40];
	add.s64 	%rd42, %rd1, %rd39;
	st.global.u64 	[%rd42], %rd41;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd63, %r35;
	setp.lt.u64 	%p9, %rd63, %rd23;
	@%p9 bra 	$L__BB32_7;

$L__BB32_17:
	ret;

}
	// .globl	cast_i64_f32
.visible .entry cast_i64_f32(
	.param .u64 cast_i64_f32_param_0,
	.param .u64 cast_i64_f32_param_1,
	.param .u64 cast_i64_f32_param_2,
	.param .u64 cast_i64_f32_param_3,
	.param .u64 cast_i64_f32_param_4
)
{
	.reg .pred 	%p<17>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<41>;
	.reg .b64 	%rd<69>;


	ld.param.u64 	%rd23, [cast_i64_f32_param_0];
	ld.param.u64 	%rd24, [cast_i64_f32_param_1];
	ld.param.u64 	%rd25, [cast_i64_f32_param_2];
	ld.param.u64 	%rd26, [cast_i64_f32_param_3];
	ld.param.u64 	%rd27, [cast_i64_f32_param_4];
	cvta.to.global.u64 	%rd1, %rd27;
	cvta.to.global.u64 	%rd2, %rd26;
	cvta.to.global.u64 	%rd3, %rd25;
	setp.eq.s64 	%p3, %rd24, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p16, %p2;
	@%p3 bra 	$L__BB33_4;

	mov.u64 	%rd63, 1;
	mov.u32 	%r34, 0;

$L__BB33_2:
	not.b32 	%r20, %r34;
	cvt.u64.u32 	%rd29, %r20;
	add.s64 	%rd30, %rd29, %rd24;
	and.b64  	%rd5, %rd30, 4294967295;
	add.s64 	%rd31, %rd5, %rd24;
	shl.b64 	%rd32, %rd31, 3;
	add.s64 	%rd33, %rd3, %rd32;
	ld.global.u64 	%rd34, [%rd33];
	setp.ne.s64 	%p5, %rd63, %rd34;
	mov.pred 	%p16, -1;
	@%p5 bra 	$L__BB33_4;

	shl.b64 	%rd35, %rd5, 3;
	add.s64 	%rd36, %rd3, %rd35;
	ld.global.u64 	%rd37, [%rd36];
	mul.lo.s64 	%rd63, %rd37, %rd63;
	add.s32 	%r34, %r34, 1;
	cvt.u64.u32 	%rd38, %r34;
	setp.lt.u64 	%p7, %rd38, %rd24;
	mov.pred 	%p16, %p2;
	@%p7 bra 	$L__BB33_2;

$L__BB33_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r35, %r21, %r3, %r22;
	cvt.u64.u32 	%rd64, %r35;
	@%p16 bra 	$L__BB33_8;
	bra.uni 	$L__BB33_5;

$L__BB33_8:
	setp.ge.u64 	%p10, %rd64, %rd23;
	@%p10 bra 	$L__BB33_17;

	mov.u32 	%r24, %nctaid.x;
	mul.lo.s32 	%r8, %r3, %r24;
	@%p3 bra 	$L__BB33_16;

$L__BB33_10:
	mov.u32 	%r37, 0;
	mov.u32 	%r38, %r35;
	mov.u32 	%r39, %r37;

$L__BB33_11:
	not.b32 	%r27, %r37;
	cvt.u64.u32 	%rd44, %r27;
	add.s64 	%rd45, %rd44, %rd24;
	cvt.u64.u32 	%rd11, %r38;
	shl.b64 	%rd46, %rd45, 3;
	and.b64  	%rd47, %rd46, 34359738360;
	add.s64 	%rd12, %rd3, %rd47;
	ld.global.u64 	%rd13, [%rd12];
	and.b64  	%rd48, %rd13, -4294967296;
	setp.eq.s64 	%p12, %rd48, 0;
	@%p12 bra 	$L__BB33_13;

	div.u64 	%rd66, %rd11, %rd13;
	mul.lo.s64 	%rd49, %rd66, %rd13;
	sub.s64 	%rd67, %rd11, %rd49;
	bra.uni 	$L__BB33_14;

$L__BB33_13:
	cvt.u32.u64 	%r28, %rd13;
	cvt.u32.u64 	%r29, %rd11;
	div.u32 	%r30, %r29, %r28;
	mul.lo.s32 	%r31, %r30, %r28;
	sub.s32 	%r32, %r29, %r31;
	cvt.u64.u32 	%rd66, %r30;
	cvt.u64.u32 	%rd67, %r32;

$L__BB33_14:
	shl.b64 	%rd50, %rd24, 3;
	add.s64 	%rd51, %rd12, %rd50;
	ld.global.u64 	%rd52, [%rd51];
	mul.lo.s64 	%rd53, %rd52, %rd67;
	cvt.u32.u64 	%r33, %rd53;
	add.s32 	%r39, %r39, %r33;
	cvt.u32.u64 	%r38, %rd66;
	add.s32 	%r37, %r37, 1;
	cvt.u64.u32 	%rd54, %r37;
	setp.lt.u64 	%p13, %rd54, %rd24;
	@%p13 bra 	$L__BB33_11;

	mul.wide.u32 	%rd55, %r39, 8;
	add.s64 	%rd56, %rd2, %rd55;
	ld.global.u64 	%rd57, [%rd56];
	cvt.rn.f32.s64 	%f2, %rd57;
	shl.b64 	%rd58, %rd64, 2;
	add.s64 	%rd59, %rd1, %rd58;
	st.global.f32 	[%rd59], %f2;
	add.s32 	%r35, %r35, %r8;
	cvt.u64.u32 	%rd64, %r35;
	setp.lt.u64 	%p14, %rd64, %rd23;
	@%p14 bra 	$L__BB33_10;
	bra.uni 	$L__BB33_17;

$L__BB33_16:
	ld.global.u64 	%rd60, [%rd2];
	cvt.rn.f32.s64 	%f3, %rd60;
	shl.b64 	%rd61, %rd64, 2;
	add.s64 	%rd62, %rd1, %rd61;
	st.global.f32 	[%rd62], %f3;
	add.s32 	%r35, %r35, %r8;
	cvt.u64.u32 	%rd64, %r35;
	setp.lt.u64 	%p15, %rd64, %rd23;
	@%p15 bra 	$L__BB33_16;
	bra.uni 	$L__BB33_17;

$L__BB33_5:
	setp.ge.u64 	%p8, %rd64, %rd23;
	@%p8 bra 	$L__BB33_17;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;

$L__BB33_7:
	shl.b64 	%rd39, %rd64, 3;
	add.s64 	%rd40, %rd2, %rd39;
	ld.global.u64 	%rd41, [%rd40];
	cvt.rn.f32.s64 	%f1, %rd41;
	shl.b64 	%rd42, %rd64, 2;
	add.s64 	%rd43, %rd1, %rd42;
	st.global.f32 	[%rd43], %f1;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd64, %r35;
	setp.lt.u64 	%p9, %rd64, %rd23;
	@%p9 bra 	$L__BB33_7;

$L__BB33_17:
	ret;

}
	// .globl	cast_i64_f64
.visible .entry cast_i64_f64(
	.param .u64 cast_i64_f64_param_0,
	.param .u64 cast_i64_f64_param_1,
	.param .u64 cast_i64_f64_param_2,
	.param .u64 cast_i64_f64_param_3,
	.param .u64 cast_i64_f64_param_4
)
{
	.reg .pred 	%p<17>;
	.reg .b32 	%r<41>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<68>;


	ld.param.u64 	%rd23, [cast_i64_f64_param_0];
	ld.param.u64 	%rd24, [cast_i64_f64_param_1];
	ld.param.u64 	%rd25, [cast_i64_f64_param_2];
	ld.param.u64 	%rd26, [cast_i64_f64_param_3];
	ld.param.u64 	%rd27, [cast_i64_f64_param_4];
	cvta.to.global.u64 	%rd1, %rd27;
	cvta.to.global.u64 	%rd2, %rd26;
	cvta.to.global.u64 	%rd3, %rd25;
	setp.eq.s64 	%p3, %rd24, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p16, %p2;
	@%p3 bra 	$L__BB34_4;

	mov.u64 	%rd62, 1;
	mov.u32 	%r34, 0;

$L__BB34_2:
	not.b32 	%r20, %r34;
	cvt.u64.u32 	%rd29, %r20;
	add.s64 	%rd30, %rd29, %rd24;
	and.b64  	%rd5, %rd30, 4294967295;
	add.s64 	%rd31, %rd5, %rd24;
	shl.b64 	%rd32, %rd31, 3;
	add.s64 	%rd33, %rd3, %rd32;
	ld.global.u64 	%rd34, [%rd33];
	setp.ne.s64 	%p5, %rd62, %rd34;
	mov.pred 	%p16, -1;
	@%p5 bra 	$L__BB34_4;

	shl.b64 	%rd35, %rd5, 3;
	add.s64 	%rd36, %rd3, %rd35;
	ld.global.u64 	%rd37, [%rd36];
	mul.lo.s64 	%rd62, %rd37, %rd62;
	add.s32 	%r34, %r34, 1;
	cvt.u64.u32 	%rd38, %r34;
	setp.lt.u64 	%p7, %rd38, %rd24;
	mov.pred 	%p16, %p2;
	@%p7 bra 	$L__BB34_2;

$L__BB34_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r35, %r21, %r3, %r22;
	cvt.u64.u32 	%rd63, %r35;
	@%p16 bra 	$L__BB34_8;
	bra.uni 	$L__BB34_5;

$L__BB34_8:
	setp.ge.u64 	%p10, %rd63, %rd23;
	@%p10 bra 	$L__BB34_17;

	mov.u32 	%r24, %nctaid.x;
	mul.lo.s32 	%r8, %r3, %r24;
	@%p3 bra 	$L__BB34_16;

$L__BB34_10:
	mov.u32 	%r37, 0;
	mov.u32 	%r38, %r35;
	mov.u32 	%r39, %r37;

$L__BB34_11:
	not.b32 	%r27, %r37;
	cvt.u64.u32 	%rd43, %r27;
	add.s64 	%rd44, %rd43, %rd24;
	cvt.u64.u32 	%rd11, %r38;
	shl.b64 	%rd45, %rd44, 3;
	and.b64  	%rd46, %rd45, 34359738360;
	add.s64 	%rd12, %rd3, %rd46;
	ld.global.u64 	%rd13, [%rd12];
	and.b64  	%rd47, %rd13, -4294967296;
	setp.eq.s64 	%p12, %rd47, 0;
	@%p12 bra 	$L__BB34_13;

	div.u64 	%rd65, %rd11, %rd13;
	mul.lo.s64 	%rd48, %rd65, %rd13;
	sub.s64 	%rd66, %rd11, %rd48;
	bra.uni 	$L__BB34_14;

$L__BB34_13:
	cvt.u32.u64 	%r28, %rd13;
	cvt.u32.u64 	%r29, %rd11;
	div.u32 	%r30, %r29, %r28;
	mul.lo.s32 	%r31, %r30, %r28;
	sub.s32 	%r32, %r29, %r31;
	cvt.u64.u32 	%rd65, %r30;
	cvt.u64.u32 	%rd66, %r32;

$L__BB34_14:
	shl.b64 	%rd49, %rd24, 3;
	add.s64 	%rd50, %rd12, %rd49;
	ld.global.u64 	%rd51, [%rd50];
	mul.lo.s64 	%rd52, %rd51, %rd66;
	cvt.u32.u64 	%r33, %rd52;
	add.s32 	%r39, %r39, %r33;
	cvt.u32.u64 	%r38, %rd65;
	add.s32 	%r37, %r37, 1;
	cvt.u64.u32 	%rd53, %r37;
	setp.lt.u64 	%p13, %rd53, %rd24;
	@%p13 bra 	$L__BB34_11;

	mul.wide.u32 	%rd54, %r39, 8;
	add.s64 	%rd55, %rd2, %rd54;
	ld.global.u64 	%rd56, [%rd55];
	cvt.rn.f64.s64 	%fd2, %rd56;
	shl.b64 	%rd57, %rd63, 3;
	add.s64 	%rd58, %rd1, %rd57;
	st.global.f64 	[%rd58], %fd2;
	add.s32 	%r35, %r35, %r8;
	cvt.u64.u32 	%rd63, %r35;
	setp.lt.u64 	%p14, %rd63, %rd23;
	@%p14 bra 	$L__BB34_10;
	bra.uni 	$L__BB34_17;

$L__BB34_16:
	ld.global.u64 	%rd59, [%rd2];
	cvt.rn.f64.s64 	%fd3, %rd59;
	shl.b64 	%rd60, %rd63, 3;
	add.s64 	%rd61, %rd1, %rd60;
	st.global.f64 	[%rd61], %fd3;
	add.s32 	%r35, %r35, %r8;
	cvt.u64.u32 	%rd63, %r35;
	setp.lt.u64 	%p15, %rd63, %rd23;
	@%p15 bra 	$L__BB34_16;
	bra.uni 	$L__BB34_17;

$L__BB34_5:
	setp.ge.u64 	%p8, %rd63, %rd23;
	@%p8 bra 	$L__BB34_17;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;

$L__BB34_7:
	shl.b64 	%rd39, %rd63, 3;
	add.s64 	%rd40, %rd2, %rd39;
	ld.global.u64 	%rd41, [%rd40];
	cvt.rn.f64.s64 	%fd1, %rd41;
	add.s64 	%rd42, %rd1, %rd39;
	st.global.f64 	[%rd42], %fd1;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd63, %r35;
	setp.lt.u64 	%p9, %rd63, %rd23;
	@%p9 bra 	$L__BB34_7;

$L__BB34_17:
	ret;

}
	// .globl	cast_f32_u8
.visible .entry cast_f32_u8(
	.param .u64 cast_f32_u8_param_0,
	.param .u64 cast_f32_u8_param_1,
	.param .u64 cast_f32_u8_param_2,
	.param .u64 cast_f32_u8_param_3,
	.param .u64 cast_f32_u8_param_4
)
{
	.reg .pred 	%p<17>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<44>;
	.reg .b64 	%rd<63>;


	ld.param.u64 	%rd23, [cast_f32_u8_param_0];
	ld.param.u64 	%rd24, [cast_f32_u8_param_1];
	ld.param.u64 	%rd25, [cast_f32_u8_param_2];
	ld.param.u64 	%rd26, [cast_f32_u8_param_3];
	ld.param.u64 	%rd27, [cast_f32_u8_param_4];
	cvta.to.global.u64 	%rd1, %rd27;
	cvta.to.global.u64 	%rd2, %rd26;
	cvta.to.global.u64 	%rd3, %rd25;
	setp.eq.s64 	%p3, %rd24, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p16, %p2;
	@%p3 bra 	$L__BB35_4;

	mov.u64 	%rd57, 1;
	mov.u32 	%r37, 0;

$L__BB35_2:
	not.b32 	%r20, %r37;
	cvt.u64.u32 	%rd29, %r20;
	add.s64 	%rd30, %rd29, %rd24;
	and.b64  	%rd5, %rd30, 4294967295;
	add.s64 	%rd31, %rd5, %rd24;
	shl.b64 	%rd32, %rd31, 3;
	add.s64 	%rd33, %rd3, %rd32;
	ld.global.u64 	%rd34, [%rd33];
	setp.ne.s64 	%p5, %rd57, %rd34;
	mov.pred 	%p16, -1;
	@%p5 bra 	$L__BB35_4;

	shl.b64 	%rd35, %rd5, 3;
	add.s64 	%rd36, %rd3, %rd35;
	ld.global.u64 	%rd37, [%rd36];
	mul.lo.s64 	%rd57, %rd37, %rd57;
	add.s32 	%r37, %r37, 1;
	cvt.u64.u32 	%rd38, %r37;
	setp.lt.u64 	%p7, %rd38, %rd24;
	mov.pred 	%p16, %p2;
	@%p7 bra 	$L__BB35_2;

$L__BB35_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r38, %r21, %r3, %r22;
	cvt.u64.u32 	%rd58, %r38;
	@%p16 bra 	$L__BB35_8;
	bra.uni 	$L__BB35_5;

$L__BB35_8:
	setp.ge.u64 	%p10, %rd58, %rd23;
	@%p10 bra 	$L__BB35_17;

	mov.u32 	%r25, %nctaid.x;
	mul.lo.s32 	%r8, %r3, %r25;
	@%p3 bra 	$L__BB35_16;

$L__BB35_10:
	mov.u32 	%r40, 0;
	mov.u32 	%r41, %r38;
	mov.u32 	%r42, %r40;

$L__BB35_11:
	not.b32 	%r28, %r40;
	cvt.u64.u32 	%rd42, %r28;
	add.s64 	%rd43, %rd42, %rd24;
	cvt.u64.u32 	%rd11, %r41;
	shl.b64 	%rd44, %rd43, 3;
	and.b64  	%rd45, %rd44, 34359738360;
	add.s64 	%rd12, %rd3, %rd45;
	ld.global.u64 	%rd13, [%rd12];
	and.b64  	%rd46, %rd13, -4294967296;
	setp.eq.s64 	%p12, %rd46, 0;
	@%p12 bra 	$L__BB35_13;

	div.u64 	%rd60, %rd11, %rd13;
	mul.lo.s64 	%rd47, %rd60, %rd13;
	sub.s64 	%rd61, %rd11, %rd47;
	bra.uni 	$L__BB35_14;

$L__BB35_13:
	cvt.u32.u64 	%r29, %rd13;
	cvt.u32.u64 	%r30, %rd11;
	div.u32 	%r31, %r30, %r29;
	mul.lo.s32 	%r32, %r31, %r29;
	sub.s32 	%r33, %r30, %r32;
	cvt.u64.u32 	%rd60, %r31;
	cvt.u64.u32 	%rd61, %r33;

$L__BB35_14:
	shl.b64 	%rd48, %rd24, 3;
	add.s64 	%rd49, %rd12, %rd48;
	ld.global.u64 	%rd50, [%rd49];
	mul.lo.s64 	%rd51, %rd50, %rd61;
	cvt.u32.u64 	%r34, %rd51;
	add.s32 	%r42, %r42, %r34;
	cvt.u32.u64 	%r41, %rd60;
	add.s32 	%r40, %r40, 1;
	cvt.u64.u32 	%rd52, %r40;
	setp.lt.u64 	%p13, %rd52, %rd24;
	@%p13 bra 	$L__BB35_11;

	mul.wide.u32 	%rd53, %r42, 4;
	add.s64 	%rd54, %rd2, %rd53;
	ld.global.f32 	%f2, [%rd54];
	cvt.rzi.u32.f32 	%r35, %f2;
	add.s64 	%rd55, %rd1, %rd58;
	st.global.u8 	[%rd55], %r35;
	add.s32 	%r38, %r38, %r8;
	cvt.u64.u32 	%rd58, %r38;
	setp.lt.u64 	%p14, %rd58, %rd23;
	@%p14 bra 	$L__BB35_10;
	bra.uni 	$L__BB35_17;

$L__BB35_16:
	ld.global.f32 	%f3, [%rd2];
	cvt.rzi.u32.f32 	%r36, %f3;
	add.s64 	%rd56, %rd1, %rd58;
	st.global.u8 	[%rd56], %r36;
	add.s32 	%r38, %r38, %r8;
	cvt.u64.u32 	%rd58, %r38;
	setp.lt.u64 	%p15, %rd58, %rd23;
	@%p15 bra 	$L__BB35_16;
	bra.uni 	$L__BB35_17;

$L__BB35_5:
	setp.ge.u64 	%p8, %rd58, %rd23;
	@%p8 bra 	$L__BB35_17;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;

$L__BB35_7:
	shl.b64 	%rd39, %rd58, 2;
	add.s64 	%rd40, %rd2, %rd39;
	ld.global.f32 	%f1, [%rd40];
	cvt.rzi.u32.f32 	%r24, %f1;
	add.s64 	%rd41, %rd1, %rd58;
	st.global.u8 	[%rd41], %r24;
	add.s32 	%r38, %r38, %r5;
	cvt.u64.u32 	%rd58, %r38;
	setp.lt.u64 	%p9, %rd58, %rd23;
	@%p9 bra 	$L__BB35_7;

$L__BB35_17:
	ret;

}
	// .globl	cast_f32_u32
.visible .entry cast_f32_u32(
	.param .u64 cast_f32_u32_param_0,
	.param .u64 cast_f32_u32_param_1,
	.param .u64 cast_f32_u32_param_2,
	.param .u64 cast_f32_u32_param_3,
	.param .u64 cast_f32_u32_param_4
)
{
	.reg .pred 	%p<17>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<44>;
	.reg .b64 	%rd<65>;


	ld.param.u64 	%rd23, [cast_f32_u32_param_0];
	ld.param.u64 	%rd24, [cast_f32_u32_param_1];
	ld.param.u64 	%rd25, [cast_f32_u32_param_2];
	ld.param.u64 	%rd26, [cast_f32_u32_param_3];
	ld.param.u64 	%rd27, [cast_f32_u32_param_4];
	cvta.to.global.u64 	%rd1, %rd27;
	cvta.to.global.u64 	%rd2, %rd26;
	cvta.to.global.u64 	%rd3, %rd25;
	setp.eq.s64 	%p3, %rd24, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p16, %p2;
	@%p3 bra 	$L__BB36_4;

	mov.u64 	%rd59, 1;
	mov.u32 	%r37, 0;

$L__BB36_2:
	not.b32 	%r20, %r37;
	cvt.u64.u32 	%rd29, %r20;
	add.s64 	%rd30, %rd29, %rd24;
	and.b64  	%rd5, %rd30, 4294967295;
	add.s64 	%rd31, %rd5, %rd24;
	shl.b64 	%rd32, %rd31, 3;
	add.s64 	%rd33, %rd3, %rd32;
	ld.global.u64 	%rd34, [%rd33];
	setp.ne.s64 	%p5, %rd59, %rd34;
	mov.pred 	%p16, -1;
	@%p5 bra 	$L__BB36_4;

	shl.b64 	%rd35, %rd5, 3;
	add.s64 	%rd36, %rd3, %rd35;
	ld.global.u64 	%rd37, [%rd36];
	mul.lo.s64 	%rd59, %rd37, %rd59;
	add.s32 	%r37, %r37, 1;
	cvt.u64.u32 	%rd38, %r37;
	setp.lt.u64 	%p7, %rd38, %rd24;
	mov.pred 	%p16, %p2;
	@%p7 bra 	$L__BB36_2;

$L__BB36_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r38, %r21, %r3, %r22;
	cvt.u64.u32 	%rd60, %r38;
	@%p16 bra 	$L__BB36_8;
	bra.uni 	$L__BB36_5;

$L__BB36_8:
	setp.ge.u64 	%p10, %rd60, %rd23;
	@%p10 bra 	$L__BB36_17;

	mov.u32 	%r25, %nctaid.x;
	mul.lo.s32 	%r8, %r3, %r25;
	@%p3 bra 	$L__BB36_16;

$L__BB36_10:
	mov.u32 	%r40, 0;
	mov.u32 	%r41, %r38;
	mov.u32 	%r42, %r40;

$L__BB36_11:
	not.b32 	%r28, %r40;
	cvt.u64.u32 	%rd42, %r28;
	add.s64 	%rd43, %rd42, %rd24;
	cvt.u64.u32 	%rd11, %r41;
	shl.b64 	%rd44, %rd43, 3;
	and.b64  	%rd45, %rd44, 34359738360;
	add.s64 	%rd12, %rd3, %rd45;
	ld.global.u64 	%rd13, [%rd12];
	and.b64  	%rd46, %rd13, -4294967296;
	setp.eq.s64 	%p12, %rd46, 0;
	@%p12 bra 	$L__BB36_13;

	div.u64 	%rd62, %rd11, %rd13;
	mul.lo.s64 	%rd47, %rd62, %rd13;
	sub.s64 	%rd63, %rd11, %rd47;
	bra.uni 	$L__BB36_14;

$L__BB36_13:
	cvt.u32.u64 	%r29, %rd13;
	cvt.u32.u64 	%r30, %rd11;
	div.u32 	%r31, %r30, %r29;
	mul.lo.s32 	%r32, %r31, %r29;
	sub.s32 	%r33, %r30, %r32;
	cvt.u64.u32 	%rd62, %r31;
	cvt.u64.u32 	%rd63, %r33;

$L__BB36_14:
	shl.b64 	%rd48, %rd24, 3;
	add.s64 	%rd49, %rd12, %rd48;
	ld.global.u64 	%rd50, [%rd49];
	mul.lo.s64 	%rd51, %rd50, %rd63;
	cvt.u32.u64 	%r34, %rd51;
	add.s32 	%r42, %r42, %r34;
	cvt.u32.u64 	%r41, %rd62;
	add.s32 	%r40, %r40, 1;
	cvt.u64.u32 	%rd52, %r40;
	setp.lt.u64 	%p13, %rd52, %rd24;
	@%p13 bra 	$L__BB36_11;

	mul.wide.u32 	%rd53, %r42, 4;
	add.s64 	%rd54, %rd2, %rd53;
	ld.global.f32 	%f2, [%rd54];
	cvt.rzi.u32.f32 	%r35, %f2;
	shl.b64 	%rd55, %rd60, 2;
	add.s64 	%rd56, %rd1, %rd55;
	st.global.u32 	[%rd56], %r35;
	add.s32 	%r38, %r38, %r8;
	cvt.u64.u32 	%rd60, %r38;
	setp.lt.u64 	%p14, %rd60, %rd23;
	@%p14 bra 	$L__BB36_10;
	bra.uni 	$L__BB36_17;

$L__BB36_16:
	ld.global.f32 	%f3, [%rd2];
	cvt.rzi.u32.f32 	%r36, %f3;
	shl.b64 	%rd57, %rd60, 2;
	add.s64 	%rd58, %rd1, %rd57;
	st.global.u32 	[%rd58], %r36;
	add.s32 	%r38, %r38, %r8;
	cvt.u64.u32 	%rd60, %r38;
	setp.lt.u64 	%p15, %rd60, %rd23;
	@%p15 bra 	$L__BB36_16;
	bra.uni 	$L__BB36_17;

$L__BB36_5:
	setp.ge.u64 	%p8, %rd60, %rd23;
	@%p8 bra 	$L__BB36_17;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;

$L__BB36_7:
	shl.b64 	%rd39, %rd60, 2;
	add.s64 	%rd40, %rd2, %rd39;
	ld.global.f32 	%f1, [%rd40];
	cvt.rzi.u32.f32 	%r24, %f1;
	add.s64 	%rd41, %rd1, %rd39;
	st.global.u32 	[%rd41], %r24;
	add.s32 	%r38, %r38, %r5;
	cvt.u64.u32 	%rd60, %r38;
	setp.lt.u64 	%p9, %rd60, %rd23;
	@%p9 bra 	$L__BB36_7;

$L__BB36_17:
	ret;

}
	// .globl	cast_f32_i64
.visible .entry cast_f32_i64(
	.param .u64 cast_f32_i64_param_0,
	.param .u64 cast_f32_i64_param_1,
	.param .u64 cast_f32_i64_param_2,
	.param .u64 cast_f32_i64_param_3,
	.param .u64 cast_f32_i64_param_4
)
{
	.reg .pred 	%p<17>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<41>;
	.reg .b64 	%rd<69>;


	ld.param.u64 	%rd23, [cast_f32_i64_param_0];
	ld.param.u64 	%rd24, [cast_f32_i64_param_1];
	ld.param.u64 	%rd25, [cast_f32_i64_param_2];
	ld.param.u64 	%rd26, [cast_f32_i64_param_3];
	ld.param.u64 	%rd27, [cast_f32_i64_param_4];
	cvta.to.global.u64 	%rd1, %rd27;
	cvta.to.global.u64 	%rd2, %rd26;
	cvta.to.global.u64 	%rd3, %rd25;
	setp.eq.s64 	%p3, %rd24, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p16, %p2;
	@%p3 bra 	$L__BB37_4;

	mov.u64 	%rd63, 1;
	mov.u32 	%r34, 0;

$L__BB37_2:
	not.b32 	%r20, %r34;
	cvt.u64.u32 	%rd29, %r20;
	add.s64 	%rd30, %rd29, %rd24;
	and.b64  	%rd5, %rd30, 4294967295;
	add.s64 	%rd31, %rd5, %rd24;
	shl.b64 	%rd32, %rd31, 3;
	add.s64 	%rd33, %rd3, %rd32;
	ld.global.u64 	%rd34, [%rd33];
	setp.ne.s64 	%p5, %rd63, %rd34;
	mov.pred 	%p16, -1;
	@%p5 bra 	$L__BB37_4;

	shl.b64 	%rd35, %rd5, 3;
	add.s64 	%rd36, %rd3, %rd35;
	ld.global.u64 	%rd37, [%rd36];
	mul.lo.s64 	%rd63, %rd37, %rd63;
	add.s32 	%r34, %r34, 1;
	cvt.u64.u32 	%rd38, %r34;
	setp.lt.u64 	%p7, %rd38, %rd24;
	mov.pred 	%p16, %p2;
	@%p7 bra 	$L__BB37_2;

$L__BB37_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r35, %r21, %r3, %r22;
	cvt.u64.u32 	%rd64, %r35;
	@%p16 bra 	$L__BB37_8;
	bra.uni 	$L__BB37_5;

$L__BB37_8:
	setp.ge.u64 	%p10, %rd64, %rd23;
	@%p10 bra 	$L__BB37_17;

	mov.u32 	%r24, %nctaid.x;
	mul.lo.s32 	%r8, %r3, %r24;
	@%p3 bra 	$L__BB37_16;

$L__BB37_10:
	mov.u32 	%r37, 0;
	mov.u32 	%r38, %r35;
	mov.u32 	%r39, %r37;

$L__BB37_11:
	not.b32 	%r27, %r37;
	cvt.u64.u32 	%rd44, %r27;
	add.s64 	%rd45, %rd44, %rd24;
	cvt.u64.u32 	%rd11, %r38;
	shl.b64 	%rd46, %rd45, 3;
	and.b64  	%rd47, %rd46, 34359738360;
	add.s64 	%rd12, %rd3, %rd47;
	ld.global.u64 	%rd13, [%rd12];
	and.b64  	%rd48, %rd13, -4294967296;
	setp.eq.s64 	%p12, %rd48, 0;
	@%p12 bra 	$L__BB37_13;

	div.u64 	%rd66, %rd11, %rd13;
	mul.lo.s64 	%rd49, %rd66, %rd13;
	sub.s64 	%rd67, %rd11, %rd49;
	bra.uni 	$L__BB37_14;

$L__BB37_13:
	cvt.u32.u64 	%r28, %rd13;
	cvt.u32.u64 	%r29, %rd11;
	div.u32 	%r30, %r29, %r28;
	mul.lo.s32 	%r31, %r30, %r28;
	sub.s32 	%r32, %r29, %r31;
	cvt.u64.u32 	%rd66, %r30;
	cvt.u64.u32 	%rd67, %r32;

$L__BB37_14:
	shl.b64 	%rd50, %rd24, 3;
	add.s64 	%rd51, %rd12, %rd50;
	ld.global.u64 	%rd52, [%rd51];
	mul.lo.s64 	%rd53, %rd52, %rd67;
	cvt.u32.u64 	%r33, %rd53;
	add.s32 	%r39, %r39, %r33;
	cvt.u32.u64 	%r38, %rd66;
	add.s32 	%r37, %r37, 1;
	cvt.u64.u32 	%rd54, %r37;
	setp.lt.u64 	%p13, %rd54, %rd24;
	@%p13 bra 	$L__BB37_11;

	mul.wide.u32 	%rd55, %r39, 4;
	add.s64 	%rd56, %rd2, %rd55;
	ld.global.f32 	%f2, [%rd56];
	cvt.rzi.s64.f32 	%rd57, %f2;
	shl.b64 	%rd58, %rd64, 3;
	add.s64 	%rd59, %rd1, %rd58;
	st.global.u64 	[%rd59], %rd57;
	add.s32 	%r35, %r35, %r8;
	cvt.u64.u32 	%rd64, %r35;
	setp.lt.u64 	%p14, %rd64, %rd23;
	@%p14 bra 	$L__BB37_10;
	bra.uni 	$L__BB37_17;

$L__BB37_16:
	ld.global.f32 	%f3, [%rd2];
	cvt.rzi.s64.f32 	%rd60, %f3;
	shl.b64 	%rd61, %rd64, 3;
	add.s64 	%rd62, %rd1, %rd61;
	st.global.u64 	[%rd62], %rd60;
	add.s32 	%r35, %r35, %r8;
	cvt.u64.u32 	%rd64, %r35;
	setp.lt.u64 	%p15, %rd64, %rd23;
	@%p15 bra 	$L__BB37_16;
	bra.uni 	$L__BB37_17;

$L__BB37_5:
	setp.ge.u64 	%p8, %rd64, %rd23;
	@%p8 bra 	$L__BB37_17;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;

$L__BB37_7:
	shl.b64 	%rd39, %rd64, 2;
	add.s64 	%rd40, %rd2, %rd39;
	ld.global.f32 	%f1, [%rd40];
	cvt.rzi.s64.f32 	%rd41, %f1;
	shl.b64 	%rd42, %rd64, 3;
	add.s64 	%rd43, %rd1, %rd42;
	st.global.u64 	[%rd43], %rd41;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd64, %r35;
	setp.lt.u64 	%p9, %rd64, %rd23;
	@%p9 bra 	$L__BB37_7;

$L__BB37_17:
	ret;

}
	// .globl	cast_f32_f32
.visible .entry cast_f32_f32(
	.param .u64 cast_f32_f32_param_0,
	.param .u64 cast_f32_f32_param_1,
	.param .u64 cast_f32_f32_param_2,
	.param .u64 cast_f32_f32_param_3,
	.param .u64 cast_f32_f32_param_4
)
{
	.reg .pred 	%p<17>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<41>;
	.reg .b64 	%rd<65>;


	ld.param.u64 	%rd23, [cast_f32_f32_param_0];
	ld.param.u64 	%rd24, [cast_f32_f32_param_1];
	ld.param.u64 	%rd25, [cast_f32_f32_param_2];
	ld.param.u64 	%rd26, [cast_f32_f32_param_3];
	ld.param.u64 	%rd27, [cast_f32_f32_param_4];
	cvta.to.global.u64 	%rd1, %rd27;
	cvta.to.global.u64 	%rd2, %rd26;
	cvta.to.global.u64 	%rd3, %rd25;
	setp.eq.s64 	%p3, %rd24, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p16, %p2;
	@%p3 bra 	$L__BB38_4;

	mov.u64 	%rd59, 1;
	mov.u32 	%r34, 0;

$L__BB38_2:
	not.b32 	%r20, %r34;
	cvt.u64.u32 	%rd29, %r20;
	add.s64 	%rd30, %rd29, %rd24;
	and.b64  	%rd5, %rd30, 4294967295;
	add.s64 	%rd31, %rd5, %rd24;
	shl.b64 	%rd32, %rd31, 3;
	add.s64 	%rd33, %rd3, %rd32;
	ld.global.u64 	%rd34, [%rd33];
	setp.ne.s64 	%p5, %rd59, %rd34;
	mov.pred 	%p16, -1;
	@%p5 bra 	$L__BB38_4;

	shl.b64 	%rd35, %rd5, 3;
	add.s64 	%rd36, %rd3, %rd35;
	ld.global.u64 	%rd37, [%rd36];
	mul.lo.s64 	%rd59, %rd37, %rd59;
	add.s32 	%r34, %r34, 1;
	cvt.u64.u32 	%rd38, %r34;
	setp.lt.u64 	%p7, %rd38, %rd24;
	mov.pred 	%p16, %p2;
	@%p7 bra 	$L__BB38_2;

$L__BB38_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r35, %r21, %r3, %r22;
	cvt.u64.u32 	%rd60, %r35;
	@%p16 bra 	$L__BB38_8;
	bra.uni 	$L__BB38_5;

$L__BB38_8:
	setp.ge.u64 	%p10, %rd60, %rd23;
	@%p10 bra 	$L__BB38_17;

	mov.u32 	%r24, %nctaid.x;
	mul.lo.s32 	%r8, %r3, %r24;
	@%p3 bra 	$L__BB38_16;

$L__BB38_10:
	mov.u32 	%r37, 0;
	mov.u32 	%r38, %r35;
	mov.u32 	%r39, %r37;

$L__BB38_11:
	not.b32 	%r27, %r37;
	cvt.u64.u32 	%rd42, %r27;
	add.s64 	%rd43, %rd42, %rd24;
	cvt.u64.u32 	%rd11, %r38;
	shl.b64 	%rd44, %rd43, 3;
	and.b64  	%rd45, %rd44, 34359738360;
	add.s64 	%rd12, %rd3, %rd45;
	ld.global.u64 	%rd13, [%rd12];
	and.b64  	%rd46, %rd13, -4294967296;
	setp.eq.s64 	%p12, %rd46, 0;
	@%p12 bra 	$L__BB38_13;

	div.u64 	%rd62, %rd11, %rd13;
	mul.lo.s64 	%rd47, %rd62, %rd13;
	sub.s64 	%rd63, %rd11, %rd47;
	bra.uni 	$L__BB38_14;

$L__BB38_13:
	cvt.u32.u64 	%r28, %rd13;
	cvt.u32.u64 	%r29, %rd11;
	div.u32 	%r30, %r29, %r28;
	mul.lo.s32 	%r31, %r30, %r28;
	sub.s32 	%r32, %r29, %r31;
	cvt.u64.u32 	%rd62, %r30;
	cvt.u64.u32 	%rd63, %r32;

$L__BB38_14:
	shl.b64 	%rd48, %rd24, 3;
	add.s64 	%rd49, %rd12, %rd48;
	ld.global.u64 	%rd50, [%rd49];
	mul.lo.s64 	%rd51, %rd50, %rd63;
	cvt.u32.u64 	%r33, %rd51;
	add.s32 	%r39, %r39, %r33;
	cvt.u32.u64 	%r38, %rd62;
	add.s32 	%r37, %r37, 1;
	cvt.u64.u32 	%rd52, %r37;
	setp.lt.u64 	%p13, %rd52, %rd24;
	@%p13 bra 	$L__BB38_11;

	mul.wide.u32 	%rd53, %r39, 4;
	add.s64 	%rd54, %rd2, %rd53;
	ld.global.f32 	%f2, [%rd54];
	shl.b64 	%rd55, %rd60, 2;
	add.s64 	%rd56, %rd1, %rd55;
	st.global.f32 	[%rd56], %f2;
	add.s32 	%r35, %r35, %r8;
	cvt.u64.u32 	%rd60, %r35;
	setp.lt.u64 	%p14, %rd60, %rd23;
	@%p14 bra 	$L__BB38_10;
	bra.uni 	$L__BB38_17;

$L__BB38_16:
	ld.global.f32 	%f3, [%rd2];
	shl.b64 	%rd57, %rd60, 2;
	add.s64 	%rd58, %rd1, %rd57;
	st.global.f32 	[%rd58], %f3;
	add.s32 	%r35, %r35, %r8;
	cvt.u64.u32 	%rd60, %r35;
	setp.lt.u64 	%p15, %rd60, %rd23;
	@%p15 bra 	$L__BB38_16;
	bra.uni 	$L__BB38_17;

$L__BB38_5:
	setp.ge.u64 	%p8, %rd60, %rd23;
	@%p8 bra 	$L__BB38_17;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;

$L__BB38_7:
	shl.b64 	%rd39, %rd60, 2;
	add.s64 	%rd40, %rd2, %rd39;
	ld.global.f32 	%f1, [%rd40];
	add.s64 	%rd41, %rd1, %rd39;
	st.global.f32 	[%rd41], %f1;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd60, %r35;
	setp.lt.u64 	%p9, %rd60, %rd23;
	@%p9 bra 	$L__BB38_7;

$L__BB38_17:
	ret;

}
	// .globl	cast_f32_f64
.visible .entry cast_f32_f64(
	.param .u64 cast_f32_f64_param_0,
	.param .u64 cast_f32_f64_param_1,
	.param .u64 cast_f32_f64_param_2,
	.param .u64 cast_f32_f64_param_3,
	.param .u64 cast_f32_f64_param_4
)
{
	.reg .pred 	%p<17>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<41>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<66>;


	ld.param.u64 	%rd23, [cast_f32_f64_param_0];
	ld.param.u64 	%rd24, [cast_f32_f64_param_1];
	ld.param.u64 	%rd25, [cast_f32_f64_param_2];
	ld.param.u64 	%rd26, [cast_f32_f64_param_3];
	ld.param.u64 	%rd27, [cast_f32_f64_param_4];
	cvta.to.global.u64 	%rd1, %rd27;
	cvta.to.global.u64 	%rd2, %rd26;
	cvta.to.global.u64 	%rd3, %rd25;
	setp.eq.s64 	%p3, %rd24, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p16, %p2;
	@%p3 bra 	$L__BB39_4;

	mov.u64 	%rd60, 1;
	mov.u32 	%r34, 0;

$L__BB39_2:
	not.b32 	%r20, %r34;
	cvt.u64.u32 	%rd29, %r20;
	add.s64 	%rd30, %rd29, %rd24;
	and.b64  	%rd5, %rd30, 4294967295;
	add.s64 	%rd31, %rd5, %rd24;
	shl.b64 	%rd32, %rd31, 3;
	add.s64 	%rd33, %rd3, %rd32;
	ld.global.u64 	%rd34, [%rd33];
	setp.ne.s64 	%p5, %rd60, %rd34;
	mov.pred 	%p16, -1;
	@%p5 bra 	$L__BB39_4;

	shl.b64 	%rd35, %rd5, 3;
	add.s64 	%rd36, %rd3, %rd35;
	ld.global.u64 	%rd37, [%rd36];
	mul.lo.s64 	%rd60, %rd37, %rd60;
	add.s32 	%r34, %r34, 1;
	cvt.u64.u32 	%rd38, %r34;
	setp.lt.u64 	%p7, %rd38, %rd24;
	mov.pred 	%p16, %p2;
	@%p7 bra 	$L__BB39_2;

$L__BB39_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r35, %r21, %r3, %r22;
	cvt.u64.u32 	%rd61, %r35;
	@%p16 bra 	$L__BB39_8;
	bra.uni 	$L__BB39_5;

$L__BB39_8:
	setp.ge.u64 	%p10, %rd61, %rd23;
	@%p10 bra 	$L__BB39_17;

	mov.u32 	%r24, %nctaid.x;
	mul.lo.s32 	%r8, %r3, %r24;
	@%p3 bra 	$L__BB39_16;

$L__BB39_10:
	mov.u32 	%r37, 0;
	mov.u32 	%r38, %r35;
	mov.u32 	%r39, %r37;

$L__BB39_11:
	not.b32 	%r27, %r37;
	cvt.u64.u32 	%rd43, %r27;
	add.s64 	%rd44, %rd43, %rd24;
	cvt.u64.u32 	%rd11, %r38;
	shl.b64 	%rd45, %rd44, 3;
	and.b64  	%rd46, %rd45, 34359738360;
	add.s64 	%rd12, %rd3, %rd46;
	ld.global.u64 	%rd13, [%rd12];
	and.b64  	%rd47, %rd13, -4294967296;
	setp.eq.s64 	%p12, %rd47, 0;
	@%p12 bra 	$L__BB39_13;

	div.u64 	%rd63, %rd11, %rd13;
	mul.lo.s64 	%rd48, %rd63, %rd13;
	sub.s64 	%rd64, %rd11, %rd48;
	bra.uni 	$L__BB39_14;

$L__BB39_13:
	cvt.u32.u64 	%r28, %rd13;
	cvt.u32.u64 	%r29, %rd11;
	div.u32 	%r30, %r29, %r28;
	mul.lo.s32 	%r31, %r30, %r28;
	sub.s32 	%r32, %r29, %r31;
	cvt.u64.u32 	%rd63, %r30;
	cvt.u64.u32 	%rd64, %r32;

$L__BB39_14:
	shl.b64 	%rd49, %rd24, 3;
	add.s64 	%rd50, %rd12, %rd49;
	ld.global.u64 	%rd51, [%rd50];
	mul.lo.s64 	%rd52, %rd51, %rd64;
	cvt.u32.u64 	%r33, %rd52;
	add.s32 	%r39, %r39, %r33;
	cvt.u32.u64 	%r38, %rd63;
	add.s32 	%r37, %r37, 1;
	cvt.u64.u32 	%rd53, %r37;
	setp.lt.u64 	%p13, %rd53, %rd24;
	@%p13 bra 	$L__BB39_11;

	mul.wide.u32 	%rd54, %r39, 4;
	add.s64 	%rd55, %rd2, %rd54;
	ld.global.f32 	%f2, [%rd55];
	cvt.f64.f32 	%fd2, %f2;
	shl.b64 	%rd56, %rd61, 3;
	add.s64 	%rd57, %rd1, %rd56;
	st.global.f64 	[%rd57], %fd2;
	add.s32 	%r35, %r35, %r8;
	cvt.u64.u32 	%rd61, %r35;
	setp.lt.u64 	%p14, %rd61, %rd23;
	@%p14 bra 	$L__BB39_10;
	bra.uni 	$L__BB39_17;

$L__BB39_16:
	ld.global.f32 	%f3, [%rd2];
	cvt.f64.f32 	%fd3, %f3;
	shl.b64 	%rd58, %rd61, 3;
	add.s64 	%rd59, %rd1, %rd58;
	st.global.f64 	[%rd59], %fd3;
	add.s32 	%r35, %r35, %r8;
	cvt.u64.u32 	%rd61, %r35;
	setp.lt.u64 	%p15, %rd61, %rd23;
	@%p15 bra 	$L__BB39_16;
	bra.uni 	$L__BB39_17;

$L__BB39_5:
	setp.ge.u64 	%p8, %rd61, %rd23;
	@%p8 bra 	$L__BB39_17;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;

$L__BB39_7:
	shl.b64 	%rd39, %rd61, 2;
	add.s64 	%rd40, %rd2, %rd39;
	ld.global.f32 	%f1, [%rd40];
	cvt.f64.f32 	%fd1, %f1;
	shl.b64 	%rd41, %rd61, 3;
	add.s64 	%rd42, %rd1, %rd41;
	st.global.f64 	[%rd42], %fd1;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd61, %r35;
	setp.lt.u64 	%p9, %rd61, %rd23;
	@%p9 bra 	$L__BB39_7;

$L__BB39_17:
	ret;

}
	// .globl	cast_f64_u8
.visible .entry cast_f64_u8(
	.param .u64 cast_f64_u8_param_0,
	.param .u64 cast_f64_u8_param_1,
	.param .u64 cast_f64_u8_param_2,
	.param .u64 cast_f64_u8_param_3,
	.param .u64 cast_f64_u8_param_4
)
{
	.reg .pred 	%p<17>;
	.reg .b32 	%r<44>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<63>;


	ld.param.u64 	%rd23, [cast_f64_u8_param_0];
	ld.param.u64 	%rd24, [cast_f64_u8_param_1];
	ld.param.u64 	%rd25, [cast_f64_u8_param_2];
	ld.param.u64 	%rd26, [cast_f64_u8_param_3];
	ld.param.u64 	%rd27, [cast_f64_u8_param_4];
	cvta.to.global.u64 	%rd1, %rd27;
	cvta.to.global.u64 	%rd2, %rd26;
	cvta.to.global.u64 	%rd3, %rd25;
	setp.eq.s64 	%p3, %rd24, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p16, %p2;
	@%p3 bra 	$L__BB40_4;

	mov.u64 	%rd57, 1;
	mov.u32 	%r37, 0;

$L__BB40_2:
	not.b32 	%r20, %r37;
	cvt.u64.u32 	%rd29, %r20;
	add.s64 	%rd30, %rd29, %rd24;
	and.b64  	%rd5, %rd30, 4294967295;
	add.s64 	%rd31, %rd5, %rd24;
	shl.b64 	%rd32, %rd31, 3;
	add.s64 	%rd33, %rd3, %rd32;
	ld.global.u64 	%rd34, [%rd33];
	setp.ne.s64 	%p5, %rd57, %rd34;
	mov.pred 	%p16, -1;
	@%p5 bra 	$L__BB40_4;

	shl.b64 	%rd35, %rd5, 3;
	add.s64 	%rd36, %rd3, %rd35;
	ld.global.u64 	%rd37, [%rd36];
	mul.lo.s64 	%rd57, %rd37, %rd57;
	add.s32 	%r37, %r37, 1;
	cvt.u64.u32 	%rd38, %r37;
	setp.lt.u64 	%p7, %rd38, %rd24;
	mov.pred 	%p16, %p2;
	@%p7 bra 	$L__BB40_2;

$L__BB40_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r38, %r21, %r3, %r22;
	cvt.u64.u32 	%rd58, %r38;
	@%p16 bra 	$L__BB40_8;
	bra.uni 	$L__BB40_5;

$L__BB40_8:
	setp.ge.u64 	%p10, %rd58, %rd23;
	@%p10 bra 	$L__BB40_17;

	mov.u32 	%r25, %nctaid.x;
	mul.lo.s32 	%r8, %r3, %r25;
	@%p3 bra 	$L__BB40_16;

$L__BB40_10:
	mov.u32 	%r40, 0;
	mov.u32 	%r41, %r38;
	mov.u32 	%r42, %r40;

$L__BB40_11:
	not.b32 	%r28, %r40;
	cvt.u64.u32 	%rd42, %r28;
	add.s64 	%rd43, %rd42, %rd24;
	cvt.u64.u32 	%rd11, %r41;
	shl.b64 	%rd44, %rd43, 3;
	and.b64  	%rd45, %rd44, 34359738360;
	add.s64 	%rd12, %rd3, %rd45;
	ld.global.u64 	%rd13, [%rd12];
	and.b64  	%rd46, %rd13, -4294967296;
	setp.eq.s64 	%p12, %rd46, 0;
	@%p12 bra 	$L__BB40_13;

	div.u64 	%rd60, %rd11, %rd13;
	mul.lo.s64 	%rd47, %rd60, %rd13;
	sub.s64 	%rd61, %rd11, %rd47;
	bra.uni 	$L__BB40_14;

$L__BB40_13:
	cvt.u32.u64 	%r29, %rd13;
	cvt.u32.u64 	%r30, %rd11;
	div.u32 	%r31, %r30, %r29;
	mul.lo.s32 	%r32, %r31, %r29;
	sub.s32 	%r33, %r30, %r32;
	cvt.u64.u32 	%rd60, %r31;
	cvt.u64.u32 	%rd61, %r33;

$L__BB40_14:
	shl.b64 	%rd48, %rd24, 3;
	add.s64 	%rd49, %rd12, %rd48;
	ld.global.u64 	%rd50, [%rd49];
	mul.lo.s64 	%rd51, %rd50, %rd61;
	cvt.u32.u64 	%r34, %rd51;
	add.s32 	%r42, %r42, %r34;
	cvt.u32.u64 	%r41, %rd60;
	add.s32 	%r40, %r40, 1;
	cvt.u64.u32 	%rd52, %r40;
	setp.lt.u64 	%p13, %rd52, %rd24;
	@%p13 bra 	$L__BB40_11;

	mul.wide.u32 	%rd53, %r42, 8;
	add.s64 	%rd54, %rd2, %rd53;
	ld.global.f64 	%fd2, [%rd54];
	cvt.rzi.u32.f64 	%r35, %fd2;
	add.s64 	%rd55, %rd1, %rd58;
	st.global.u8 	[%rd55], %r35;
	add.s32 	%r38, %r38, %r8;
	cvt.u64.u32 	%rd58, %r38;
	setp.lt.u64 	%p14, %rd58, %rd23;
	@%p14 bra 	$L__BB40_10;
	bra.uni 	$L__BB40_17;

$L__BB40_16:
	ld.global.f64 	%fd3, [%rd2];
	cvt.rzi.u32.f64 	%r36, %fd3;
	add.s64 	%rd56, %rd1, %rd58;
	st.global.u8 	[%rd56], %r36;
	add.s32 	%r38, %r38, %r8;
	cvt.u64.u32 	%rd58, %r38;
	setp.lt.u64 	%p15, %rd58, %rd23;
	@%p15 bra 	$L__BB40_16;
	bra.uni 	$L__BB40_17;

$L__BB40_5:
	setp.ge.u64 	%p8, %rd58, %rd23;
	@%p8 bra 	$L__BB40_17;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;

$L__BB40_7:
	shl.b64 	%rd39, %rd58, 3;
	add.s64 	%rd40, %rd2, %rd39;
	ld.global.f64 	%fd1, [%rd40];
	cvt.rzi.u32.f64 	%r24, %fd1;
	add.s64 	%rd41, %rd1, %rd58;
	st.global.u8 	[%rd41], %r24;
	add.s32 	%r38, %r38, %r5;
	cvt.u64.u32 	%rd58, %r38;
	setp.lt.u64 	%p9, %rd58, %rd23;
	@%p9 bra 	$L__BB40_7;

$L__BB40_17:
	ret;

}
	// .globl	cast_f64_u32
.visible .entry cast_f64_u32(
	.param .u64 cast_f64_u32_param_0,
	.param .u64 cast_f64_u32_param_1,
	.param .u64 cast_f64_u32_param_2,
	.param .u64 cast_f64_u32_param_3,
	.param .u64 cast_f64_u32_param_4
)
{
	.reg .pred 	%p<17>;
	.reg .b32 	%r<44>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<66>;


	ld.param.u64 	%rd23, [cast_f64_u32_param_0];
	ld.param.u64 	%rd24, [cast_f64_u32_param_1];
	ld.param.u64 	%rd25, [cast_f64_u32_param_2];
	ld.param.u64 	%rd26, [cast_f64_u32_param_3];
	ld.param.u64 	%rd27, [cast_f64_u32_param_4];
	cvta.to.global.u64 	%rd1, %rd27;
	cvta.to.global.u64 	%rd2, %rd26;
	cvta.to.global.u64 	%rd3, %rd25;
	setp.eq.s64 	%p3, %rd24, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p16, %p2;
	@%p3 bra 	$L__BB41_4;

	mov.u64 	%rd60, 1;
	mov.u32 	%r37, 0;

$L__BB41_2:
	not.b32 	%r20, %r37;
	cvt.u64.u32 	%rd29, %r20;
	add.s64 	%rd30, %rd29, %rd24;
	and.b64  	%rd5, %rd30, 4294967295;
	add.s64 	%rd31, %rd5, %rd24;
	shl.b64 	%rd32, %rd31, 3;
	add.s64 	%rd33, %rd3, %rd32;
	ld.global.u64 	%rd34, [%rd33];
	setp.ne.s64 	%p5, %rd60, %rd34;
	mov.pred 	%p16, -1;
	@%p5 bra 	$L__BB41_4;

	shl.b64 	%rd35, %rd5, 3;
	add.s64 	%rd36, %rd3, %rd35;
	ld.global.u64 	%rd37, [%rd36];
	mul.lo.s64 	%rd60, %rd37, %rd60;
	add.s32 	%r37, %r37, 1;
	cvt.u64.u32 	%rd38, %r37;
	setp.lt.u64 	%p7, %rd38, %rd24;
	mov.pred 	%p16, %p2;
	@%p7 bra 	$L__BB41_2;

$L__BB41_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r38, %r21, %r3, %r22;
	cvt.u64.u32 	%rd61, %r38;
	@%p16 bra 	$L__BB41_8;
	bra.uni 	$L__BB41_5;

$L__BB41_8:
	setp.ge.u64 	%p10, %rd61, %rd23;
	@%p10 bra 	$L__BB41_17;

	mov.u32 	%r25, %nctaid.x;
	mul.lo.s32 	%r8, %r3, %r25;
	@%p3 bra 	$L__BB41_16;

$L__BB41_10:
	mov.u32 	%r40, 0;
	mov.u32 	%r41, %r38;
	mov.u32 	%r42, %r40;

$L__BB41_11:
	not.b32 	%r28, %r40;
	cvt.u64.u32 	%rd43, %r28;
	add.s64 	%rd44, %rd43, %rd24;
	cvt.u64.u32 	%rd11, %r41;
	shl.b64 	%rd45, %rd44, 3;
	and.b64  	%rd46, %rd45, 34359738360;
	add.s64 	%rd12, %rd3, %rd46;
	ld.global.u64 	%rd13, [%rd12];
	and.b64  	%rd47, %rd13, -4294967296;
	setp.eq.s64 	%p12, %rd47, 0;
	@%p12 bra 	$L__BB41_13;

	div.u64 	%rd63, %rd11, %rd13;
	mul.lo.s64 	%rd48, %rd63, %rd13;
	sub.s64 	%rd64, %rd11, %rd48;
	bra.uni 	$L__BB41_14;

$L__BB41_13:
	cvt.u32.u64 	%r29, %rd13;
	cvt.u32.u64 	%r30, %rd11;
	div.u32 	%r31, %r30, %r29;
	mul.lo.s32 	%r32, %r31, %r29;
	sub.s32 	%r33, %r30, %r32;
	cvt.u64.u32 	%rd63, %r31;
	cvt.u64.u32 	%rd64, %r33;

$L__BB41_14:
	shl.b64 	%rd49, %rd24, 3;
	add.s64 	%rd50, %rd12, %rd49;
	ld.global.u64 	%rd51, [%rd50];
	mul.lo.s64 	%rd52, %rd51, %rd64;
	cvt.u32.u64 	%r34, %rd52;
	add.s32 	%r42, %r42, %r34;
	cvt.u32.u64 	%r41, %rd63;
	add.s32 	%r40, %r40, 1;
	cvt.u64.u32 	%rd53, %r40;
	setp.lt.u64 	%p13, %rd53, %rd24;
	@%p13 bra 	$L__BB41_11;

	mul.wide.u32 	%rd54, %r42, 8;
	add.s64 	%rd55, %rd2, %rd54;
	ld.global.f64 	%fd2, [%rd55];
	cvt.rzi.u32.f64 	%r35, %fd2;
	shl.b64 	%rd56, %rd61, 2;
	add.s64 	%rd57, %rd1, %rd56;
	st.global.u32 	[%rd57], %r35;
	add.s32 	%r38, %r38, %r8;
	cvt.u64.u32 	%rd61, %r38;
	setp.lt.u64 	%p14, %rd61, %rd23;
	@%p14 bra 	$L__BB41_10;
	bra.uni 	$L__BB41_17;

$L__BB41_16:
	ld.global.f64 	%fd3, [%rd2];
	cvt.rzi.u32.f64 	%r36, %fd3;
	shl.b64 	%rd58, %rd61, 2;
	add.s64 	%rd59, %rd1, %rd58;
	st.global.u32 	[%rd59], %r36;
	add.s32 	%r38, %r38, %r8;
	cvt.u64.u32 	%rd61, %r38;
	setp.lt.u64 	%p15, %rd61, %rd23;
	@%p15 bra 	$L__BB41_16;
	bra.uni 	$L__BB41_17;

$L__BB41_5:
	setp.ge.u64 	%p8, %rd61, %rd23;
	@%p8 bra 	$L__BB41_17;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;

$L__BB41_7:
	shl.b64 	%rd39, %rd61, 3;
	add.s64 	%rd40, %rd2, %rd39;
	ld.global.f64 	%fd1, [%rd40];
	cvt.rzi.u32.f64 	%r24, %fd1;
	shl.b64 	%rd41, %rd61, 2;
	add.s64 	%rd42, %rd1, %rd41;
	st.global.u32 	[%rd42], %r24;
	add.s32 	%r38, %r38, %r5;
	cvt.u64.u32 	%rd61, %r38;
	setp.lt.u64 	%p9, %rd61, %rd23;
	@%p9 bra 	$L__BB41_7;

$L__BB41_17:
	ret;

}
	// .globl	cast_f64_i64
.visible .entry cast_f64_i64(
	.param .u64 cast_f64_i64_param_0,
	.param .u64 cast_f64_i64_param_1,
	.param .u64 cast_f64_i64_param_2,
	.param .u64 cast_f64_i64_param_3,
	.param .u64 cast_f64_i64_param_4
)
{
	.reg .pred 	%p<17>;
	.reg .b32 	%r<41>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<68>;


	ld.param.u64 	%rd23, [cast_f64_i64_param_0];
	ld.param.u64 	%rd24, [cast_f64_i64_param_1];
	ld.param.u64 	%rd25, [cast_f64_i64_param_2];
	ld.param.u64 	%rd26, [cast_f64_i64_param_3];
	ld.param.u64 	%rd27, [cast_f64_i64_param_4];
	cvta.to.global.u64 	%rd1, %rd27;
	cvta.to.global.u64 	%rd2, %rd26;
	cvta.to.global.u64 	%rd3, %rd25;
	setp.eq.s64 	%p3, %rd24, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p16, %p2;
	@%p3 bra 	$L__BB42_4;

	mov.u64 	%rd62, 1;
	mov.u32 	%r34, 0;

$L__BB42_2:
	not.b32 	%r20, %r34;
	cvt.u64.u32 	%rd29, %r20;
	add.s64 	%rd30, %rd29, %rd24;
	and.b64  	%rd5, %rd30, 4294967295;
	add.s64 	%rd31, %rd5, %rd24;
	shl.b64 	%rd32, %rd31, 3;
	add.s64 	%rd33, %rd3, %rd32;
	ld.global.u64 	%rd34, [%rd33];
	setp.ne.s64 	%p5, %rd62, %rd34;
	mov.pred 	%p16, -1;
	@%p5 bra 	$L__BB42_4;

	shl.b64 	%rd35, %rd5, 3;
	add.s64 	%rd36, %rd3, %rd35;
	ld.global.u64 	%rd37, [%rd36];
	mul.lo.s64 	%rd62, %rd37, %rd62;
	add.s32 	%r34, %r34, 1;
	cvt.u64.u32 	%rd38, %r34;
	setp.lt.u64 	%p7, %rd38, %rd24;
	mov.pred 	%p16, %p2;
	@%p7 bra 	$L__BB42_2;

$L__BB42_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r35, %r21, %r3, %r22;
	cvt.u64.u32 	%rd63, %r35;
	@%p16 bra 	$L__BB42_8;
	bra.uni 	$L__BB42_5;

$L__BB42_8:
	setp.ge.u64 	%p10, %rd63, %rd23;
	@%p10 bra 	$L__BB42_17;

	mov.u32 	%r24, %nctaid.x;
	mul.lo.s32 	%r8, %r3, %r24;
	@%p3 bra 	$L__BB42_16;

$L__BB42_10:
	mov.u32 	%r37, 0;
	mov.u32 	%r38, %r35;
	mov.u32 	%r39, %r37;

$L__BB42_11:
	not.b32 	%r27, %r37;
	cvt.u64.u32 	%rd43, %r27;
	add.s64 	%rd44, %rd43, %rd24;
	cvt.u64.u32 	%rd11, %r38;
	shl.b64 	%rd45, %rd44, 3;
	and.b64  	%rd46, %rd45, 34359738360;
	add.s64 	%rd12, %rd3, %rd46;
	ld.global.u64 	%rd13, [%rd12];
	and.b64  	%rd47, %rd13, -4294967296;
	setp.eq.s64 	%p12, %rd47, 0;
	@%p12 bra 	$L__BB42_13;

	div.u64 	%rd65, %rd11, %rd13;
	mul.lo.s64 	%rd48, %rd65, %rd13;
	sub.s64 	%rd66, %rd11, %rd48;
	bra.uni 	$L__BB42_14;

$L__BB42_13:
	cvt.u32.u64 	%r28, %rd13;
	cvt.u32.u64 	%r29, %rd11;
	div.u32 	%r30, %r29, %r28;
	mul.lo.s32 	%r31, %r30, %r28;
	sub.s32 	%r32, %r29, %r31;
	cvt.u64.u32 	%rd65, %r30;
	cvt.u64.u32 	%rd66, %r32;

$L__BB42_14:
	shl.b64 	%rd49, %rd24, 3;
	add.s64 	%rd50, %rd12, %rd49;
	ld.global.u64 	%rd51, [%rd50];
	mul.lo.s64 	%rd52, %rd51, %rd66;
	cvt.u32.u64 	%r33, %rd52;
	add.s32 	%r39, %r39, %r33;
	cvt.u32.u64 	%r38, %rd65;
	add.s32 	%r37, %r37, 1;
	cvt.u64.u32 	%rd53, %r37;
	setp.lt.u64 	%p13, %rd53, %rd24;
	@%p13 bra 	$L__BB42_11;

	mul.wide.u32 	%rd54, %r39, 8;
	add.s64 	%rd55, %rd2, %rd54;
	ld.global.f64 	%fd2, [%rd55];
	cvt.rzi.s64.f64 	%rd56, %fd2;
	shl.b64 	%rd57, %rd63, 3;
	add.s64 	%rd58, %rd1, %rd57;
	st.global.u64 	[%rd58], %rd56;
	add.s32 	%r35, %r35, %r8;
	cvt.u64.u32 	%rd63, %r35;
	setp.lt.u64 	%p14, %rd63, %rd23;
	@%p14 bra 	$L__BB42_10;
	bra.uni 	$L__BB42_17;

$L__BB42_16:
	ld.global.f64 	%fd3, [%rd2];
	cvt.rzi.s64.f64 	%rd59, %fd3;
	shl.b64 	%rd60, %rd63, 3;
	add.s64 	%rd61, %rd1, %rd60;
	st.global.u64 	[%rd61], %rd59;
	add.s32 	%r35, %r35, %r8;
	cvt.u64.u32 	%rd63, %r35;
	setp.lt.u64 	%p15, %rd63, %rd23;
	@%p15 bra 	$L__BB42_16;
	bra.uni 	$L__BB42_17;

$L__BB42_5:
	setp.ge.u64 	%p8, %rd63, %rd23;
	@%p8 bra 	$L__BB42_17;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;

$L__BB42_7:
	shl.b64 	%rd39, %rd63, 3;
	add.s64 	%rd40, %rd2, %rd39;
	ld.global.f64 	%fd1, [%rd40];
	cvt.rzi.s64.f64 	%rd41, %fd1;
	add.s64 	%rd42, %rd1, %rd39;
	st.global.u64 	[%rd42], %rd41;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd63, %r35;
	setp.lt.u64 	%p9, %rd63, %rd23;
	@%p9 bra 	$L__BB42_7;

$L__BB42_17:
	ret;

}
	// .globl	cast_f64_f32
.visible .entry cast_f64_f32(
	.param .u64 cast_f64_f32_param_0,
	.param .u64 cast_f64_f32_param_1,
	.param .u64 cast_f64_f32_param_2,
	.param .u64 cast_f64_f32_param_3,
	.param .u64 cast_f64_f32_param_4
)
{
	.reg .pred 	%p<17>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<41>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<66>;


	ld.param.u64 	%rd23, [cast_f64_f32_param_0];
	ld.param.u64 	%rd24, [cast_f64_f32_param_1];
	ld.param.u64 	%rd25, [cast_f64_f32_param_2];
	ld.param.u64 	%rd26, [cast_f64_f32_param_3];
	ld.param.u64 	%rd27, [cast_f64_f32_param_4];
	cvta.to.global.u64 	%rd1, %rd27;
	cvta.to.global.u64 	%rd2, %rd26;
	cvta.to.global.u64 	%rd3, %rd25;
	setp.eq.s64 	%p3, %rd24, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p16, %p2;
	@%p3 bra 	$L__BB43_4;

	mov.u64 	%rd60, 1;
	mov.u32 	%r34, 0;

$L__BB43_2:
	not.b32 	%r20, %r34;
	cvt.u64.u32 	%rd29, %r20;
	add.s64 	%rd30, %rd29, %rd24;
	and.b64  	%rd5, %rd30, 4294967295;
	add.s64 	%rd31, %rd5, %rd24;
	shl.b64 	%rd32, %rd31, 3;
	add.s64 	%rd33, %rd3, %rd32;
	ld.global.u64 	%rd34, [%rd33];
	setp.ne.s64 	%p5, %rd60, %rd34;
	mov.pred 	%p16, -1;
	@%p5 bra 	$L__BB43_4;

	shl.b64 	%rd35, %rd5, 3;
	add.s64 	%rd36, %rd3, %rd35;
	ld.global.u64 	%rd37, [%rd36];
	mul.lo.s64 	%rd60, %rd37, %rd60;
	add.s32 	%r34, %r34, 1;
	cvt.u64.u32 	%rd38, %r34;
	setp.lt.u64 	%p7, %rd38, %rd24;
	mov.pred 	%p16, %p2;
	@%p7 bra 	$L__BB43_2;

$L__BB43_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r35, %r21, %r3, %r22;
	cvt.u64.u32 	%rd61, %r35;
	@%p16 bra 	$L__BB43_8;
	bra.uni 	$L__BB43_5;

$L__BB43_8:
	setp.ge.u64 	%p10, %rd61, %rd23;
	@%p10 bra 	$L__BB43_17;

	mov.u32 	%r24, %nctaid.x;
	mul.lo.s32 	%r8, %r3, %r24;
	@%p3 bra 	$L__BB43_16;

$L__BB43_10:
	mov.u32 	%r37, 0;
	mov.u32 	%r38, %r35;
	mov.u32 	%r39, %r37;

$L__BB43_11:
	not.b32 	%r27, %r37;
	cvt.u64.u32 	%rd43, %r27;
	add.s64 	%rd44, %rd43, %rd24;
	cvt.u64.u32 	%rd11, %r38;
	shl.b64 	%rd45, %rd44, 3;
	and.b64  	%rd46, %rd45, 34359738360;
	add.s64 	%rd12, %rd3, %rd46;
	ld.global.u64 	%rd13, [%rd12];
	and.b64  	%rd47, %rd13, -4294967296;
	setp.eq.s64 	%p12, %rd47, 0;
	@%p12 bra 	$L__BB43_13;

	div.u64 	%rd63, %rd11, %rd13;
	mul.lo.s64 	%rd48, %rd63, %rd13;
	sub.s64 	%rd64, %rd11, %rd48;
	bra.uni 	$L__BB43_14;

$L__BB43_13:
	cvt.u32.u64 	%r28, %rd13;
	cvt.u32.u64 	%r29, %rd11;
	div.u32 	%r30, %r29, %r28;
	mul.lo.s32 	%r31, %r30, %r28;
	sub.s32 	%r32, %r29, %r31;
	cvt.u64.u32 	%rd63, %r30;
	cvt.u64.u32 	%rd64, %r32;

$L__BB43_14:
	shl.b64 	%rd49, %rd24, 3;
	add.s64 	%rd50, %rd12, %rd49;
	ld.global.u64 	%rd51, [%rd50];
	mul.lo.s64 	%rd52, %rd51, %rd64;
	cvt.u32.u64 	%r33, %rd52;
	add.s32 	%r39, %r39, %r33;
	cvt.u32.u64 	%r38, %rd63;
	add.s32 	%r37, %r37, 1;
	cvt.u64.u32 	%rd53, %r37;
	setp.lt.u64 	%p13, %rd53, %rd24;
	@%p13 bra 	$L__BB43_11;

	mul.wide.u32 	%rd54, %r39, 8;
	add.s64 	%rd55, %rd2, %rd54;
	ld.global.f64 	%fd2, [%rd55];
	cvt.rn.f32.f64 	%f2, %fd2;
	shl.b64 	%rd56, %rd61, 2;
	add.s64 	%rd57, %rd1, %rd56;
	st.global.f32 	[%rd57], %f2;
	add.s32 	%r35, %r35, %r8;
	cvt.u64.u32 	%rd61, %r35;
	setp.lt.u64 	%p14, %rd61, %rd23;
	@%p14 bra 	$L__BB43_10;
	bra.uni 	$L__BB43_17;

$L__BB43_16:
	ld.global.f64 	%fd3, [%rd2];
	cvt.rn.f32.f64 	%f3, %fd3;
	shl.b64 	%rd58, %rd61, 2;
	add.s64 	%rd59, %rd1, %rd58;
	st.global.f32 	[%rd59], %f3;
	add.s32 	%r35, %r35, %r8;
	cvt.u64.u32 	%rd61, %r35;
	setp.lt.u64 	%p15, %rd61, %rd23;
	@%p15 bra 	$L__BB43_16;
	bra.uni 	$L__BB43_17;

$L__BB43_5:
	setp.ge.u64 	%p8, %rd61, %rd23;
	@%p8 bra 	$L__BB43_17;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;

$L__BB43_7:
	shl.b64 	%rd39, %rd61, 3;
	add.s64 	%rd40, %rd2, %rd39;
	ld.global.f64 	%fd1, [%rd40];
	cvt.rn.f32.f64 	%f1, %fd1;
	shl.b64 	%rd41, %rd61, 2;
	add.s64 	%rd42, %rd1, %rd41;
	st.global.f32 	[%rd42], %f1;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd61, %r35;
	setp.lt.u64 	%p9, %rd61, %rd23;
	@%p9 bra 	$L__BB43_7;

$L__BB43_17:
	ret;

}
	// .globl	cast_f64_f64
.visible .entry cast_f64_f64(
	.param .u64 cast_f64_f64_param_0,
	.param .u64 cast_f64_f64_param_1,
	.param .u64 cast_f64_f64_param_2,
	.param .u64 cast_f64_f64_param_3,
	.param .u64 cast_f64_f64_param_4
)
{
	.reg .pred 	%p<17>;
	.reg .b32 	%r<41>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<65>;


	ld.param.u64 	%rd23, [cast_f64_f64_param_0];
	ld.param.u64 	%rd24, [cast_f64_f64_param_1];
	ld.param.u64 	%rd25, [cast_f64_f64_param_2];
	ld.param.u64 	%rd26, [cast_f64_f64_param_3];
	ld.param.u64 	%rd27, [cast_f64_f64_param_4];
	cvta.to.global.u64 	%rd1, %rd27;
	cvta.to.global.u64 	%rd2, %rd26;
	cvta.to.global.u64 	%rd3, %rd25;
	setp.eq.s64 	%p3, %rd24, 0;
	mov.pred 	%p2, 0;
	mov.pred 	%p16, %p2;
	@%p3 bra 	$L__BB44_4;

	mov.u64 	%rd59, 1;
	mov.u32 	%r34, 0;

$L__BB44_2:
	not.b32 	%r20, %r34;
	cvt.u64.u32 	%rd29, %r20;
	add.s64 	%rd30, %rd29, %rd24;
	and.b64  	%rd5, %rd30, 4294967295;
	add.s64 	%rd31, %rd5, %rd24;
	shl.b64 	%rd32, %rd31, 3;
	add.s64 	%rd33, %rd3, %rd32;
	ld.global.u64 	%rd34, [%rd33];
	setp.ne.s64 	%p5, %rd59, %rd34;
	mov.pred 	%p16, -1;
	@%p5 bra 	$L__BB44_4;

	shl.b64 	%rd35, %rd5, 3;
	add.s64 	%rd36, %rd3, %rd35;
	ld.global.u64 	%rd37, [%rd36];
	mul.lo.s64 	%rd59, %rd37, %rd59;
	add.s32 	%r34, %r34, 1;
	cvt.u64.u32 	%rd38, %r34;
	setp.lt.u64 	%p7, %rd38, %rd24;
	mov.pred 	%p16, %p2;
	@%p7 bra 	$L__BB44_2;

$L__BB44_4:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r35, %r21, %r3, %r22;
	cvt.u64.u32 	%rd60, %r35;
	@%p16 bra 	$L__BB44_8;
	bra.uni 	$L__BB44_5;

$L__BB44_8:
	setp.ge.u64 	%p10, %rd60, %rd23;
	@%p10 bra 	$L__BB44_17;

	mov.u32 	%r24, %nctaid.x;
	mul.lo.s32 	%r8, %r3, %r24;
	@%p3 bra 	$L__BB44_16;

$L__BB44_10:
	mov.u32 	%r37, 0;
	mov.u32 	%r38, %r35;
	mov.u32 	%r39, %r37;

$L__BB44_11:
	not.b32 	%r27, %r37;
	cvt.u64.u32 	%rd42, %r27;
	add.s64 	%rd43, %rd42, %rd24;
	cvt.u64.u32 	%rd11, %r38;
	shl.b64 	%rd44, %rd43, 3;
	and.b64  	%rd45, %rd44, 34359738360;
	add.s64 	%rd12, %rd3, %rd45;
	ld.global.u64 	%rd13, [%rd12];
	and.b64  	%rd46, %rd13, -4294967296;
	setp.eq.s64 	%p12, %rd46, 0;
	@%p12 bra 	$L__BB44_13;

	div.u64 	%rd62, %rd11, %rd13;
	mul.lo.s64 	%rd47, %rd62, %rd13;
	sub.s64 	%rd63, %rd11, %rd47;
	bra.uni 	$L__BB44_14;

$L__BB44_13:
	cvt.u32.u64 	%r28, %rd13;
	cvt.u32.u64 	%r29, %rd11;
	div.u32 	%r30, %r29, %r28;
	mul.lo.s32 	%r31, %r30, %r28;
	sub.s32 	%r32, %r29, %r31;
	cvt.u64.u32 	%rd62, %r30;
	cvt.u64.u32 	%rd63, %r32;

$L__BB44_14:
	shl.b64 	%rd48, %rd24, 3;
	add.s64 	%rd49, %rd12, %rd48;
	ld.global.u64 	%rd50, [%rd49];
	mul.lo.s64 	%rd51, %rd50, %rd63;
	cvt.u32.u64 	%r33, %rd51;
	add.s32 	%r39, %r39, %r33;
	cvt.u32.u64 	%r38, %rd62;
	add.s32 	%r37, %r37, 1;
	cvt.u64.u32 	%rd52, %r37;
	setp.lt.u64 	%p13, %rd52, %rd24;
	@%p13 bra 	$L__BB44_11;

	mul.wide.u32 	%rd53, %r39, 8;
	add.s64 	%rd54, %rd2, %rd53;
	ld.global.f64 	%fd2, [%rd54];
	shl.b64 	%rd55, %rd60, 3;
	add.s64 	%rd56, %rd1, %rd55;
	st.global.f64 	[%rd56], %fd2;
	add.s32 	%r35, %r35, %r8;
	cvt.u64.u32 	%rd60, %r35;
	setp.lt.u64 	%p14, %rd60, %rd23;
	@%p14 bra 	$L__BB44_10;
	bra.uni 	$L__BB44_17;

$L__BB44_16:
	ld.global.f64 	%fd3, [%rd2];
	shl.b64 	%rd57, %rd60, 3;
	add.s64 	%rd58, %rd1, %rd57;
	st.global.f64 	[%rd58], %fd3;
	add.s32 	%r35, %r35, %r8;
	cvt.u64.u32 	%rd60, %r35;
	setp.lt.u64 	%p15, %rd60, %rd23;
	@%p15 bra 	$L__BB44_16;
	bra.uni 	$L__BB44_17;

$L__BB44_5:
	setp.ge.u64 	%p8, %rd60, %rd23;
	@%p8 bra 	$L__BB44_17;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;

$L__BB44_7:
	shl.b64 	%rd39, %rd60, 3;
	add.s64 	%rd40, %rd2, %rd39;
	ld.global.f64 	%fd1, [%rd40];
	add.s64 	%rd41, %rd1, %rd39;
	st.global.f64 	[%rd41], %fd1;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd60, %r35;
	setp.lt.u64 	%p9, %rd60, %rd23;
	@%p9 bra 	$L__BB44_7;

$L__BB44_17:
	ret;

}

